(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function n(n){for(var o,i,s=n[0],l=n[1],u=n[2],c=0,h=[];c<s.length;c++)i=s[c],Object.prototype.hasOwnProperty.call(a,i)&&a[i]&&h.push(a[i][0]),a[i]=0;for(o in l)Object.prototype.hasOwnProperty.call(l,o)&&(e[o]=l[o]);for(d&&d(n);h.length;)h.shift()();return r.push.apply(r,u||[]),t()}function t(){for(var e,n=0;n<r.length;n++){for(var t=r[n],o=!0,s=1;s<t.length;s++){var l=t[s];0!==a[l]&&(o=!1)}o&&(r.splice(n--,1),e=i(i.s=t[0]))}return e}var o={},a={1:0},r=[];function i(n){if(o[n])return o[n].exports;var t=o[n]={i:n,l:!1,exports:{}};return e[n].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.e=function(e){var n=[],t=a[e];if(0!==t)if(t)n.push(t[2]);else{var o=new Promise((function(n,o){t=a[e]=[n,o]}));n.push(t[2]=o);var r,s=document.createElement("script");s.charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.src=function(e){return i.p+"assets/js/"+({}[e]||e)+"."+{2:"55fe9868",3:"1c12d040",4:"2df6fe54",5:"1de3cb7e",6:"bd855a5e",7:"145e2f29",8:"649d554c",9:"16079c0b",10:"b0a59540",11:"025b823b",12:"021c98bb",13:"435bedc3",14:"89b17b8f",15:"293cc7c9",16:"75a90a42",17:"9ede011e",18:"e13fc20e",19:"65ca8473",20:"50ab7146",21:"d925c5cd",22:"c143a751",23:"d40a20ba",24:"f7158835",25:"bf04a0f3",26:"b09b330a",27:"e8f900f2",28:"00a541aa",29:"6e22c308",30:"dbe058c7",31:"3557f94c",32:"7f2db2c9",33:"77e64a94",34:"441f8fd3",35:"804b43ff",36:"9892e513",37:"ed7c8cfa",38:"c86b3b1a",39:"81197ed1",40:"7be52e3b",41:"712ad4e1",42:"eb626a12",43:"0d7679b4",44:"ad5a1206",45:"cbe66219",46:"83d39c6e",47:"57e01177",48:"c5a966a9",49:"644c02aa",50:"9f53ffd3",51:"328760d2",52:"a8e486df",53:"402a2b80",54:"f068a011",55:"5c2602fa",56:"d0a40d16",57:"917b3c40",58:"4ca76f79",59:"aa1a4713",60:"89043193",61:"a887dd7a",62:"95f0f1ff",63:"3e1b9843",64:"844d8eaf",65:"320db1d8",66:"7413ccd5",67:"edf372ef",68:"ff280240",69:"ded3ac15",70:"df5954d4",71:"40923f5a",72:"89aea7e1",73:"338334bb",74:"2e92a264",75:"d691c49d",76:"caf89015",77:"99eec6ee",78:"3056d2d6",79:"8cd3afd2",80:"e253e02f",81:"477ee9a9",82:"d009bf10",83:"565e9752",84:"c6e1d7e4",85:"0ae4f8b5",86:"a3217e39",87:"0f80f785",88:"b51d5d5e",89:"df48c6fc",90:"53ee721a",91:"6ad2f535",92:"07dd7e50",93:"0c40ec23",94:"dde6eec7",95:"0db976e6",96:"b8928acb",97:"eba38649",98:"42831d0b",99:"a0385235",100:"013206d7",101:"81619aab",102:"d935473e",103:"c05db74a",104:"e9d55df8",105:"eb6ba833",106:"865ddbb2",107:"936b7ff6",108:"e964317a",109:"7e38ea71",110:"fb940143",111:"ba4da2a5",112:"7a709a1f",113:"920b95a2",114:"627f914b",115:"fd91cec9",116:"7e5d5e56",117:"d357a42f",118:"e08d8c97",119:"ed3126e8",120:"93e2a688",121:"128b6074",122:"b6ca2d85",123:"274954e6",124:"5d561c13",125:"cc5498f6",126:"f3cc6710",127:"f4a9a0c7",128:"9fc7e62f",129:"b5fa5c69",130:"7af79ec7",131:"eab80885",132:"95816af2",133:"93200046",134:"6cec2419",135:"c07340ed",136:"664f0607",137:"14e90171",138:"c2798ae3",139:"3111c674",140:"52c4c343",141:"ba166826",142:"67662f61",143:"7ebe7d39",144:"f4b76b41",145:"00b50d7b",146:"4cd0531e",147:"87e644bf",148:"a48d3e56",149:"08656d65",150:"6d526d25",151:"53976f2e",152:"6df0c8fd",153:"18e35658",154:"01530661",155:"ee0ea6dc",156:"e419b155",157:"3bac4dd1",158:"a03c58d4",159:"37ae6e62",160:"8aa99f65",161:"9a6ba792",162:"ee381b70",163:"65a05b24",164:"0145fb78",165:"4d01bc20",166:"45093d62",167:"00561f40",168:"3a357c51",169:"181352e1",170:"ec4ee21c",171:"8adbffd2",172:"1196ecfe",173:"20f252ce",174:"4485a176",175:"2a355cf2",176:"447b6328",177:"f5ebd294",178:"cdec00c1",179:"4be1c6a0",180:"5ad0faf2",181:"578f62ca",182:"2436ee01",183:"164a9f77",184:"70939884",185:"40b18144",186:"c0bec0de",187:"7ede1e79",188:"57f45488",189:"2c3937b9",190:"9fdcfd30",191:"525c794b",192:"e1a1d451",193:"a2376a2b",194:"98e9005c",195:"172f973e",196:"1c0b02b4",197:"cc1430d6",198:"6b6a9280",199:"a252afea",200:"3585f55e",201:"69d3e8d6",202:"42012e72",203:"8aa955be",204:"0cbe3e29",205:"a8efe622",206:"83b97680",207:"d1b0106c",208:"7e741ac6",209:"36b2da15",210:"4efdecf1",211:"0052a673",212:"80213b76",213:"b1e72f11",214:"1541110f",215:"8adbaeb9",216:"295b6c8b",217:"6da0096f",218:"7deefada",219:"f9cd714f",220:"8218dcee",221:"3bb3d89a",222:"b8566a0c",223:"1e9f7900",224:"2a2bc245",225:"d3e94870",226:"5b830e2b",227:"72a258e1",228:"d7a1c6bf",229:"ec2db3b8",230:"9d210a82",231:"c099ac50",232:"9e1c67c2",233:"f07e8824",234:"9edb18c6",235:"0af11588",236:"48fb86ef",237:"ba4a94d7",238:"5cd16ec1",239:"48ab0875",240:"275cb900",241:"4a935013",242:"9b6ef2c2",243:"16cd3422",244:"b0adbede",245:"4514fd4f",246:"e9a6b91b",247:"bde6b23e",248:"d159f9bc",249:"aac40196",250:"fe0ac93e",251:"41d6352d",252:"465a8fbe",253:"4d2559e5",254:"24a219c7",255:"79fa4788",256:"63b9843f",257:"7f637be4",258:"f55283b5",259:"5ea66944",260:"743edb05",261:"b173d2fe",262:"e47d7698",263:"fe87b167",264:"3211b1b6",265:"909e4fd6",266:"c410a7f3",267:"16f4ad72",268:"c63310dc",269:"38c9b366",270:"30d38899",271:"99f143d9",272:"14b09db2",273:"a8a8158c",274:"fe3f566d",275:"679fd464",276:"7e140992",277:"5985d729",278:"c5317b3a",279:"bf4f716f",280:"56cfce91",281:"57949af8"}[e]+".js"}(e);var l=new Error;r=function(n){s.onerror=s.onload=null,clearTimeout(u);var t=a[e];if(0!==t){if(t){var o=n&&("load"===n.type?"missing":n.type),r=n&&n.target&&n.target.src;l.message="Loading chunk "+e+" failed.\n("+o+": "+r+")",l.name="ChunkLoadError",l.type=o,l.request=r,t[1](l)}a[e]=void 0}};var u=setTimeout((function(){r({type:"timeout",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(n)},i.m=e,i.c=o,i.d=function(e,n,t){i.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:t})},i.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.t=function(e,n){if(1&n&&(e=i(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var o in e)i.d(t,o,function(n){return e[n]}.bind(null,o));return t},i.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return i.d(n,"a",n),n},i.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},i.p="/",i.oe=function(e){throw console.error(e),e};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=n,s=s.slice();for(var u=0;u<s.length;u++)n(s[u]);var d=l;r.push([188,0]),t()}([function(e,n,t){var o=t(3),a=t(24).f,r=t(12),i=t(14),s=t(73),l=t(117),u=t(104);e.exports=function(e,n){var t,d,c,h,p,y=e.target,m=e.global,g=e.stat;if(t=m?o:g?o[y]||s(y,{}):(o[y]||{}).prototype)for(d in n){if(h=n[d],c=e.noTargetGet?(p=a(t,d))&&p.value:t[d],!u(m?d:y+(g?".":"#")+d,e.forced)&&void 0!==c){if(typeof h==typeof c)continue;l(h,c)}(e.sham||c&&c.sham)&&r(h,"sham",!0),i(t,d,h,e)}}},function(e,n,t){var o=t(3),a=t(50),r=t(7),i=t(52),s=t(75),l=t(112),u=a("wks"),d=o.Symbol,c=l?d:d&&d.withoutSetter||i;e.exports=function(e){return r(u,e)&&(s||"string"==typeof u[e])||(s&&r(d,e)?u[e]=d[e]:u[e]=c("Symbol."+e)),u[e]}},function(e,n){e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,n){var t=function(e){return e&&e.Math==Math&&e};e.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(e,n){e.exports=function(e){return"object"==typeof e?null!==e:"function"==typeof e}},function(e,n,t){var o=t(4);e.exports=function(e){if(!o(e))throw TypeError(String(e)+" is not an object");return e}},function(e,n,t){var o=t(2);e.exports=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,n,t){var o=t(10),a={}.hasOwnProperty;e.exports=Object.hasOwn||function(e,n){return a.call(o(e),n)}},function(e,n,t){var o=t(6),a=t(110),r=t(5),i=t(51),s=Object.defineProperty;n.f=o?s:function(e,n,t){if(r(e),n=i(n,!0),r(t),a)try{return s(e,n,t)}catch(e){}if("get"in t||"set"in t)throw TypeError("Accessors not supported");return"value"in t&&(e[n]=t.value),e}},function(e,n,t){var o=t(83),a=t(14),r=t(200);o||a(Object.prototype,"toString",r,{unsafe:!0})},function(e,n,t){var o=t(23);e.exports=function(e){return Object(o(e))}},function(e,n,t){"use strict";var o=t(107).charAt,a=t(28),r=t(116),i=a.set,s=a.getterFor("String Iterator");r(String,"String",(function(e){i(this,{type:"String Iterator",string:String(e),index:0})}),(function(){var e,n=s(this),t=n.string,a=n.index;return a>=t.length?{value:void 0,done:!0}:(e=o(t,a),n.index+=e.length,{value:e,done:!1})}))},function(e,n,t){var o=t(6),a=t(8),r=t(31);e.exports=o?function(e,n,t){return a.f(e,n,r(1,t))}:function(e,n,t){return e[n]=t,e}},function(e,n,t){var o=t(46),a=Math.min;e.exports=function(e){return e>0?a(o(e),9007199254740991):0}},function(e,n,t){var o=t(3),a=t(12),r=t(7),i=t(73),s=t(79),l=t(28),u=l.get,d=l.enforce,c=String(String).split("String");(e.exports=function(e,n,t,s){var l,u=!!s&&!!s.unsafe,h=!!s&&!!s.enumerable,p=!!s&&!!s.noTargetGet;"function"==typeof t&&("string"!=typeof n||r(t,"name")||a(t,"name",n),(l=d(t)).source||(l.source=c.join("string"==typeof n?n:""))),e!==o?(u?!p&&e[n]&&(h=!0):delete e[n],h?e[n]=t:a(e,n,t)):h?e[n]=t:i(n,t)})(Function.prototype,"toString",(function(){return"function"==typeof this&&u(this).source||s(this)}))},function(e,n,t){var o=t(36),a=t(23);e.exports=function(e){return o(a(e))}},function(e,n,t){var o=t(3),a=t(129),r=t(102),i=t(12),s=t(1),l=s("iterator"),u=s("toStringTag"),d=r.values;for(var c in a){var h=o[c],p=h&&h.prototype;if(p){if(p[l]!==d)try{i(p,l,d)}catch(e){p[l]=d}if(p[u]||i(p,u,c),a[c])for(var y in r)if(p[y]!==r[y])try{i(p,y,r[y])}catch(e){p[y]=r[y]}}}},function(e,n){var t=Array.isArray;e.exports=t},function(e,n,t){var o=t(140),a="object"==typeof self&&self&&self.Object===Object&&self,r=o||a||Function("return this")();e.exports=r},function(e,n,t){var o=t(111),a=t(3),r=function(e){return"function"==typeof e?e:void 0};e.exports=function(e,n){return arguments.length<2?r(o[e])||r(a[e]):o[e]&&o[e][n]||a[e]&&a[e][n]}},function(e,n){e.exports=!1},function(e,n){e.exports=function(e){if("function"!=typeof e)throw TypeError(String(e)+" is not a function");return e}},function(e,n,t){var o=t(227),a=t(230);e.exports=function(e,n){var t=a(e,n);return o(t)?t:void 0}},function(e,n){e.exports=function(e){if(null==e)throw TypeError("Can't call method on "+e);return e}},function(e,n,t){var o=t(6),a=t(80),r=t(31),i=t(15),s=t(51),l=t(7),u=t(110),d=Object.getOwnPropertyDescriptor;n.f=o?d:function(e,n){if(e=i(e),n=s(n,!0),u)try{return d(e,n)}catch(e){}if(l(e,n))return r(!a.f.call(e,n),e[n])}},function(e,n){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,n){var t={}.toString;e.exports=function(e){return t.call(e).slice(8,-1)}},function(e,n,t){"use strict";var o=t(0),a=t(29).filter;o({target:"Array",proto:!0,forced:!t(58)("filter")},{filter:function(e){return a(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){var o,a,r,i=t(189),s=t(3),l=t(4),u=t(12),d=t(7),c=t(72),h=t(54),p=t(38),y=s.WeakMap;if(i||c.state){var m=c.state||(c.state=new y),g=m.get,b=m.has,f=m.set;o=function(e,n){if(b.call(m,e))throw new TypeError("Object already initialized");return n.facade=e,f.call(m,e,n),n},a=function(e){return g.call(m,e)||{}},r=function(e){return b.call(m,e)}}else{var w=h("state");p[w]=!0,o=function(e,n){if(d(e,w))throw new TypeError("Object already initialized");return n.facade=e,u(e,w,n),n},a=function(e){return d(e,w)?e[w]:{}},r=function(e){return d(e,w)}}e.exports={set:o,get:a,has:r,enforce:function(e){return r(e)?a(e):o(e,{})},getterFor:function(e){return function(n){var t;if(!l(n)||(t=a(n)).type!==e)throw TypeError("Incompatible receiver, "+e+" required");return t}}}},function(e,n,t){var o=t(48),a=t(36),r=t(10),i=t(13),s=t(108),l=[].push,u=function(e){var n=1==e,t=2==e,u=3==e,d=4==e,c=6==e,h=7==e,p=5==e||c;return function(y,m,g,b){for(var f,w,v=r(y),k=a(v),q=o(m,g,3),x=i(k.length),j=0,S=b||s,T=n?S(y,x):t||h?S(y,0):void 0;x>j;j++)if((p||j in k)&&(w=q(f=k[j],j,v),e))if(n)T[j]=w;else if(w)switch(e){case 3:return!0;case 5:return f;case 6:return j;case 2:l.call(T,f)}else switch(e){case 4:return!1;case 7:l.call(T,f)}return c?-1:u||d?d:T}};e.exports={forEach:u(0),map:u(1),filter:u(2),some:u(3),every:u(4),find:u(5),findIndex:u(6),filterOut:u(7)}},function(e,n,t){"use strict";var o=t(2);e.exports=function(e,n){var t=[][e];return!!t&&o((function(){t.call(null,n||function(){throw 1},1)}))}},function(e,n){e.exports=function(e,n){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:n}}},function(e,n,t){var o,a=t(5),r=t(169),i=t(78),s=t(38),l=t(115),u=t(74),d=t(54),c=d("IE_PROTO"),h=function(){},p=function(e){return"<script>"+e+"<\/script>"},y=function(){try{o=document.domain&&new ActiveXObject("htmlfile")}catch(e){}var e,n;y=o?function(e){e.write(p("")),e.close();var n=e.parentWindow.Object;return e=null,n}(o):((n=u("iframe")).style.display="none",l.appendChild(n),n.src=String("javascript:"),(e=n.contentWindow.document).open(),e.write(p("document.F=Object")),e.close(),e.F);for(var t=i.length;t--;)delete y.prototype[i[t]];return y()};s[c]=!0,e.exports=Object.create||function(e,n){var t;return null!==e?(h.prototype=a(e),t=new h,h.prototype=null,t[c]=e):t=y(),void 0===n?t:r(t,n)}},function(e,n,t){var o=t(26);e.exports=Array.isArray||function(e){return"Array"==o(e)}},function(e,n,t){"use strict";var o=t(0),a=t(3),r=t(19),i=t(20),s=t(6),l=t(75),u=t(112),d=t(2),c=t(7),h=t(33),p=t(4),y=t(5),m=t(10),g=t(15),b=t(51),f=t(31),w=t(32),v=t(53),k=t(69),q=t(206),x=t(81),j=t(24),S=t(8),T=t(80),I=t(12),P=t(14),Q=t(50),z=t(54),C=t(38),A=t(52),E=t(1),L=t(135),O=t(136),H=t(47),D=t(28),_=t(29).forEach,N=z("hidden"),M=E("toPrimitive"),G=D.set,R=D.getterFor("Symbol"),B=Object.prototype,W=a.Symbol,F=r("JSON","stringify"),U=j.f,Y=S.f,V=q.f,J=T.f,$=Q("symbols"),K=Q("op-symbols"),Z=Q("string-to-symbol-registry"),X=Q("symbol-to-string-registry"),ee=Q("wks"),ne=a.QObject,te=!ne||!ne.prototype||!ne.prototype.findChild,oe=s&&d((function(){return 7!=w(Y({},"a",{get:function(){return Y(this,"a",{value:7}).a}})).a}))?function(e,n,t){var o=U(B,n);o&&delete B[n],Y(e,n,t),o&&e!==B&&Y(B,n,o)}:Y,ae=function(e,n){var t=$[e]=w(W.prototype);return G(t,{type:"Symbol",tag:e,description:n}),s||(t.description=n),t},re=u?function(e){return"symbol"==typeof e}:function(e){return Object(e)instanceof W},ie=function(e,n,t){e===B&&ie(K,n,t),y(e);var o=b(n,!0);return y(t),c($,o)?(t.enumerable?(c(e,N)&&e[N][o]&&(e[N][o]=!1),t=w(t,{enumerable:f(0,!1)})):(c(e,N)||Y(e,N,f(1,{})),e[N][o]=!0),oe(e,o,t)):Y(e,o,t)},se=function(e,n){y(e);var t=g(n),o=v(t).concat(ce(t));return _(o,(function(n){s&&!le.call(t,n)||ie(e,n,t[n])})),e},le=function(e){var n=b(e,!0),t=J.call(this,n);return!(this===B&&c($,n)&&!c(K,n))&&(!(t||!c(this,n)||!c($,n)||c(this,N)&&this[N][n])||t)},ue=function(e,n){var t=g(e),o=b(n,!0);if(t!==B||!c($,o)||c(K,o)){var a=U(t,o);return!a||!c($,o)||c(t,N)&&t[N][o]||(a.enumerable=!0),a}},de=function(e){var n=V(g(e)),t=[];return _(n,(function(e){c($,e)||c(C,e)||t.push(e)})),t},ce=function(e){var n=e===B,t=V(n?K:g(e)),o=[];return _(t,(function(e){!c($,e)||n&&!c(B,e)||o.push($[e])})),o};(l||(P((W=function(){if(this instanceof W)throw TypeError("Symbol is not a constructor");var e=arguments.length&&void 0!==arguments[0]?String(arguments[0]):void 0,n=A(e),t=function(e){this===B&&t.call(K,e),c(this,N)&&c(this[N],n)&&(this[N][n]=!1),oe(this,n,f(1,e))};return s&&te&&oe(B,n,{configurable:!0,set:t}),ae(n,e)}).prototype,"toString",(function(){return R(this).tag})),P(W,"withoutSetter",(function(e){return ae(A(e),e)})),T.f=le,S.f=ie,j.f=ue,k.f=q.f=de,x.f=ce,L.f=function(e){return ae(E(e),e)},s&&(Y(W.prototype,"description",{configurable:!0,get:function(){return R(this).description}}),i||P(B,"propertyIsEnumerable",le,{unsafe:!0}))),o({global:!0,wrap:!0,forced:!l,sham:!l},{Symbol:W}),_(v(ee),(function(e){O(e)})),o({target:"Symbol",stat:!0,forced:!l},{for:function(e){var n=String(e);if(c(Z,n))return Z[n];var t=W(n);return Z[n]=t,X[t]=n,t},keyFor:function(e){if(!re(e))throw TypeError(e+" is not a symbol");if(c(X,e))return X[e]},useSetter:function(){te=!0},useSimple:function(){te=!1}}),o({target:"Object",stat:!0,forced:!l,sham:!s},{create:function(e,n){return void 0===n?w(e):se(w(e),n)},defineProperty:ie,defineProperties:se,getOwnPropertyDescriptor:ue}),o({target:"Object",stat:!0,forced:!l},{getOwnPropertyNames:de,getOwnPropertySymbols:ce}),o({target:"Object",stat:!0,forced:d((function(){x.f(1)}))},{getOwnPropertySymbols:function(e){return x.f(m(e))}}),F)&&o({target:"JSON",stat:!0,forced:!l||d((function(){var e=W();return"[null]"!=F([e])||"{}"!=F({a:e})||"{}"!=F(Object(e))}))},{stringify:function(e,n,t){for(var o,a=[e],r=1;arguments.length>r;)a.push(arguments[r++]);if(o=n,(p(n)||void 0!==e)&&!re(e))return h(n)||(n=function(e,n){if("function"==typeof o&&(n=o.call(this,e,n)),!re(n))return n}),a[1]=n,F.apply(null,a)}});W.prototype[M]||I(W.prototype,M,W.prototype.valueOf),H(W,"Symbol"),C[N]=!0},function(e,n,t){var o=t(41),a=t(212),r=t(213),i=o?o.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":i&&i in Object(e)?a(e):r(e)}},function(e,n,t){var o=t(2),a=t(26),r="".split;e.exports=o((function(){return!Object("z").propertyIsEnumerable(0)}))?function(e){return"String"==a(e)?r.call(e,""):Object(e)}:Object},function(e,n,t){var o,a,r=t(3),i=t(76),s=r.process,l=s&&s.versions,u=l&&l.v8;u?a=(o=u.split("."))[0]<4?1:o[0]+o[1]:i&&(!(o=i.match(/Edge\/(\d+)/))||o[1]>=74)&&(o=i.match(/Chrome\/(\d+)/))&&(a=o[1]),e.exports=a&&+a},function(e,n){e.exports={}},function(e,n){e.exports={}},function(e,n,t){"use strict";var o=t(0),a=t(6),r=t(3),i=t(7),s=t(4),l=t(8).f,u=t(117),d=r.Symbol;if(a&&"function"==typeof d&&(!("description"in d.prototype)||void 0!==d().description)){var c={},h=function(){var e=arguments.length<1||void 0===arguments[0]?void 0:String(arguments[0]),n=this instanceof h?new d(e):void 0===e?d():d(e);return""===e&&(c[n]=!0),n};u(h,d);var p=h.prototype=d.prototype;p.constructor=h;var y=p.toString,m="Symbol(test)"==String(d("test")),g=/^Symbol\((.*)\)[^)]+$/;l(p,"description",{configurable:!0,get:function(){var e=s(this)?this.valueOf():this,n=y.call(e);if(i(c,e))return"";var t=m?n.slice(7,-1):n.replace(g,"$1");return""===t?void 0:t}}),o({global:!0,forced:!0},{Symbol:h})}},function(e,n,t){var o=t(18).Symbol;e.exports=o},function(e,n,t){"use strict";t.d(n,"a",(function(){return r}));t(101);var o=t(43);t(34),t(40),t(9),t(59),t(11),t(16),t(137);var a=t(65);function r(e){return function(e){if(Array.isArray(e))return Object(o.a)(e)}(e)||function(e){if("undefined"!=typeof Symbol&&null!=e[Symbol.iterator]||null!=e["@@iterator"])return Array.from(e)}(e)||Object(a.a)(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(e,n,t){"use strict";function o(e,n){(null==n||n>e.length)&&(n=e.length);for(var t=0,o=new Array(n);t<n;t++)o[t]=e[t];return o}t.d(n,"a",(function(){return o}))},function(e,n,t){"use strict";function o(e,n,t,o,a,r,i,s){var l,u="function"==typeof e?e.options:e;if(n&&(u.render=n,u.staticRenderFns=t,u._compiled=!0),o&&(u.functional=!0),r&&(u._scopeId="data-v-"+r),i?(l=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),a&&a.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(i)},u._ssrRegister=l):a&&(l=s?function(){a.call(this,(u.functional?this.parent:this).$root.$options.shadowRoot)}:a),l)if(u.functional){u._injectStyles=l;var d=u.render;u.render=function(e,n){return l.call(n),d(e,n)}}else{var c=u.beforeCreate;u.beforeCreate=c?[].concat(c,l):[l]}return{exports:e,options:u}}t.d(n,"a",(function(){return o}))},function(e,n,t){"use strict";var o=t(0),a=t(71);o({target:"RegExp",proto:!0,forced:/./.exec!==a},{exec:a})},function(e,n){var t=Math.ceil,o=Math.floor;e.exports=function(e){return isNaN(e=+e)?0:(e>0?o:t)(e)}},function(e,n,t){var o=t(8).f,a=t(7),r=t(1)("toStringTag");e.exports=function(e,n,t){e&&!a(e=t?e:e.prototype,r)&&o(e,r,{configurable:!0,value:n})}},function(e,n,t){var o=t(21);e.exports=function(e,n,t){if(o(e),void 0===n)return e;switch(t){case 0:return function(){return e.call(n)};case 1:return function(t){return e.call(n,t)};case 2:return function(t,o){return e.call(n,t,o)};case 3:return function(t,o,a){return e.call(n,t,o,a)}}return function(){return e.apply(n,arguments)}}},function(e,n,t){"use strict";var o=t(0),a=t(4),r=t(33),i=t(114),s=t(13),l=t(15),u=t(57),d=t(1),c=t(58)("slice"),h=d("species"),p=[].slice,y=Math.max;o({target:"Array",proto:!0,forced:!c},{slice:function(e,n){var t,o,d,c=l(this),m=s(c.length),g=i(e,m),b=i(void 0===n?m:n,m);if(r(c)&&("function"!=typeof(t=c.constructor)||t!==Array&&!r(t.prototype)?a(t)&&null===(t=t[h])&&(t=void 0):t=void 0,t===Array||void 0===t))return p.call(c,g,b);for(o=new(void 0===t?Array:t)(y(b-g,0)),d=0;g<b;g++,d++)g in c&&u(o,d,c[g]);return o.length=d,o}})},function(e,n,t){var o=t(20),a=t(72);(e.exports=function(e,n){return a[e]||(a[e]=void 0!==n?n:{})})("versions",[]).push({version:"3.14.0",mode:o?"pure":"global",copyright:"Â© 2021 Denis Pushkarev (zloirock.ru)"})},function(e,n,t){var o=t(4);e.exports=function(e,n){if(!o(e))return e;var t,a;if(n&&"function"==typeof(t=e.toString)&&!o(a=t.call(e)))return a;if("function"==typeof(t=e.valueOf)&&!o(a=t.call(e)))return a;if(!n&&"function"==typeof(t=e.toString)&&!o(a=t.call(e)))return a;throw TypeError("Can't convert object to primitive value")}},function(e,n){var t=0,o=Math.random();e.exports=function(e){return"Symbol("+String(void 0===e?"":e)+")_"+(++t+o).toString(36)}},function(e,n,t){var o=t(113),a=t(78);e.exports=Object.keys||function(e){return o(e,a)}},function(e,n,t){var o=t(50),a=t(52),r=o("keys");e.exports=function(e){return r[e]||(r[e]=a(e))}},function(e,n,t){var o=t(26),a=t(3);e.exports="process"==o(a.process)},function(e,n,t){"use strict";t.d(n,"a",(function(){return a}));t(9);function o(e,n,t,o,a,r,i){try{var s=e[r](i),l=s.value}catch(e){return void t(e)}s.done?n(l):Promise.resolve(l).then(o,a)}function a(e){return function(){var n=this,t=arguments;return new Promise((function(a,r){var i=e.apply(n,t);function s(e){o(i,a,r,s,l,"next",e)}function l(e){o(i,a,r,s,l,"throw",e)}s(void 0)}))}}},function(e,n,t){"use strict";var o=t(51),a=t(8),r=t(31);e.exports=function(e,n,t){var i=o(n);i in e?a.f(e,i,r(0,t)):e[i]=t}},function(e,n,t){var o=t(2),a=t(1),r=t(37),i=a("species");e.exports=function(e){return r>=51||!o((function(){var n=[];return(n.constructor={})[i]=function(){return{foo:1}},1!==n[e](Boolean).foo}))}},function(e,n,t){t(136)("iterator")},function(e,n,t){var o=t(217),a=t(218),r=t(219),i=t(220),s=t(221);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var o=e[n];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=r,l.prototype.has=i,l.prototype.set=s,e.exports=l},function(e,n,t){var o=t(142);e.exports=function(e,n){for(var t=e.length;t--;)if(o(e[t][0],n))return t;return-1}},function(e,n,t){var o=t(22)(Object,"create");e.exports=o},function(e,n,t){var o=t(239);e.exports=function(e,n){var t=e.__data__;return o(n)?t["string"==typeof n?"string":"hash"]:t.map}},function(e,n,t){var o=t(92);e.exports=function(e){if("string"==typeof e||o(e))return e;var n=e+"";return"0"==n&&1/e==-1/0?"-0":n}},function(e,n,t){"use strict";t.d(n,"a",(function(){return a}));t(49),t(9),t(84),t(137),t(11);var o=t(43);function a(e,n){if(e){if("string"==typeof e)return Object(o.a)(e,n);var t=Object.prototype.toString.call(e).slice(8,-1);return"Object"===t&&e.constructor&&(t=e.constructor.name),"Map"===t||"Set"===t?Array.from(e):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(o.a)(e,n):void 0}}},function(e,n,t){var o,a;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(a="function"==typeof(o=function(){var e,n,t={version:"0.2.0"},o=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function a(e,n,t){return e<n?n:e>t?t:e}function r(e){return 100*(-1+e)}t.configure=function(e){var n,t;for(n in e)void 0!==(t=e[n])&&e.hasOwnProperty(n)&&(o[n]=t);return this},t.status=null,t.set=function(e){var n=t.isStarted();e=a(e,o.minimum,1),t.status=1===e?null:e;var l=t.render(!n),u=l.querySelector(o.barSelector),d=o.speed,c=o.easing;return l.offsetWidth,i((function(n){""===o.positionUsing&&(o.positionUsing=t.getPositioningCSS()),s(u,function(e,n,t){var a;return(a="translate3d"===o.positionUsing?{transform:"translate3d("+r(e)+"%,0,0)"}:"translate"===o.positionUsing?{transform:"translate("+r(e)+"%,0)"}:{"margin-left":r(e)+"%"}).transition="all "+n+"ms "+t,a}(e,d,c)),1===e?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),n()}),d)}),d)):setTimeout(n,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var e=function(){setTimeout((function(){t.status&&(t.trickle(),e())}),o.trickleSpeed)};return o.trickle&&e(),this},t.done=function(e){return e||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(e){var n=t.status;return n?("number"!=typeof e&&(e=(1-n)*a(Math.random()*n,.1,.95)),n=a(n+e,0,.994),t.set(n)):t.start()},t.trickle=function(){return t.inc(Math.random()*o.trickleRate)},e=0,n=0,t.promise=function(o){return o&&"resolved"!==o.state()?(0===n&&t.start(),e++,n++,o.always((function(){0==--n?(e=0,t.done()):t.set((e-n)/e)})),this):this},t.render=function(e){if(t.isRendered())return document.getElementById("nprogress");u(document.documentElement,"nprogress-busy");var n=document.createElement("div");n.id="nprogress",n.innerHTML=o.template;var a,i=n.querySelector(o.barSelector),l=e?"-100":r(t.status||0),d=document.querySelector(o.parent);return s(i,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),o.showSpinner||(a=n.querySelector(o.spinnerSelector))&&h(a),d!=document.body&&u(d,"nprogress-custom-parent"),d.appendChild(n),n},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(o.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&h(e)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var e=document.body.style,n="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return n+"Perspective"in e?"translate3d":n+"Transform"in e?"translate":"margin"};var i=function(){var e=[];function n(){var t=e.shift();t&&t(n)}return function(t){e.push(t),1==e.length&&n()}}(),s=function(){var e=["Webkit","O","Moz","ms"],n={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,n){return n.toUpperCase()})),n[t]||(n[t]=function(n){var t=document.body.style;if(n in t)return n;for(var o,a=e.length,r=n.charAt(0).toUpperCase()+n.slice(1);a--;)if((o=e[a]+r)in t)return o;return n}(t))}function o(e,n,o){n=t(n),e.style[n]=o}return function(e,n){var t,a,r=arguments;if(2==r.length)for(t in n)void 0!==(a=n[t])&&n.hasOwnProperty(t)&&o(e,t,a);else o(e,r[1],r[2])}}();function l(e,n){return("string"==typeof e?e:c(e)).indexOf(" "+n+" ")>=0}function u(e,n){var t=c(e),o=t+n;l(t,n)||(e.className=o.substring(1))}function d(e,n){var t,o=c(e);l(e,n)&&(t=o.replace(" "+n+" "," "),e.className=t.substring(1,t.length-1))}function c(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function h(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return t})?o.call(n,t,n,e):o)||(e.exports=a)},function(e,n,t){"use strict";var o=t(0),a=t(29).map;o({target:"Array",proto:!0,forced:!t(58)("map")},{map:function(e){return a(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var o=t(166),a=t(5),r=t(13),i=t(46),s=t(23),l=t(174),u=t(208),d=t(167),c=Math.max,h=Math.min;o("replace",2,(function(e,n,t,o){var p=o.REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE,y=o.REPLACE_KEEPS_$0,m=p?"$":"$0";return[function(t,o){var a=s(this),r=null==t?void 0:t[e];return void 0!==r?r.call(t,a,o):n.call(String(a),t,o)},function(e,o){if(!p&&y||"string"==typeof o&&-1===o.indexOf(m)){var s=t(n,e,this,o);if(s.done)return s.value}var g=a(e),b=String(this),f="function"==typeof o;f||(o=String(o));var w=g.global;if(w){var v=g.unicode;g.lastIndex=0}for(var k=[];;){var q=d(g,b);if(null===q)break;if(k.push(q),!w)break;""===String(q[0])&&(g.lastIndex=l(b,r(g.lastIndex),v))}for(var x,j="",S=0,T=0;T<k.length;T++){q=k[T];for(var I=String(q[0]),P=c(h(i(q.index),b.length),0),Q=[],z=1;z<q.length;z++)Q.push(void 0===(x=q[z])?x:String(x));var C=q.groups;if(f){var A=[I].concat(Q,P,b);void 0!==C&&A.push(C);var E=String(o.apply(void 0,A))}else E=u(I,b,P,Q,C,o);P>=S&&(j+=b.slice(S,P)+E,S=P+I.length)}return j+b.slice(S)}]}))},function(e,n,t){var o=t(113),a=t(78).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(e){return o(e,a)}},function(e,n,t){var o=t(5),a=t(190);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,n=!1,t={};try{(e=Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set).call(t,[]),n=t instanceof Array}catch(e){}return function(t,r){return o(t),a(r),n?e.call(t,r):t.__proto__=r,t}}():void 0)},function(e,n,t){"use strict";var o,a,r=t(172),i=t(173),s=t(50),l=RegExp.prototype.exec,u=s("native-string-replace",String.prototype.replace),d=l,c=(o=/a/,a=/b*/g,l.call(o,"a"),l.call(a,"a"),0!==o.lastIndex||0!==a.lastIndex),h=i.UNSUPPORTED_Y||i.BROKEN_CARET,p=void 0!==/()??/.exec("")[1];(c||p||h)&&(d=function(e){var n,t,o,a,i=this,s=h&&i.sticky,d=r.call(i),y=i.source,m=0,g=e;return s&&(-1===(d=d.replace("y","")).indexOf("g")&&(d+="g"),g=String(e).slice(i.lastIndex),i.lastIndex>0&&(!i.multiline||i.multiline&&"\n"!==e[i.lastIndex-1])&&(y="(?: "+y+")",g=" "+g,m++),t=new RegExp("^(?:"+y+")",d)),p&&(t=new RegExp("^"+y+"$(?!\\s)",d)),c&&(n=i.lastIndex),o=l.call(s?t:i,g),s?o?(o.input=o.input.slice(m),o[0]=o[0].slice(m),o.index=i.lastIndex,i.lastIndex+=o[0].length):i.lastIndex=0:c&&o&&(i.lastIndex=i.global?o.index+o[0].length:n),p&&o&&o.length>1&&u.call(o[0],t,(function(){for(a=1;a<arguments.length-2;a++)void 0===arguments[a]&&(o[a]=void 0)})),o}),e.exports=d},function(e,n,t){var o=t(3),a=t(73),r=o["__core-js_shared__"]||a("__core-js_shared__",{});e.exports=r},function(e,n,t){var o=t(3),a=t(12);e.exports=function(e,n){try{a(o,e,n)}catch(t){o[e]=n}return n}},function(e,n,t){var o=t(3),a=t(4),r=o.document,i=a(r)&&a(r.createElement);e.exports=function(e){return i?r.createElement(e):{}}},function(e,n,t){var o=t(37),a=t(2);e.exports=!!Object.getOwnPropertySymbols&&!a((function(){var e=Symbol();return!String(e)||!(Object(e)instanceof Symbol)||!Symbol.sham&&o&&o<41}))},function(e,n,t){var o=t(19);e.exports=o("navigator","userAgent")||""},function(e,n,t){var o=t(15),a=t(13),r=t(114),i=function(e){return function(n,t,i){var s,l=o(n),u=a(l.length),d=r(i,u);if(e&&t!=t){for(;u>d;)if((s=l[d++])!=s)return!0}else for(;u>d;d++)if((e||d in l)&&l[d]===t)return e||d||0;return!e&&-1}};e.exports={includes:i(!0),indexOf:i(!1)}},function(e,n){e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,n,t){var o=t(72),a=Function.toString;"function"!=typeof o.inspectSource&&(o.inspectSource=function(e){return a.call(e)}),e.exports=o.inspectSource},function(e,n,t){"use strict";var o={}.propertyIsEnumerable,a=Object.getOwnPropertyDescriptor,r=a&&!o.call({1:2},1);n.f=r?function(e){var n=a(this,e);return!!n&&n.enumerable}:o},function(e,n){n.f=Object.getOwnPropertySymbols},function(e,n,t){var o=t(7),a=t(10),r=t(54),i=t(120),s=r("IE_PROTO"),l=Object.prototype;e.exports=i?Object.getPrototypeOf:function(e){return e=a(e),o(e,s)?e[s]:"function"==typeof e.constructor&&e instanceof e.constructor?e.constructor.prototype:e instanceof Object?l:null}},function(e,n,t){var o={};o[t(1)("toStringTag")]="z",e.exports="[object z]"===String(o)},function(e,n,t){var o=t(6),a=t(8).f,r=Function.prototype,i=r.toString,s=/^\s*function ([^ (]*)/;o&&!("name"in r)&&a(r,"name",{configurable:!0,get:function(){try{return i.call(this).match(s)[1]}catch(e){return""}}})},function(e,n,t){var o=t(211),a=t(25),r=Object.prototype,i=r.hasOwnProperty,s=r.propertyIsEnumerable,l=o(function(){return arguments}())?o:function(e){return a(e)&&i.call(e,"callee")&&!s.call(e,"callee")};e.exports=l},function(e,n,t){var o=t(22)(t(18),"Map");e.exports=o},function(e,n){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},function(e,n,t){var o=t(231),a=t(238),r=t(240),i=t(241),s=t(242);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var o=e[n];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=r,l.prototype.has=i,l.prototype.set=s,e.exports=l},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},function(e,n){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,n,t){var o=t(17),a=t(92),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;e.exports=function(e,n){if(o(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!a(e))||(i.test(e)||!r.test(e)||null!=n&&e in Object(n))}},function(e,n,t){var o=t(35),a=t(25);e.exports=function(e){return"symbol"==typeof e||a(e)&&"[object Symbol]"==o(e)}},function(e,n){e.exports=function(e){return e}},function(e,n,t){"use strict";t.d(n,"a",(function(){return a}));t(101);t(34),t(40),t(9),t(59),t(11),t(16);var o=t(65);function a(e,n){return function(e){if(Array.isArray(e))return e}(e)||function(e,n){var t=null==e?null:"undefined"!=typeof Symbol&&e[Symbol.iterator]||e["@@iterator"];if(null!=t){var o,a,r=[],i=!0,s=!1;try{for(t=t.call(e);!(i=(o=t.next()).done)&&(r.push(o.value),!n||r.length!==n);i=!0);}catch(e){s=!0,a=e}finally{try{i||null==t.return||t.return()}finally{if(s)throw a}}return r}}(e,n)||Object(o.a)(e,n)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(e,n,t){"use strict";var o=t(0),a=t(29).some;o({target:"Array",proto:!0,forced:!t(30)("some")},{some:function(e){return a(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){var o=t(0),a=t(10),r=t(53);o({target:"Object",stat:!0,forced:t(2)((function(){r(1)}))},{keys:function(e){return r(a(e))}})},function(e,n,t){"use strict";var o=t(0),a=t(133);o({target:"Array",proto:!0,forced:[].forEach!=a},{forEach:a})},function(e,n,t){var o=t(3),a=t(129),r=t(133),i=t(12);for(var s in a){var l=o[s],u=l&&l.prototype;if(u&&u.forEach!==r)try{i(u,"forEach",r)}catch(e){u.forEach=r}}},function(e,n,t){var o=t(105),a=t(39),r=t(1)("iterator");e.exports=function(e){if(null!=e)return e[r]||e["@@iterator"]||a[o(e)]}},function(e,n,t){var o=function(e){"use strict";var n=Object.prototype,t=n.hasOwnProperty,o="function"==typeof Symbol?Symbol:{},a=o.iterator||"@@iterator",r=o.asyncIterator||"@@asyncIterator",i=o.toStringTag||"@@toStringTag";function s(e,n,t){return Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}),e[n]}try{s({},"")}catch(e){s=function(e,n,t){return e[n]=t}}function l(e,n,t,o){var a=n&&n.prototype instanceof c?n:c,r=Object.create(a.prototype),i=new x(o||[]);return r._invoke=function(e,n,t){var o="suspendedStart";return function(a,r){if("executing"===o)throw new Error("Generator is already running");if("completed"===o){if("throw"===a)throw r;return S()}for(t.method=a,t.arg=r;;){var i=t.delegate;if(i){var s=v(i,t);if(s){if(s===d)continue;return s}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===o)throw o="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);o="executing";var l=u(e,n,t);if("normal"===l.type){if(o=t.done?"completed":"suspendedYield",l.arg===d)continue;return{value:l.arg,done:t.done}}"throw"===l.type&&(o="completed",t.method="throw",t.arg=l.arg)}}}(e,t,i),r}function u(e,n,t){try{return{type:"normal",arg:e.call(n,t)}}catch(e){return{type:"throw",arg:e}}}e.wrap=l;var d={};function c(){}function h(){}function p(){}var y={};s(y,a,(function(){return this}));var m=Object.getPrototypeOf,g=m&&m(m(j([])));g&&g!==n&&t.call(g,a)&&(y=g);var b=p.prototype=c.prototype=Object.create(y);function f(e){["next","throw","return"].forEach((function(n){s(e,n,(function(e){return this._invoke(n,e)}))}))}function w(e,n){var o;this._invoke=function(a,r){function i(){return new n((function(o,i){!function o(a,r,i,s){var l=u(e[a],e,r);if("throw"!==l.type){var d=l.arg,c=d.value;return c&&"object"==typeof c&&t.call(c,"__await")?n.resolve(c.__await).then((function(e){o("next",e,i,s)}),(function(e){o("throw",e,i,s)})):n.resolve(c).then((function(e){d.value=e,i(d)}),(function(e){return o("throw",e,i,s)}))}s(l.arg)}(a,r,o,i)}))}return o=o?o.then(i,i):i()}}function v(e,n){var t=e.iterator[n.method];if(void 0===t){if(n.delegate=null,"throw"===n.method){if(e.iterator.return&&(n.method="return",n.arg=void 0,v(e,n),"throw"===n.method))return d;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return d}var o=u(t,e.iterator,n.arg);if("throw"===o.type)return n.method="throw",n.arg=o.arg,n.delegate=null,d;var a=o.arg;return a?a.done?(n[e.resultName]=a.value,n.next=e.nextLoc,"return"!==n.method&&(n.method="next",n.arg=void 0),n.delegate=null,d):a:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,d)}function k(e){var n={tryLoc:e[0]};1 in e&&(n.catchLoc=e[1]),2 in e&&(n.finallyLoc=e[2],n.afterLoc=e[3]),this.tryEntries.push(n)}function q(e){var n=e.completion||{};n.type="normal",delete n.arg,e.completion=n}function x(e){this.tryEntries=[{tryLoc:"root"}],e.forEach(k,this),this.reset(!0)}function j(e){if(e){var n=e[a];if(n)return n.call(e);if("function"==typeof e.next)return e;if(!isNaN(e.length)){var o=-1,r=function n(){for(;++o<e.length;)if(t.call(e,o))return n.value=e[o],n.done=!1,n;return n.value=void 0,n.done=!0,n};return r.next=r}}return{next:S}}function S(){return{value:void 0,done:!0}}return h.prototype=p,s(b,"constructor",p),s(p,"constructor",h),h.displayName=s(p,i,"GeneratorFunction"),e.isGeneratorFunction=function(e){var n="function"==typeof e&&e.constructor;return!!n&&(n===h||"GeneratorFunction"===(n.displayName||n.name))},e.mark=function(e){return Object.setPrototypeOf?Object.setPrototypeOf(e,p):(e.__proto__=p,s(e,i,"GeneratorFunction")),e.prototype=Object.create(b),e},e.awrap=function(e){return{__await:e}},f(w.prototype),s(w.prototype,r,(function(){return this})),e.AsyncIterator=w,e.async=function(n,t,o,a,r){void 0===r&&(r=Promise);var i=new w(l(n,t,o,a),r);return e.isGeneratorFunction(t)?i:i.next().then((function(e){return e.done?e.value:i.next()}))},f(b),s(b,i,"Generator"),s(b,a,(function(){return this})),s(b,"toString",(function(){return"[object Generator]"})),e.keys=function(e){var n=[];for(var t in e)n.push(t);return n.reverse(),function t(){for(;n.length;){var o=n.pop();if(o in e)return t.value=o,t.done=!1,t}return t.done=!0,t}},e.values=j,x.prototype={constructor:x,reset:function(e){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(q),!e)for(var n in this)"t"===n.charAt(0)&&t.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=void 0)},stop:function(){this.done=!0;var e=this.tryEntries[0].completion;if("throw"===e.type)throw e.arg;return this.rval},dispatchException:function(e){if(this.done)throw e;var n=this;function o(t,o){return i.type="throw",i.arg=e,n.next=t,o&&(n.method="next",n.arg=void 0),!!o}for(var a=this.tryEntries.length-1;a>=0;--a){var r=this.tryEntries[a],i=r.completion;if("root"===r.tryLoc)return o("end");if(r.tryLoc<=this.prev){var s=t.call(r,"catchLoc"),l=t.call(r,"finallyLoc");if(s&&l){if(this.prev<r.catchLoc)return o(r.catchLoc,!0);if(this.prev<r.finallyLoc)return o(r.finallyLoc)}else if(s){if(this.prev<r.catchLoc)return o(r.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<r.finallyLoc)return o(r.finallyLoc)}}}},abrupt:function(e,n){for(var o=this.tryEntries.length-1;o>=0;--o){var a=this.tryEntries[o];if(a.tryLoc<=this.prev&&t.call(a,"finallyLoc")&&this.prev<a.finallyLoc){var r=a;break}}r&&("break"===e||"continue"===e)&&r.tryLoc<=n&&n<=r.finallyLoc&&(r=null);var i=r?r.completion:{};return i.type=e,i.arg=n,r?(this.method="next",this.next=r.finallyLoc,d):this.complete(i)},complete:function(e,n){if("throw"===e.type)throw e.arg;return"break"===e.type||"continue"===e.type?this.next=e.arg:"return"===e.type?(this.rval=this.arg=e.arg,this.method="return",this.next="end"):"normal"===e.type&&n&&(this.next=n),d},finish:function(e){for(var n=this.tryEntries.length-1;n>=0;--n){var t=this.tryEntries[n];if(t.finallyLoc===e)return this.complete(t.completion,t.afterLoc),q(t),d}},catch:function(e){for(var n=this.tryEntries.length-1;n>=0;--n){var t=this.tryEntries[n];if(t.tryLoc===e){var o=t.completion;if("throw"===o.type){var a=o.arg;q(t)}return a}}throw new Error("illegal catch attempt")},delegateYield:function(e,n,t){return this.delegate={iterator:j(e),resultName:n,nextLoc:t},"next"===this.method&&(this.arg=void 0),d}},e}(e.exports);try{regeneratorRuntime=o}catch(e){Function("r","regeneratorRuntime = r")(o)}},function(e,n,t){t(0)({target:"Array",stat:!0},{isArray:t(33)})},function(e,n,t){"use strict";var o=t(15),a=t(103),r=t(39),i=t(28),s=t(116),l=i.set,u=i.getterFor("Array Iterator");e.exports=s(Array,"Array",(function(e,n){l(this,{type:"Array Iterator",target:o(e),index:0,kind:n})}),(function(){var e=u(this),n=e.target,t=e.kind,o=e.index++;return!n||o>=n.length?(e.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:o,done:!1}:"values"==t?{value:n[o],done:!1}:{value:[o,n[o]],done:!1}}),"values"),r.Arguments=r.Array,a("keys"),a("values"),a("entries")},function(e,n,t){var o=t(1),a=t(32),r=t(8),i=o("unscopables"),s=Array.prototype;null==s[i]&&r.f(s,i,{configurable:!0,value:a(null)}),e.exports=function(e){s[i][e]=!0}},function(e,n,t){var o=t(2),a=/#|\.prototype\./,r=function(e,n){var t=s[i(e)];return t==u||t!=l&&("function"==typeof n?o(n):!!n)},i=r.normalize=function(e){return String(e).replace(a,".").toLowerCase()},s=r.data={},l=r.NATIVE="N",u=r.POLYFILL="P";e.exports=r},function(e,n,t){var o=t(83),a=t(26),r=t(1)("toStringTag"),i="Arguments"==a(function(){return arguments}());e.exports=o?a:function(e){var n,t,o;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(t=function(e,n){try{return e[n]}catch(e){}}(n=Object(e),r))?t:i?a(n):"Object"==(o=a(n))&&"function"==typeof n.callee?"Arguments":o}},function(e,n,t){var o=t(5),a=t(21),r=t(1)("species");e.exports=function(e,n){var t,i=o(e).constructor;return void 0===i||null==(t=o(i)[r])?n:a(t)}},function(e,n,t){var o=t(46),a=t(23),r=function(e){return function(n,t){var r,i,s=String(a(n)),l=o(t),u=s.length;return l<0||l>=u?e?"":void 0:(r=s.charCodeAt(l))<55296||r>56319||l+1===u||(i=s.charCodeAt(l+1))<56320||i>57343?e?s.charAt(l):r:e?s.slice(l,l+2):i-56320+(r-55296<<10)+65536}};e.exports={codeAt:r(!1),charAt:r(!0)}},function(e,n,t){var o=t(4),a=t(33),r=t(1)("species");e.exports=function(e,n){var t;return a(e)&&("function"!=typeof(t=e.constructor)||t!==Array&&!a(t.prototype)?o(t)&&null===(t=t[r])&&(t=void 0):t=void 0),new(void 0===t?Array:t)(0===n?0:n)}},function(e,n){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,n,t){var o=t(6),a=t(2),r=t(74);e.exports=!o&&!a((function(){return 7!=Object.defineProperty(r("div"),"a",{get:function(){return 7}}).a}))},function(e,n,t){var o=t(3);e.exports=o},function(e,n,t){var o=t(75);e.exports=o&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,n,t){var o=t(7),a=t(15),r=t(77).indexOf,i=t(38);e.exports=function(e,n){var t,s=a(e),l=0,u=[];for(t in s)!o(i,t)&&o(s,t)&&u.push(t);for(;n.length>l;)o(s,t=n[l++])&&(~r(u,t)||u.push(t));return u}},function(e,n,t){var o=t(46),a=Math.max,r=Math.min;e.exports=function(e,n){var t=o(e);return t<0?a(t+n,0):r(t,n)}},function(e,n,t){var o=t(19);e.exports=o("document","documentElement")},function(e,n,t){"use strict";var o=t(0),a=t(178),r=t(82),i=t(70),s=t(47),l=t(12),u=t(14),d=t(1),c=t(20),h=t(39),p=t(119),y=p.IteratorPrototype,m=p.BUGGY_SAFARI_ITERATORS,g=d("iterator"),b=function(){return this};e.exports=function(e,n,t,d,p,f,w){a(t,n,d);var v,k,q,x=function(e){if(e===p&&P)return P;if(!m&&e in T)return T[e];switch(e){case"keys":case"values":case"entries":return function(){return new t(this,e)}}return function(){return new t(this)}},j=n+" Iterator",S=!1,T=e.prototype,I=T[g]||T["@@iterator"]||p&&T[p],P=!m&&I||x(p),Q="Array"==n&&T.entries||I;if(Q&&(v=r(Q.call(new e)),y!==Object.prototype&&v.next&&(c||r(v)===y||(i?i(v,y):"function"!=typeof v[g]&&l(v,g,b)),s(v,j,!0,!0),c&&(h[j]=b))),"values"==p&&I&&"values"!==I.name&&(S=!0,P=function(){return I.call(this)}),c&&!w||T[g]===P||l(T,g,P),h[n]=P,p)if(k={values:x("values"),keys:f?P:x("keys"),entries:x("entries")},w)for(q in k)(m||S||!(q in T))&&u(T,q,k[q]);else o({target:n,proto:!0,forced:m||S},k);return k}},function(e,n,t){var o=t(7),a=t(118),r=t(24),i=t(8);e.exports=function(e,n){for(var t=a(n),s=i.f,l=r.f,u=0;u<t.length;u++){var d=t[u];o(e,d)||s(e,d,l(n,d))}}},function(e,n,t){var o=t(19),a=t(69),r=t(81),i=t(5);e.exports=o("Reflect","ownKeys")||function(e){var n=a.f(i(e)),t=r.f;return t?n.concat(t(e)):n}},function(e,n,t){"use strict";var o,a,r,i=t(2),s=t(82),l=t(12),u=t(7),d=t(1),c=t(20),h=d("iterator"),p=!1;[].keys&&("next"in(r=[].keys())?(a=s(s(r)))!==Object.prototype&&(o=a):p=!0);var y=null==o||i((function(){var e={};return o[h].call(e)!==e}));y&&(o={}),c&&!y||u(o,h)||l(o,h,(function(){return this})),e.exports={IteratorPrototype:o,BUGGY_SAFARI_ITERATORS:p}},function(e,n,t){var o=t(2);e.exports=!o((function(){function e(){}return e.prototype.constructor=null,Object.getPrototypeOf(new e)!==e.prototype}))},function(e,n,t){var o=t(3);e.exports=o.Promise},function(e,n,t){var o=t(1),a=t(39),r=o("iterator"),i=Array.prototype;e.exports=function(e){return void 0!==e&&(a.Array===e||i[r]===e)}},function(e,n,t){var o=t(5);e.exports=function(e){var n=e.return;if(void 0!==n)return o(n.call(e)).value}},function(e,n,t){var o=t(1)("iterator"),a=!1;try{var r=0,i={next:function(){return{done:!!r++}},return:function(){a=!0}};i[o]=function(){return this},Array.from(i,(function(){throw 2}))}catch(e){}e.exports=function(e,n){if(!n&&!a)return!1;var t=!1;try{var r={};r[o]=function(){return{next:function(){return{done:t=!0}}}},e(r)}catch(e){}return t}},function(e,n,t){var o,a,r,i=t(3),s=t(2),l=t(48),u=t(115),d=t(74),c=t(126),h=t(55),p=i.location,y=i.setImmediate,m=i.clearImmediate,g=i.process,b=i.MessageChannel,f=i.Dispatch,w=0,v={},k=function(e){if(v.hasOwnProperty(e)){var n=v[e];delete v[e],n()}},q=function(e){return function(){k(e)}},x=function(e){k(e.data)},j=function(e){i.postMessage(e+"",p.protocol+"//"+p.host)};y&&m||(y=function(e){for(var n=[],t=1;arguments.length>t;)n.push(arguments[t++]);return v[++w]=function(){("function"==typeof e?e:Function(e)).apply(void 0,n)},o(w),w},m=function(e){delete v[e]},h?o=function(e){g.nextTick(q(e))}:f&&f.now?o=function(e){f.now(q(e))}:b&&!c?(r=(a=new b).port2,a.port1.onmessage=x,o=l(r.postMessage,r,1)):i.addEventListener&&"function"==typeof postMessage&&!i.importScripts&&p&&"file:"!==p.protocol&&!s(j)?(o=j,i.addEventListener("message",x,!1)):o="onreadystatechange"in d("script")?function(e){u.appendChild(d("script")).onreadystatechange=function(){u.removeChild(this),k(e)}}:function(e){setTimeout(q(e),0)}),e.exports={set:y,clear:m}},function(e,n,t){var o=t(76);e.exports=/(?:iphone|ipod|ipad).*applewebkit/i.test(o)},function(e,n,t){var o=t(5),a=t(4),r=t(128);e.exports=function(e,n){if(o(e),a(n)&&n.constructor===e)return n;var t=r.f(e);return(0,t.resolve)(n),t.promise}},function(e,n,t){"use strict";var o=t(21),a=function(e){var n,t;this.promise=new e((function(e,o){if(void 0!==n||void 0!==t)throw TypeError("Bad Promise constructor");n=e,t=o})),this.resolve=o(n),this.reject=o(t)};e.exports.f=function(e){return new a(e)}},function(e,n){e.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(e,n,t){var o=t(0),a=t(2),r=t(10),i=t(82),s=t(120);o({target:"Object",stat:!0,forced:a((function(){i(1)})),sham:!s},{getPrototypeOf:function(e){return i(r(e))}})},function(e,n,t){var o=t(171);e.exports=function(e){if(o(e))throw TypeError("The method doesn't accept regular expressions");return e}},function(e,n,t){var o=t(1)("match");e.exports=function(e){var n=/./;try{"/./"[e](n)}catch(t){try{return n[o]=!1,"/./"[e](n)}catch(e){}}return!1}},function(e,n,t){"use strict";var o=t(29).forEach,a=t(30)("forEach");e.exports=a?[].forEach:function(e){return o(this,e,arguments.length>1?arguments[1]:void 0)}},function(e,n,t){var o=t(2);e.exports=!o((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(e,n,t){var o=t(1);n.f=o},function(e,n,t){var o=t(111),a=t(7),r=t(135),i=t(8).f;e.exports=function(e){var n=o.Symbol||(o.Symbol={});a(n,e)||i(n,e,{value:r.f(e)})}},function(e,n,t){var o=t(0),a=t(183);o({target:"Array",stat:!0,forced:!t(124)((function(e){Array.from(e)}))},{from:a})},function(e,n,t){t(0)({target:"Object",stat:!0,sham:!t(6)},{create:t(32)})},function(e,n){e.exports=function(e,n){for(var t=-1,o=n.length,a=e.length;++t<o;)e[a+t]=n[t];return e}},function(e,n){var t="object"==typeof global&&global&&global.Object===Object&&global;e.exports=t},function(e,n,t){var o=t(60),a=t(222),r=t(223),i=t(224),s=t(225),l=t(226);function u(e){var n=this.__data__=new o(e);this.size=n.size}u.prototype.clear=a,u.prototype.delete=r,u.prototype.get=i,u.prototype.has=s,u.prototype.set=l,e.exports=u},function(e,n){e.exports=function(e,n){return e===n||e!=e&&n!=n}},function(e,n,t){var o=t(35),a=t(87);e.exports=function(e){if(!a(e))return!1;var n=o(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},function(e,n){var t=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return t.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,n,t){var o=t(243),a=t(25);e.exports=function e(n,t,r,i,s){return n===t||(null==n||null==t||!a(n)&&!a(t)?n!=n&&t!=t:o(n,t,r,i,e,s))}},function(e,n,t){var o=t(147),a=t(246),r=t(148);e.exports=function(e,n,t,i,s,l){var u=1&t,d=e.length,c=n.length;if(d!=c&&!(u&&c>d))return!1;var h=l.get(e),p=l.get(n);if(h&&p)return h==n&&p==e;var y=-1,m=!0,g=2&t?new o:void 0;for(l.set(e,n),l.set(n,e);++y<d;){var b=e[y],f=n[y];if(i)var w=u?i(f,b,y,n,e,l):i(b,f,y,e,n,l);if(void 0!==w){if(w)continue;m=!1;break}if(g){if(!a(n,(function(e,n){if(!r(g,n)&&(b===e||s(b,e,t,i,l)))return g.push(n)}))){m=!1;break}}else if(b!==f&&!s(b,f,t,i,l)){m=!1;break}}return l.delete(e),l.delete(n),m}},function(e,n,t){var o=t(88),a=t(244),r=t(245);function i(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new o;++n<t;)this.add(e[n])}i.prototype.add=i.prototype.push=a,i.prototype.has=r,e.exports=i},function(e,n){e.exports=function(e,n){return e.has(n)}},function(e,n,t){var o=t(256),a=t(262),r=t(153);e.exports=function(e){return r(e)?o(e):a(e)}},function(e,n,t){(function(e){var o=t(18),a=t(258),r=n&&!n.nodeType&&n,i=r&&"object"==typeof e&&e&&!e.nodeType&&e,s=i&&i.exports===r?o.Buffer:void 0,l=(s?s.isBuffer:void 0)||a;e.exports=l}).call(this,t(109)(e))},function(e,n){var t=/^(?:0|[1-9]\d*)$/;e.exports=function(e,n){var o=typeof e;return!!(n=null==n?9007199254740991:n)&&("number"==o||"symbol"!=o&&t.test(e))&&e>-1&&e%1==0&&e<n}},function(e,n,t){var o=t(259),a=t(260),r=t(261),i=r&&r.isTypedArray,s=i?a(i):o;e.exports=s},function(e,n,t){var o=t(143),a=t(90);e.exports=function(e){return null!=e&&a(e.length)&&!o(e)}},function(e,n,t){var o=t(22)(t(18),"Set");e.exports=o},function(e,n,t){var o=t(87);e.exports=function(e){return e==e&&!o(e)}},function(e,n){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},function(e,n,t){var o=t(158),a=t(64);e.exports=function(e,n){for(var t=0,r=(n=o(n,e)).length;null!=e&&t<r;)e=e[a(n[t++])];return t&&t==r?e:void 0}},function(e,n,t){var o=t(17),a=t(91),r=t(273),i=t(276);e.exports=function(e,n){return o(e)?e:a(e,n)?[e]:r(i(e))}},function(e,n,t){t(0)({target:"Object",stat:!0},{setPrototypeOf:t(70)})},function(e,n,t){var o=t(0),a=t(19),r=t(21),i=t(5),s=t(4),l=t(32),u=t(305),d=t(2),c=a("Reflect","construct"),h=d((function(){function e(){}return!(c((function(){}),[],e)instanceof e)})),p=!d((function(){c((function(){}))})),y=h||p;o({target:"Reflect",stat:!0,forced:y,sham:y},{construct:function(e,n){r(e),i(n);var t=arguments.length<3?e:r(arguments[2]);if(p&&!h)return c(e,n,t);if(e==t){switch(n.length){case 0:return new e;case 1:return new e(n[0]);case 2:return new e(n[0],n[1]);case 3:return new e(n[0],n[1],n[2]);case 4:return new e(n[0],n[1],n[2],n[3])}var o=[null];return o.push.apply(o,n),new(u.apply(e,o))}var a=t.prototype,d=l(s(a)?a:Object.prototype),y=Function.apply.call(e,d,n);return s(y)?y:d}})},function(e,n,t){},function(e,n,t){},function(e,n,t){var o=t(209),a=t(214),r=t(285),i=t(293),s=t(302),l=t(186),u=r((function(e){var n=l(e);return s(n)&&(n=void 0),i(o(e,1,s,!0),a(n,2))}));e.exports=u},function(e,n){var t=/^\s+|\s+$/g,o=/^[-+]0x[0-9a-f]+$/i,a=/^0b[01]+$/i,r=/^0o[0-7]+$/i,i=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,u=s||l||Function("return this")(),d=Object.prototype.toString,c=Math.max,h=Math.min,p=function(){return u.Date.now()};function y(e){var n=typeof e;return!!e&&("object"==n||"function"==n)}function m(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(y(e)){var n="function"==typeof e.valueOf?e.valueOf():e;e=y(n)?n+"":n}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(t,"");var s=a.test(e);return s||r.test(e)?i(e.slice(2),s?2:8):o.test(e)?NaN:+e}e.exports=function(e,n,t){var o,a,r,i,s,l,u=0,d=!1,g=!1,b=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function f(n){var t=o,r=a;return o=a=void 0,u=n,i=e.apply(r,t)}function w(e){return u=e,s=setTimeout(k,n),d?f(e):i}function v(e){var t=e-l;return void 0===l||t>=n||t<0||g&&e-u>=r}function k(){var e=p();if(v(e))return q(e);s=setTimeout(k,function(e){var t=n-(e-l);return g?h(t,r-(e-u)):t}(e))}function q(e){return s=void 0,b&&o?f(e):(o=a=void 0,i)}function x(){var e=p(),t=v(e);if(o=arguments,a=this,l=e,t){if(void 0===s)return w(l);if(g)return s=setTimeout(k,n),f(l)}return void 0===s&&(s=setTimeout(k,n)),i}return n=m(n)||0,y(t)&&(d=!!t.leading,r=(g="maxWait"in t)?c(m(t.maxWait)||0,n):r,b="trailing"in t?!!t.trailing:b),x.cancel=function(){void 0!==s&&clearTimeout(s),u=0,o=l=a=s=void 0},x.flush=function(){return void 0===s?i:q(p())},x}},function(e,n,t){function o(n){return"function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?(e.exports=o=function(e){return typeof e},e.exports.default=e.exports,e.exports.__esModule=!0):(e.exports=o=function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},e.exports.default=e.exports,e.exports.__esModule=!0),o(n)}t(34),t(40),t(9),t(59),t(11),t(16),e.exports=o,e.exports.default=e.exports,e.exports.__esModule=!0},function(e,n,t){"use strict";t(45);var o=t(14),a=t(71),r=t(2),i=t(1),s=t(12),l=i("species"),u=RegExp.prototype,d=!r((function(){var e=/./;return e.exec=function(){var e=[];return e.groups={a:"7"},e},"7"!=="".replace(e,"$<a>")})),c="$0"==="a".replace(/./,"$0"),h=i("replace"),p=!!/./[h]&&""===/./[h]("a","$0"),y=!r((function(){var e=/(?:)/,n=e.exec;e.exec=function(){return n.apply(this,arguments)};var t="ab".split(e);return 2!==t.length||"a"!==t[0]||"b"!==t[1]}));e.exports=function(e,n,t,h){var m=i(e),g=!r((function(){var n={};return n[m]=function(){return 7},7!=""[e](n)})),b=g&&!r((function(){var n=!1,t=/a/;return"split"===e&&((t={}).constructor={},t.constructor[l]=function(){return t},t.flags="",t[m]=/./[m]),t.exec=function(){return n=!0,null},t[m](""),!n}));if(!g||!b||"replace"===e&&(!d||!c||p)||"split"===e&&!y){var f=/./[m],w=t(m,""[e],(function(e,n,t,o,r){var i=n.exec;return i===a||i===u.exec?g&&!r?{done:!0,value:f.call(n,t,o)}:{done:!0,value:e.call(t,n,o)}:{done:!1}}),{REPLACE_KEEPS_$0:c,REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE:p}),v=w[0],k=w[1];o(String.prototype,e,v),o(u,m,2==n?function(e,n){return k.call(e,this,n)}:function(e){return k.call(e,this)})}h&&s(u[m],"sham",!0)}},function(e,n,t){var o=t(26),a=t(71);e.exports=function(e,n){var t=e.exec;if("function"==typeof t){var r=t.call(e,n);if("object"!=typeof r)throw TypeError("RegExp exec method returned something other than an Object or null");return r}if("RegExp"!==o(e))throw TypeError("RegExp#exec called on incompatible receiver");return a.call(e,n)}},function(e,n,t){"use strict";var o=t(0),a=t(77).indexOf,r=t(30),i=[].indexOf,s=!!i&&1/[1].indexOf(1,-0)<0,l=r("indexOf");o({target:"Array",proto:!0,forced:s||!l},{indexOf:function(e){return s?i.apply(this,arguments)||0:a(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){var o=t(6),a=t(8),r=t(5),i=t(53);e.exports=o?Object.defineProperties:function(e,n){r(e);for(var t,o=i(n),s=o.length,l=0;s>l;)a.f(e,t=o[l++],n[t]);return e}},function(e,n){e.exports=function(e,n,t){if(!(e instanceof n))throw TypeError("Incorrect "+(t?t+" ":"")+"invocation");return e}},function(e,n,t){var o=t(4),a=t(26),r=t(1)("match");e.exports=function(e){var n;return o(e)&&(void 0!==(n=e[r])?!!n:"RegExp"==a(e))}},function(e,n,t){"use strict";var o=t(5);e.exports=function(){var e=o(this),n="";return e.global&&(n+="g"),e.ignoreCase&&(n+="i"),e.multiline&&(n+="m"),e.dotAll&&(n+="s"),e.unicode&&(n+="u"),e.sticky&&(n+="y"),n}},function(e,n,t){"use strict";var o=t(2);function a(e,n){return RegExp(e,n)}n.UNSUPPORTED_Y=o((function(){var e=a("a","y");return e.lastIndex=2,null!=e.exec("abcd")})),n.BROKEN_CARET=o((function(){var e=a("^r","gy");return e.lastIndex=2,null!=e.exec("str")}))},function(e,n,t){"use strict";var o=t(107).charAt;e.exports=function(e,n,t){return n+(t?o(e,n).length:1)}},function(e,n,t){"use strict";var o=t(0),a=t(36),r=t(15),i=t(30),s=[].join,l=a!=Object,u=i("join",",");o({target:"Array",proto:!0,forced:l||!u},{join:function(e){return s.call(r(this),void 0===e?",":e)}})},function(e,n,t){"use strict";var o=t(0),a=t(2),r=t(33),i=t(4),s=t(10),l=t(13),u=t(57),d=t(108),c=t(58),h=t(1),p=t(37),y=h("isConcatSpreadable"),m=p>=51||!a((function(){var e=[];return e[y]=!1,e.concat()[0]!==e})),g=c("concat"),b=function(e){if(!i(e))return!1;var n=e[y];return void 0!==n?!!n:r(e)};o({target:"Array",proto:!0,forced:!m||!g},{concat:function(e){var n,t,o,a,r,i=s(this),c=d(i,0),h=0;for(n=-1,o=arguments.length;n<o;n++)if(b(r=-1===n?i:arguments[n])){if(h+(a=l(r.length))>9007199254740991)throw TypeError("Maximum allowed index exceeded");for(t=0;t<a;t++,h++)t in r&&u(c,h,r[t])}else{if(h>=9007199254740991)throw TypeError("Maximum allowed index exceeded");u(c,h++,r)}return c.length=h,c}})},function(e,n,t){var o=t(0),a=t(6);o({target:"Object",stat:!0,forced:!a,sham:!a},{defineProperty:t(8).f})},function(e,n,t){"use strict";var o=t(119).IteratorPrototype,a=t(32),r=t(31),i=t(47),s=t(39),l=function(){return this};e.exports=function(e,n,t){var u=n+" Iterator";return e.prototype=a(o,{next:r(1,t)}),i(e,u,!1,!0),s[u]=l,e}},function(e,n,t){var o=t(14);e.exports=function(e,n,t){for(var a in n)o(e,a,n[a],t);return e}},function(e,n,t){"use strict";var o=t(19),a=t(8),r=t(1),i=t(6),s=r("species");e.exports=function(e){var n=o(e),t=a.f;i&&n&&!n[s]&&t(n,s,{configurable:!0,get:function(){return this}})}},function(e,n,t){"use strict";var o=t(6),a=t(2),r=t(53),i=t(81),s=t(80),l=t(10),u=t(36),d=Object.assign,c=Object.defineProperty;e.exports=!d||a((function(){if(o&&1!==d({b:1},d(c({},"a",{enumerable:!0,get:function(){c(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var e={},n={},t=Symbol();return e[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(e){n[e]=e})),7!=d({},e)[t]||"abcdefghijklmnopqrst"!=r(d({},n)).join("")}))?function(e,n){for(var t=l(e),a=arguments.length,d=1,c=i.f,h=s.f;a>d;)for(var p,y=u(arguments[d++]),m=c?r(y).concat(c(y)):r(y),g=m.length,b=0;g>b;)p=m[b++],o&&!h.call(y,p)||(t[p]=y[p]);return t}:d},function(e,n,t){var o=t(0),a=t(6),r=t(118),i=t(15),s=t(24),l=t(57);o({target:"Object",stat:!0,sham:!a},{getOwnPropertyDescriptors:function(e){for(var n,t,o=i(e),a=s.f,u=r(o),d={},c=0;u.length>c;)void 0!==(t=a(o,n=u[c++]))&&l(d,n,t);return d}})},function(e,n,t){"use strict";var o=t(48),a=t(10),r=t(207),i=t(122),s=t(13),l=t(57),u=t(99);e.exports=function(e){var n,t,d,c,h,p,y=a(e),m="function"==typeof this?this:Array,g=arguments.length,b=g>1?arguments[1]:void 0,f=void 0!==b,w=u(y),v=0;if(f&&(b=o(b,g>2?arguments[2]:void 0,2)),null==w||m==Array&&i(w))for(t=new m(n=s(y.length));n>v;v++)p=f?b(y[v],v):y[v],l(t,v,p);else for(h=(c=w.call(y)).next,t=new m;!(d=h.call(c)).done;v++)p=f?r(c,b,[d.value,v],!0):d.value,l(t,v,p);return t.length=v,t}},function(e,n,t){"use strict";var o=t(0),a=t(77).includes,r=t(103);o({target:"Array",proto:!0},{includes:function(e){return a(this,e,arguments.length>1?arguments[1]:void 0)}}),r("includes")},function(e,n,t){"use strict";var o=t(0),a=t(131),r=t(23);o({target:"String",proto:!0,forced:!t(132)("includes")},{includes:function(e){return!!~String(r(this)).indexOf(a(e),arguments.length>1?arguments[1]:void 0)}})},function(e,n){e.exports=function(e){var n=null==e?0:e.length;return n?e[n-1]:void 0}},function(e,n,t){var o=t(14),a=Date.prototype,r=a.toString,i=a.getTime;new Date(NaN)+""!="Invalid Date"&&o(a,"toString",(function(){var e=i.call(this);return e==e?r.call(this):"Invalid Date"}))},function(e,n,t){e.exports=t(308)},function(e,n,t){var o=t(3),a=t(79),r=o.WeakMap;e.exports="function"==typeof r&&/native code/.test(a(r))},function(e,n,t){var o=t(4);e.exports=function(e){if(!o(e)&&null!==e)throw TypeError("Can't set "+String(e)+" as a prototype");return e}},function(e,n,t){"use strict";var o,a,r,i,s=t(0),l=t(20),u=t(3),d=t(19),c=t(121),h=t(14),p=t(179),y=t(70),m=t(47),g=t(180),b=t(4),f=t(21),w=t(170),v=t(79),k=t(192),q=t(124),x=t(106),j=t(125).set,S=t(193),T=t(127),I=t(195),P=t(128),Q=t(196),z=t(28),C=t(104),A=t(1),E=t(197),L=t(55),O=t(37),H=A("species"),D="Promise",_=z.get,N=z.set,M=z.getterFor(D),G=c&&c.prototype,R=c,B=G,W=u.TypeError,F=u.document,U=u.process,Y=P.f,V=Y,J=!!(F&&F.createEvent&&u.dispatchEvent),$="function"==typeof PromiseRejectionEvent,K=!1,Z=C(D,(function(){var e=v(R)!==String(R);if(!e&&66===O)return!0;if(l&&!B.finally)return!0;if(O>=51&&/native code/.test(R))return!1;var n=new R((function(e){e(1)})),t=function(e){e((function(){}),(function(){}))};return(n.constructor={})[H]=t,!(K=n.then((function(){}))instanceof t)||!e&&E&&!$})),X=Z||!q((function(e){R.all(e).catch((function(){}))})),ee=function(e){var n;return!(!b(e)||"function"!=typeof(n=e.then))&&n},ne=function(e,n){if(!e.notified){e.notified=!0;var t=e.reactions;S((function(){for(var o=e.value,a=1==e.state,r=0;t.length>r;){var i,s,l,u=t[r++],d=a?u.ok:u.fail,c=u.resolve,h=u.reject,p=u.domain;try{d?(a||(2===e.rejection&&re(e),e.rejection=1),!0===d?i=o:(p&&p.enter(),i=d(o),p&&(p.exit(),l=!0)),i===u.promise?h(W("Promise-chain cycle")):(s=ee(i))?s.call(i,c,h):c(i)):h(o)}catch(e){p&&!l&&p.exit(),h(e)}}e.reactions=[],e.notified=!1,n&&!e.rejection&&oe(e)}))}},te=function(e,n,t){var o,a;J?((o=F.createEvent("Event")).promise=n,o.reason=t,o.initEvent(e,!1,!0),u.dispatchEvent(o)):o={promise:n,reason:t},!$&&(a=u["on"+e])?a(o):"unhandledrejection"===e&&I("Unhandled promise rejection",t)},oe=function(e){j.call(u,(function(){var n,t=e.facade,o=e.value;if(ae(e)&&(n=Q((function(){L?U.emit("unhandledRejection",o,t):te("unhandledrejection",t,o)})),e.rejection=L||ae(e)?2:1,n.error))throw n.value}))},ae=function(e){return 1!==e.rejection&&!e.parent},re=function(e){j.call(u,(function(){var n=e.facade;L?U.emit("rejectionHandled",n):te("rejectionhandled",n,e.value)}))},ie=function(e,n,t){return function(o){e(n,o,t)}},se=function(e,n,t){e.done||(e.done=!0,t&&(e=t),e.value=n,e.state=2,ne(e,!0))},le=function(e,n,t){if(!e.done){e.done=!0,t&&(e=t);try{if(e.facade===n)throw W("Promise can't be resolved itself");var o=ee(n);o?S((function(){var t={done:!1};try{o.call(n,ie(le,t,e),ie(se,t,e))}catch(n){se(t,n,e)}})):(e.value=n,e.state=1,ne(e,!1))}catch(n){se({done:!1},n,e)}}};if(Z&&(B=(R=function(e){w(this,R,D),f(e),o.call(this);var n=_(this);try{e(ie(le,n),ie(se,n))}catch(e){se(n,e)}}).prototype,(o=function(e){N(this,{type:D,done:!1,notified:!1,parent:!1,reactions:[],rejection:!1,state:0,value:void 0})}).prototype=p(B,{then:function(e,n){var t=M(this),o=Y(x(this,R));return o.ok="function"!=typeof e||e,o.fail="function"==typeof n&&n,o.domain=L?U.domain:void 0,t.parent=!0,t.reactions.push(o),0!=t.state&&ne(t,!1),o.promise},catch:function(e){return this.then(void 0,e)}}),a=function(){var e=new o,n=_(e);this.promise=e,this.resolve=ie(le,n),this.reject=ie(se,n)},P.f=Y=function(e){return e===R||e===r?new a(e):V(e)},!l&&"function"==typeof c&&G!==Object.prototype)){i=G.then,K||(h(G,"then",(function(e,n){var t=this;return new R((function(e,n){i.call(t,e,n)})).then(e,n)}),{unsafe:!0}),h(G,"catch",B.catch,{unsafe:!0}));try{delete G.constructor}catch(e){}y&&y(G,B)}s({global:!0,wrap:!0,forced:Z},{Promise:R}),m(R,D,!1,!0),g(D),r=d(D),s({target:D,stat:!0,forced:Z},{reject:function(e){var n=Y(this);return n.reject.call(void 0,e),n.promise}}),s({target:D,stat:!0,forced:l||Z},{resolve:function(e){return T(l&&this===r?R:this,e)}}),s({target:D,stat:!0,forced:X},{all:function(e){var n=this,t=Y(n),o=t.resolve,a=t.reject,r=Q((function(){var t=f(n.resolve),r=[],i=0,s=1;k(e,(function(e){var l=i++,u=!1;r.push(void 0),s++,t.call(n,e).then((function(e){u||(u=!0,r[l]=e,--s||o(r))}),a)})),--s||o(r)}));return r.error&&a(r.value),t.promise},race:function(e){var n=this,t=Y(n),o=t.reject,a=Q((function(){var a=f(n.resolve);k(e,(function(e){a.call(n,e).then(t.resolve,o)}))}));return a.error&&o(a.value),t.promise}})},function(e,n,t){var o=t(5),a=t(122),r=t(13),i=t(48),s=t(99),l=t(123),u=function(e,n){this.stopped=e,this.result=n};e.exports=function(e,n,t){var d,c,h,p,y,m,g,b=t&&t.that,f=!(!t||!t.AS_ENTRIES),w=!(!t||!t.IS_ITERATOR),v=!(!t||!t.INTERRUPTED),k=i(n,b,1+f+v),q=function(e){return d&&l(d),new u(!0,e)},x=function(e){return f?(o(e),v?k(e[0],e[1],q):k(e[0],e[1])):v?k(e,q):k(e)};if(w)d=e;else{if("function"!=typeof(c=s(e)))throw TypeError("Target is not iterable");if(a(c)){for(h=0,p=r(e.length);p>h;h++)if((y=x(e[h]))&&y instanceof u)return y;return new u(!1)}d=c.call(e)}for(m=d.next;!(g=m.call(d)).done;){try{y=x(g.value)}catch(e){throw l(d),e}if("object"==typeof y&&y&&y instanceof u)return y}return new u(!1)}},function(e,n,t){var o,a,r,i,s,l,u,d,c=t(3),h=t(24).f,p=t(125).set,y=t(126),m=t(194),g=t(55),b=c.MutationObserver||c.WebKitMutationObserver,f=c.document,w=c.process,v=c.Promise,k=h(c,"queueMicrotask"),q=k&&k.value;q||(o=function(){var e,n;for(g&&(e=w.domain)&&e.exit();a;){n=a.fn,a=a.next;try{n()}catch(e){throw a?i():r=void 0,e}}r=void 0,e&&e.enter()},y||g||m||!b||!f?v&&v.resolve?((u=v.resolve(void 0)).constructor=v,d=u.then,i=function(){d.call(u,o)}):i=g?function(){w.nextTick(o)}:function(){p.call(c,o)}:(s=!0,l=f.createTextNode(""),new b(o).observe(l,{characterData:!0}),i=function(){l.data=s=!s})),e.exports=q||function(e){var n={fn:e,next:void 0};r&&(r.next=n),a||(a=n,i()),r=n}},function(e,n,t){var o=t(76);e.exports=/web0s(?!.*chrome)/i.test(o)},function(e,n,t){var o=t(3);e.exports=function(e,n){var t=o.console;t&&t.error&&(1===arguments.length?t.error(e):t.error(e,n))}},function(e,n){e.exports=function(e){try{return{error:!1,value:e()}}catch(e){return{error:!0,value:e}}}},function(e,n){e.exports="object"==typeof window},function(e,n,t){var o=t(0),a=t(181);o({target:"Object",stat:!0,forced:Object.assign!==a},{assign:a})},function(e,n,t){"use strict";var o=t(0),a=t(20),r=t(121),i=t(2),s=t(19),l=t(106),u=t(127),d=t(14);if(o({target:"Promise",proto:!0,real:!0,forced:!!r&&i((function(){r.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(e){var n=l(this,s("Promise")),t="function"==typeof e;return this.then(t?function(t){return u(n,e()).then((function(){return t}))}:e,t?function(t){return u(n,e()).then((function(){throw t}))}:e)}}),!a&&"function"==typeof r){var c=s("Promise").prototype.finally;r.prototype.finally!==c&&d(r.prototype,"finally",c,{unsafe:!0})}},function(e,n,t){"use strict";var o=t(83),a=t(105);e.exports=o?{}.toString:function(){return"[object "+a(this)+"]"}},function(e,n,t){"use strict";var o=t(0),a=t(202).left,r=t(30),i=t(37),s=t(55);o({target:"Array",proto:!0,forced:!r("reduce")||!s&&i>79&&i<83},{reduce:function(e){return a(this,e,arguments.length,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){var o=t(21),a=t(10),r=t(36),i=t(13),s=function(e){return function(n,t,s,l){o(t);var u=a(n),d=r(u),c=i(u.length),h=e?c-1:0,p=e?-1:1;if(s<2)for(;;){if(h in d){l=d[h],h+=p;break}if(h+=p,e?h<0:c<=h)throw TypeError("Reduce of empty array with no initial value")}for(;e?h>=0:c>h;h+=p)h in d&&(l=t(l,d[h],h,u));return l}};e.exports={left:s(!1),right:s(!0)}},function(e,n,t){"use strict";var o,a=t(0),r=t(24).f,i=t(13),s=t(131),l=t(23),u=t(132),d=t(20),c="".startsWith,h=Math.min,p=u("startsWith");a({target:"String",proto:!0,forced:!!(d||p||(o=r(String.prototype,"startsWith"),!o||o.writable))&&!p},{startsWith:function(e){var n=String(l(this));s(e);var t=i(h(arguments.length>1?arguments[1]:void 0,n.length)),o=String(e);return c?c.call(n,o,t):n.slice(t,t+o.length)===o}})},function(e,n,t){var o=t(0),a=t(134),r=t(2),i=t(4),s=t(205).onFreeze,l=Object.freeze;o({target:"Object",stat:!0,forced:r((function(){l(1)})),sham:!a},{freeze:function(e){return l&&i(e)?l(s(e)):e}})},function(e,n,t){var o=t(38),a=t(4),r=t(7),i=t(8).f,s=t(52),l=t(134),u=s("meta"),d=0,c=Object.isExtensible||function(){return!0},h=function(e){i(e,u,{value:{objectID:"O"+ ++d,weakData:{}}})},p=e.exports={REQUIRED:!1,fastKey:function(e,n){if(!a(e))return"symbol"==typeof e?e:("string"==typeof e?"S":"P")+e;if(!r(e,u)){if(!c(e))return"F";if(!n)return"E";h(e)}return e[u].objectID},getWeakData:function(e,n){if(!r(e,u)){if(!c(e))return!0;if(!n)return!1;h(e)}return e[u].weakData},onFreeze:function(e){return l&&p.REQUIRED&&c(e)&&!r(e,u)&&h(e),e}};o[u]=!0},function(e,n,t){var o=t(15),a=t(69).f,r={}.toString,i="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];e.exports.f=function(e){return i&&"[object Window]"==r.call(e)?function(e){try{return a(e)}catch(e){return i.slice()}}(e):a(o(e))}},function(e,n,t){var o=t(5),a=t(123);e.exports=function(e,n,t,r){try{return r?n(o(t)[0],t[1]):n(t)}catch(n){throw a(e),n}}},function(e,n,t){var o=t(10),a=Math.floor,r="".replace,i=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,s=/\$([$&'`]|\d{1,2})/g;e.exports=function(e,n,t,l,u,d){var c=t+e.length,h=l.length,p=s;return void 0!==u&&(u=o(u),p=i),r.call(d,p,(function(o,r){var i;switch(r.charAt(0)){case"$":return"$";case"&":return e;case"`":return n.slice(0,t);case"'":return n.slice(c);case"<":i=u[r.slice(1,-1)];break;default:var s=+r;if(0===s)return o;if(s>h){var d=a(s/10);return 0===d?o:d<=h?void 0===l[d-1]?r.charAt(1):l[d-1]+r.charAt(1):o}i=l[s-1]}return void 0===i?"":i}))}},function(e,n,t){var o=t(139),a=t(210);e.exports=function e(n,t,r,i,s){var l=-1,u=n.length;for(r||(r=a),s||(s=[]);++l<u;){var d=n[l];t>0&&r(d)?t>1?e(d,t-1,r,i,s):o(s,d):i||(s[s.length]=d)}return s}},function(e,n,t){var o=t(41),a=t(85),r=t(17),i=o?o.isConcatSpreadable:void 0;e.exports=function(e){return r(e)||a(e)||!!(i&&e&&e[i])}},function(e,n,t){var o=t(35),a=t(25);e.exports=function(e){return a(e)&&"[object Arguments]"==o(e)}},function(e,n,t){var o=t(41),a=Object.prototype,r=a.hasOwnProperty,i=a.toString,s=o?o.toStringTag:void 0;e.exports=function(e){var n=r.call(e,s),t=e[s];try{e[s]=void 0;var o=!0}catch(e){}var a=i.call(e);return o&&(n?e[s]=t:delete e[s]),a}},function(e,n){var t=Object.prototype.toString;e.exports=function(e){return t.call(e)}},function(e,n,t){var o=t(215),a=t(271),r=t(93),i=t(17),s=t(282);e.exports=function(e){return"function"==typeof e?e:null==e?r:"object"==typeof e?i(e)?a(e[0],e[1]):o(e):s(e)}},function(e,n,t){var o=t(216),a=t(270),r=t(156);e.exports=function(e){var n=a(e);return 1==n.length&&n[0][2]?r(n[0][0],n[0][1]):function(t){return t===e||o(t,e,n)}}},function(e,n,t){var o=t(141),a=t(145);e.exports=function(e,n,t,r){var i=t.length,s=i,l=!r;if(null==e)return!s;for(e=Object(e);i--;){var u=t[i];if(l&&u[2]?u[1]!==e[u[0]]:!(u[0]in e))return!1}for(;++i<s;){var d=(u=t[i])[0],c=e[d],h=u[1];if(l&&u[2]){if(void 0===c&&!(d in e))return!1}else{var p=new o;if(r)var y=r(c,h,d,e,n,p);if(!(void 0===y?a(h,c,3,r,p):y))return!1}}return!0}},function(e,n){e.exports=function(){this.__data__=[],this.size=0}},function(e,n,t){var o=t(61),a=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=o(n,e);return!(t<0)&&(t==n.length-1?n.pop():a.call(n,t,1),--this.size,!0)}},function(e,n,t){var o=t(61);e.exports=function(e){var n=this.__data__,t=o(n,e);return t<0?void 0:n[t][1]}},function(e,n,t){var o=t(61);e.exports=function(e){return o(this.__data__,e)>-1}},function(e,n,t){var o=t(61);e.exports=function(e,n){var t=this.__data__,a=o(t,e);return a<0?(++this.size,t.push([e,n])):t[a][1]=n,this}},function(e,n,t){var o=t(60);e.exports=function(){this.__data__=new o,this.size=0}},function(e,n){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},function(e,n){e.exports=function(e){return this.__data__.get(e)}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n,t){var o=t(60),a=t(86),r=t(88);e.exports=function(e,n){var t=this.__data__;if(t instanceof o){var i=t.__data__;if(!a||i.length<199)return i.push([e,n]),this.size=++t.size,this;t=this.__data__=new r(i)}return t.set(e,n),this.size=t.size,this}},function(e,n,t){var o=t(143),a=t(228),r=t(87),i=t(144),s=/^\[object .+?Constructor\]$/,l=Function.prototype,u=Object.prototype,d=l.toString,c=u.hasOwnProperty,h=RegExp("^"+d.call(c).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!r(e)||a(e))&&(o(e)?h:s).test(i(e))}},function(e,n,t){var o,a=t(229),r=(o=/[^.]+$/.exec(a&&a.keys&&a.keys.IE_PROTO||""))?"Symbol(src)_1."+o:"";e.exports=function(e){return!!r&&r in e}},function(e,n,t){var o=t(18)["__core-js_shared__"];e.exports=o},function(e,n){e.exports=function(e,n){return null==e?void 0:e[n]}},function(e,n,t){var o=t(232),a=t(60),r=t(86);e.exports=function(){this.size=0,this.__data__={hash:new o,map:new(r||a),string:new o}}},function(e,n,t){var o=t(233),a=t(234),r=t(235),i=t(236),s=t(237);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var o=e[n];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=a,l.prototype.get=r,l.prototype.has=i,l.prototype.set=s,e.exports=l},function(e,n,t){var o=t(62);e.exports=function(){this.__data__=o?o(null):{},this.size=0}},function(e,n){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},function(e,n,t){var o=t(62),a=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(o){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return a.call(n,e)?n[e]:void 0}},function(e,n,t){var o=t(62),a=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return o?void 0!==n[e]:a.call(n,e)}},function(e,n,t){var o=t(62);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=o&&void 0===n?"__lodash_hash_undefined__":n,this}},function(e,n,t){var o=t(63);e.exports=function(e){var n=o(this,e).delete(e);return this.size-=n?1:0,n}},function(e,n){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},function(e,n,t){var o=t(63);e.exports=function(e){return o(this,e).get(e)}},function(e,n,t){var o=t(63);e.exports=function(e){return o(this,e).has(e)}},function(e,n,t){var o=t(63);e.exports=function(e,n){var t=o(this,e),a=t.size;return t.set(e,n),this.size+=t.size==a?0:1,this}},function(e,n,t){var o=t(141),a=t(146),r=t(247),i=t(250),s=t(266),l=t(17),u=t(150),d=t(152),c="[object Object]",h=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,p,y,m){var g=l(e),b=l(n),f=g?"[object Array]":s(e),w=b?"[object Array]":s(n),v=(f="[object Arguments]"==f?c:f)==c,k=(w="[object Arguments]"==w?c:w)==c,q=f==w;if(q&&u(e)){if(!u(n))return!1;g=!0,v=!1}if(q&&!v)return m||(m=new o),g||d(e)?a(e,n,t,p,y,m):r(e,n,f,t,p,y,m);if(!(1&t)){var x=v&&h.call(e,"__wrapped__"),j=k&&h.call(n,"__wrapped__");if(x||j){var S=x?e.value():e,T=j?n.value():n;return m||(m=new o),y(S,T,t,p,m)}}return!!q&&(m||(m=new o),i(e,n,t,p,y,m))}},function(e,n){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n){e.exports=function(e,n){for(var t=-1,o=null==e?0:e.length;++t<o;)if(n(e[t],t,e))return!0;return!1}},function(e,n,t){var o=t(41),a=t(248),r=t(142),i=t(146),s=t(249),l=t(89),u=o?o.prototype:void 0,d=u?u.valueOf:void 0;e.exports=function(e,n,t,o,u,c,h){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!c(new a(e),new a(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var p=s;case"[object Set]":var y=1&o;if(p||(p=l),e.size!=n.size&&!y)return!1;var m=h.get(e);if(m)return m==n;o|=2,h.set(e,n);var g=i(p(e),p(n),o,u,c,h);return h.delete(e),g;case"[object Symbol]":if(d)return d.call(e)==d.call(n)}return!1}},function(e,n,t){var o=t(18).Uint8Array;e.exports=o},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,o){t[++n]=[o,e]})),t}},function(e,n,t){var o=t(251),a=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,r,i,s){var l=1&t,u=o(e),d=u.length;if(d!=o(n).length&&!l)return!1;for(var c=d;c--;){var h=u[c];if(!(l?h in n:a.call(n,h)))return!1}var p=s.get(e),y=s.get(n);if(p&&y)return p==n&&y==e;var m=!0;s.set(e,n),s.set(n,e);for(var g=l;++c<d;){var b=e[h=u[c]],f=n[h];if(r)var w=l?r(f,b,h,n,e,s):r(b,f,h,e,n,s);if(!(void 0===w?b===f||i(b,f,t,r,s):w)){m=!1;break}g||(g="constructor"==h)}if(m&&!g){var v=e.constructor,k=n.constructor;v==k||!("constructor"in e)||!("constructor"in n)||"function"==typeof v&&v instanceof v&&"function"==typeof k&&k instanceof k||(m=!1)}return s.delete(e),s.delete(n),m}},function(e,n,t){var o=t(252),a=t(253),r=t(149);e.exports=function(e){return o(e,r,a)}},function(e,n,t){var o=t(139),a=t(17);e.exports=function(e,n,t){var r=n(e);return a(e)?r:o(r,t(e))}},function(e,n,t){var o=t(254),a=t(255),r=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(e){return null==e?[]:(e=Object(e),o(i(e),(function(n){return r.call(e,n)})))}:a;e.exports=s},function(e,n){e.exports=function(e,n){for(var t=-1,o=null==e?0:e.length,a=0,r=[];++t<o;){var i=e[t];n(i,t,e)&&(r[a++]=i)}return r}},function(e,n){e.exports=function(){return[]}},function(e,n,t){var o=t(257),a=t(85),r=t(17),i=t(150),s=t(151),l=t(152),u=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=r(e),d=!t&&a(e),c=!t&&!d&&i(e),h=!t&&!d&&!c&&l(e),p=t||d||c||h,y=p?o(e.length,String):[],m=y.length;for(var g in e)!n&&!u.call(e,g)||p&&("length"==g||c&&("offset"==g||"parent"==g)||h&&("buffer"==g||"byteLength"==g||"byteOffset"==g)||s(g,m))||y.push(g);return y}},function(e,n){e.exports=function(e,n){for(var t=-1,o=Array(e);++t<e;)o[t]=n(t);return o}},function(e,n){e.exports=function(){return!1}},function(e,n,t){var o=t(35),a=t(90),r=t(25),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,e.exports=function(e){return r(e)&&a(e.length)&&!!i[o(e)]}},function(e,n){e.exports=function(e){return function(n){return e(n)}}},function(e,n,t){(function(e){var o=t(140),a=n&&!n.nodeType&&n,r=a&&"object"==typeof e&&e&&!e.nodeType&&e,i=r&&r.exports===a&&o.process,s=function(){try{var e=r&&r.require&&r.require("util").types;return e||i&&i.binding&&i.binding("util")}catch(e){}}();e.exports=s}).call(this,t(109)(e))},function(e,n,t){var o=t(263),a=t(264),r=Object.prototype.hasOwnProperty;e.exports=function(e){if(!o(e))return a(e);var n=[];for(var t in Object(e))r.call(e,t)&&"constructor"!=t&&n.push(t);return n}},function(e,n){var t=Object.prototype;e.exports=function(e){var n=e&&e.constructor;return e===("function"==typeof n&&n.prototype||t)}},function(e,n,t){var o=t(265)(Object.keys,Object);e.exports=o},function(e,n){e.exports=function(e,n){return function(t){return e(n(t))}}},function(e,n,t){var o=t(267),a=t(86),r=t(268),i=t(154),s=t(269),l=t(35),u=t(144),d=u(o),c=u(a),h=u(r),p=u(i),y=u(s),m=l;(o&&"[object DataView]"!=m(new o(new ArrayBuffer(1)))||a&&"[object Map]"!=m(new a)||r&&"[object Promise]"!=m(r.resolve())||i&&"[object Set]"!=m(new i)||s&&"[object WeakMap]"!=m(new s))&&(m=function(e){var n=l(e),t="[object Object]"==n?e.constructor:void 0,o=t?u(t):"";if(o)switch(o){case d:return"[object DataView]";case c:return"[object Map]";case h:return"[object Promise]";case p:return"[object Set]";case y:return"[object WeakMap]"}return n}),e.exports=m},function(e,n,t){var o=t(22)(t(18),"DataView");e.exports=o},function(e,n,t){var o=t(22)(t(18),"Promise");e.exports=o},function(e,n,t){var o=t(22)(t(18),"WeakMap");e.exports=o},function(e,n,t){var o=t(155),a=t(149);e.exports=function(e){for(var n=a(e),t=n.length;t--;){var r=n[t],i=e[r];n[t]=[r,i,o(i)]}return n}},function(e,n,t){var o=t(145),a=t(272),r=t(279),i=t(91),s=t(155),l=t(156),u=t(64);e.exports=function(e,n){return i(e)&&s(n)?l(u(e),n):function(t){var i=a(t,e);return void 0===i&&i===n?r(t,e):o(n,i,3)}}},function(e,n,t){var o=t(157);e.exports=function(e,n,t){var a=null==e?void 0:o(e,n);return void 0===a?t:a}},function(e,n,t){var o=t(274),a=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,i=o((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(a,(function(e,t,o,a){n.push(o?a.replace(r,"$1"):t||e)})),n}));e.exports=i},function(e,n,t){var o=t(275);e.exports=function(e){var n=o(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},function(e,n,t){var o=t(88);function a(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var o=arguments,a=n?n.apply(this,o):o[0],r=t.cache;if(r.has(a))return r.get(a);var i=e.apply(this,o);return t.cache=r.set(a,i)||r,i};return t.cache=new(a.Cache||o),t}a.Cache=o,e.exports=a},function(e,n,t){var o=t(277);e.exports=function(e){return null==e?"":o(e)}},function(e,n,t){var o=t(41),a=t(278),r=t(17),i=t(92),s=o?o.prototype:void 0,l=s?s.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(r(n))return a(n,e)+"";if(i(n))return l?l.call(n):"";var t=n+"";return"0"==t&&1/n==-1/0?"-0":t}},function(e,n){e.exports=function(e,n){for(var t=-1,o=null==e?0:e.length,a=Array(o);++t<o;)a[t]=n(e[t],t,e);return a}},function(e,n,t){var o=t(280),a=t(281);e.exports=function(e,n){return null!=e&&a(e,n,o)}},function(e,n){e.exports=function(e,n){return null!=e&&n in Object(e)}},function(e,n,t){var o=t(158),a=t(85),r=t(17),i=t(151),s=t(90),l=t(64);e.exports=function(e,n,t){for(var u=-1,d=(n=o(n,e)).length,c=!1;++u<d;){var h=l(n[u]);if(!(c=null!=e&&t(e,h)))break;e=e[h]}return c||++u!=d?c:!!(d=null==e?0:e.length)&&s(d)&&i(h,d)&&(r(e)||a(e))}},function(e,n,t){var o=t(283),a=t(284),r=t(91),i=t(64);e.exports=function(e){return r(e)?o(i(e)):a(e)}},function(e,n){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},function(e,n,t){var o=t(157);e.exports=function(e){return function(n){return o(n,e)}}},function(e,n,t){var o=t(93),a=t(286),r=t(288);e.exports=function(e,n){return r(a(e,n,o),e+"")}},function(e,n,t){var o=t(287),a=Math.max;e.exports=function(e,n,t){return n=a(void 0===n?e.length-1:n,0),function(){for(var r=arguments,i=-1,s=a(r.length-n,0),l=Array(s);++i<s;)l[i]=r[n+i];i=-1;for(var u=Array(n+1);++i<n;)u[i]=r[i];return u[n]=t(l),o(e,this,u)}}},function(e,n){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},function(e,n,t){var o=t(289),a=t(292)(o);e.exports=a},function(e,n,t){var o=t(290),a=t(291),r=t(93),i=a?function(e,n){return a(e,"toString",{configurable:!0,enumerable:!1,value:o(n),writable:!0})}:r;e.exports=i},function(e,n){e.exports=function(e){return function(){return e}}},function(e,n,t){var o=t(22),a=function(){try{var e=o(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=a},function(e,n){var t=Date.now;e.exports=function(e){var n=0,o=0;return function(){var a=t(),r=16-(a-o);if(o=a,r>0){if(++n>=800)return arguments[0]}else n=0;return e.apply(void 0,arguments)}}},function(e,n,t){var o=t(147),a=t(294),r=t(299),i=t(148),s=t(300),l=t(89);e.exports=function(e,n,t){var u=-1,d=a,c=e.length,h=!0,p=[],y=p;if(t)h=!1,d=r;else if(c>=200){var m=n?null:s(e);if(m)return l(m);h=!1,d=i,y=new o}else y=n?[]:p;e:for(;++u<c;){var g=e[u],b=n?n(g):g;if(g=t||0!==g?g:0,h&&b==b){for(var f=y.length;f--;)if(y[f]===b)continue e;n&&y.push(b),p.push(g)}else d(y,b,t)||(y!==p&&y.push(b),p.push(g))}return p}},function(e,n,t){var o=t(295);e.exports=function(e,n){return!!(null==e?0:e.length)&&o(e,n,0)>-1}},function(e,n,t){var o=t(296),a=t(297),r=t(298);e.exports=function(e,n,t){return n==n?r(e,n,t):o(e,a,t)}},function(e,n){e.exports=function(e,n,t,o){for(var a=e.length,r=t+(o?1:-1);o?r--:++r<a;)if(n(e[r],r,e))return r;return-1}},function(e,n){e.exports=function(e){return e!=e}},function(e,n){e.exports=function(e,n,t){for(var o=t-1,a=e.length;++o<a;)if(e[o]===n)return o;return-1}},function(e,n){e.exports=function(e,n,t){for(var o=-1,a=null==e?0:e.length;++o<a;)if(t(n,e[o]))return!0;return!1}},function(e,n,t){var o=t(154),a=t(301),r=t(89),i=o&&1/r(new o([,-0]))[1]==1/0?function(e){return new o(e)}:a;e.exports=i},function(e,n){e.exports=function(){}},function(e,n,t){var o=t(153),a=t(25);e.exports=function(e){return a(e)&&o(e)}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";var o=t(21),a=t(4),r=[].slice,i={},s=function(e,n,t){if(!(n in i)){for(var o=[],a=0;a<n;a++)o[a]="a["+a+"]";i[n]=Function("C,a","return new C("+o.join(",")+")")}return i[n](e,t)};e.exports=Function.bind||function(e){var n=o(this),t=r.call(arguments,1),i=function(){var o=t.concat(r.call(arguments));return this instanceof i?s(n,o.length,o):n.apply(e,o)};return a(n.prototype)&&(i.prototype=n.prototype),i}},function(e,n,t){"use strict";t(161)},function(e,n,t){"use strict";t(162)},function(e,n,t){"use strict";t.r(n);t(102),t(191),t(198),t(199);var o=t(56),a=(t(100),t(49),t(9),t(11),t(16),t(67),t(27),Object.freeze({}));function r(e){return null==e}function i(e){return null!=e}function s(e){return!0===e}function l(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function u(e){return null!==e&&"object"==typeof e}var d=Object.prototype.toString;function c(e){return"[object Object]"===d.call(e)}function h(e){return"[object RegExp]"===d.call(e)}function p(e){var n=parseFloat(String(e));return n>=0&&Math.floor(n)===n&&isFinite(e)}function y(e){return i(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function m(e){return null==e?"":Array.isArray(e)||c(e)&&e.toString===d?JSON.stringify(e,null,2):String(e)}function g(e){var n=parseFloat(e);return isNaN(n)?e:n}function b(e,n){for(var t=Object.create(null),o=e.split(","),a=0;a<o.length;a++)t[o[a]]=!0;return n?function(e){return t[e.toLowerCase()]}:function(e){return t[e]}}b("slot,component",!0);var f=b("key,ref,slot,slot-scope,is");function w(e,n){if(e.length){var t=e.indexOf(n);if(t>-1)return e.splice(t,1)}}var v=Object.prototype.hasOwnProperty;function k(e,n){return v.call(e,n)}function q(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var x=/-(\w)/g,j=q((function(e){return e.replace(x,(function(e,n){return n?n.toUpperCase():""}))})),S=q((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),T=/\B([A-Z])/g,I=q((function(e){return e.replace(T,"-$1").toLowerCase()}));var P=Function.prototype.bind?function(e,n){return e.bind(n)}:function(e,n){function t(t){var o=arguments.length;return o?o>1?e.apply(n,arguments):e.call(n,t):e.call(n)}return t._length=e.length,t};function Q(e,n){n=n||0;for(var t=e.length-n,o=new Array(t);t--;)o[t]=e[t+n];return o}function z(e,n){for(var t in n)e[t]=n[t];return e}function C(e){for(var n={},t=0;t<e.length;t++)e[t]&&z(n,e[t]);return n}function A(e,n,t){}var E=function(e,n,t){return!1},L=function(e){return e};function O(e,n){if(e===n)return!0;var t=u(e),o=u(n);if(!t||!o)return!t&&!o&&String(e)===String(n);try{var a=Array.isArray(e),r=Array.isArray(n);if(a&&r)return e.length===n.length&&e.every((function(e,t){return O(e,n[t])}));if(e instanceof Date&&n instanceof Date)return e.getTime()===n.getTime();if(a||r)return!1;var i=Object.keys(e),s=Object.keys(n);return i.length===s.length&&i.every((function(t){return O(e[t],n[t])}))}catch(e){return!1}}function H(e,n){for(var t=0;t<e.length;t++)if(O(e[t],n))return t;return-1}function D(e){var n=!1;return function(){n||(n=!0,e.apply(this,arguments))}}var _=["component","directive","filter"],N=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],M={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:E,isReservedAttr:E,isUnknownElement:E,getTagNamespace:A,parsePlatformTagName:L,mustUseProp:E,async:!0,_lifecycleHooks:N},G=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function R(e,n,t,o){Object.defineProperty(e,n,{value:t,enumerable:!!o,writable:!0,configurable:!0})}var B=new RegExp("[^"+G.source+".$_\\d]");var W,F="__proto__"in{},U="undefined"!=typeof window,Y="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,V=Y&&WXEnvironment.platform.toLowerCase(),J=U&&window.navigator.userAgent.toLowerCase(),$=J&&/msie|trident/.test(J),K=J&&J.indexOf("msie 9.0")>0,Z=J&&J.indexOf("edge/")>0,X=(J&&J.indexOf("android"),J&&/iphone|ipad|ipod|ios/.test(J)||"ios"===V),ee=(J&&/chrome\/\d+/.test(J),J&&/phantomjs/.test(J),J&&J.match(/firefox\/(\d+)/)),ne={}.watch,te=!1;if(U)try{var oe={};Object.defineProperty(oe,"passive",{get:function(){te=!0}}),window.addEventListener("test-passive",null,oe)}catch(e){}var ae=function(){return void 0===W&&(W=!U&&!Y&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),W},re=U&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ie(e){return"function"==typeof e&&/native code/.test(e.toString())}var se,le="undefined"!=typeof Symbol&&ie(Symbol)&&"undefined"!=typeof Reflect&&ie(Reflect.ownKeys);se="undefined"!=typeof Set&&ie(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var ue=A,de=0,ce=function(){this.id=de++,this.subs=[]};ce.prototype.addSub=function(e){this.subs.push(e)},ce.prototype.removeSub=function(e){w(this.subs,e)},ce.prototype.depend=function(){ce.target&&ce.target.addDep(this)},ce.prototype.notify=function(){var e=this.subs.slice();for(var n=0,t=e.length;n<t;n++)e[n].update()},ce.target=null;var he=[];function pe(e){he.push(e),ce.target=e}function ye(){he.pop(),ce.target=he[he.length-1]}var me=function(e,n,t,o,a,r,i,s){this.tag=e,this.data=n,this.children=t,this.text=o,this.elm=a,this.ns=void 0,this.context=r,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=n&&n.key,this.componentOptions=i,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},ge={child:{configurable:!0}};ge.child.get=function(){return this.componentInstance},Object.defineProperties(me.prototype,ge);var be=function(e){void 0===e&&(e="");var n=new me;return n.text=e,n.isComment=!0,n};function fe(e){return new me(void 0,void 0,void 0,String(e))}function we(e){var n=new me(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return n.ns=e.ns,n.isStatic=e.isStatic,n.key=e.key,n.isComment=e.isComment,n.fnContext=e.fnContext,n.fnOptions=e.fnOptions,n.fnScopeId=e.fnScopeId,n.asyncMeta=e.asyncMeta,n.isCloned=!0,n}var ve=Array.prototype,ke=Object.create(ve);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var n=ve[e];R(ke,e,(function(){for(var t=[],o=arguments.length;o--;)t[o]=arguments[o];var a,r=n.apply(this,t),i=this.__ob__;switch(e){case"push":case"unshift":a=t;break;case"splice":a=t.slice(2)}return a&&i.observeArray(a),i.dep.notify(),r}))}));var qe=Object.getOwnPropertyNames(ke),xe=!0;function je(e){xe=e}var Se=function(e){this.value=e,this.dep=new ce,this.vmCount=0,R(e,"__ob__",this),Array.isArray(e)?(F?function(e,n){e.__proto__=n}(e,ke):function(e,n,t){for(var o=0,a=t.length;o<a;o++){var r=t[o];R(e,r,n[r])}}(e,ke,qe),this.observeArray(e)):this.walk(e)};function Te(e,n){var t;if(u(e)&&!(e instanceof me))return k(e,"__ob__")&&e.__ob__ instanceof Se?t=e.__ob__:xe&&!ae()&&(Array.isArray(e)||c(e))&&Object.isExtensible(e)&&!e._isVue&&(t=new Se(e)),n&&t&&t.vmCount++,t}function Ie(e,n,t,o,a){var r=new ce,i=Object.getOwnPropertyDescriptor(e,n);if(!i||!1!==i.configurable){var s=i&&i.get,l=i&&i.set;s&&!l||2!==arguments.length||(t=e[n]);var u=!a&&Te(t);Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var n=s?s.call(e):t;return ce.target&&(r.depend(),u&&(u.dep.depend(),Array.isArray(n)&&ze(n))),n},set:function(n){var o=s?s.call(e):t;n===o||n!=n&&o!=o||s&&!l||(l?l.call(e,n):t=n,u=!a&&Te(n),r.notify())}})}}function Pe(e,n,t){if(Array.isArray(e)&&p(n))return e.length=Math.max(e.length,n),e.splice(n,1,t),t;if(n in e&&!(n in Object.prototype))return e[n]=t,t;var o=e.__ob__;return e._isVue||o&&o.vmCount?t:o?(Ie(o.value,n,t),o.dep.notify(),t):(e[n]=t,t)}function Qe(e,n){if(Array.isArray(e)&&p(n))e.splice(n,1);else{var t=e.__ob__;e._isVue||t&&t.vmCount||k(e,n)&&(delete e[n],t&&t.dep.notify())}}function ze(e){for(var n=void 0,t=0,o=e.length;t<o;t++)(n=e[t])&&n.__ob__&&n.__ob__.dep.depend(),Array.isArray(n)&&ze(n)}Se.prototype.walk=function(e){for(var n=Object.keys(e),t=0;t<n.length;t++)Ie(e,n[t])},Se.prototype.observeArray=function(e){for(var n=0,t=e.length;n<t;n++)Te(e[n])};var Ce=M.optionMergeStrategies;function Ae(e,n){if(!n)return e;for(var t,o,a,r=le?Reflect.ownKeys(n):Object.keys(n),i=0;i<r.length;i++)"__ob__"!==(t=r[i])&&(o=e[t],a=n[t],k(e,t)?o!==a&&c(o)&&c(a)&&Ae(o,a):Pe(e,t,a));return e}function Ee(e,n,t){return t?function(){var o="function"==typeof n?n.call(t,t):n,a="function"==typeof e?e.call(t,t):e;return o?Ae(o,a):a}:n?e?function(){return Ae("function"==typeof n?n.call(this,this):n,"function"==typeof e?e.call(this,this):e)}:n:e}function Le(e,n){var t=n?e?e.concat(n):Array.isArray(n)?n:[n]:e;return t?function(e){for(var n=[],t=0;t<e.length;t++)-1===n.indexOf(e[t])&&n.push(e[t]);return n}(t):t}function Oe(e,n,t,o){var a=Object.create(e||null);return n?z(a,n):a}Ce.data=function(e,n,t){return t?Ee(e,n,t):n&&"function"!=typeof n?e:Ee(e,n)},N.forEach((function(e){Ce[e]=Le})),_.forEach((function(e){Ce[e+"s"]=Oe})),Ce.watch=function(e,n,t,o){if(e===ne&&(e=void 0),n===ne&&(n=void 0),!n)return Object.create(e||null);if(!e)return n;var a={};for(var r in z(a,e),n){var i=a[r],s=n[r];i&&!Array.isArray(i)&&(i=[i]),a[r]=i?i.concat(s):Array.isArray(s)?s:[s]}return a},Ce.props=Ce.methods=Ce.inject=Ce.computed=function(e,n,t,o){if(!e)return n;var a=Object.create(null);return z(a,e),n&&z(a,n),a},Ce.provide=Ee;var He=function(e,n){return void 0===n?e:n};function De(e,n,t){if("function"==typeof n&&(n=n.options),function(e,n){var t=e.props;if(t){var o,a,r={};if(Array.isArray(t))for(o=t.length;o--;)"string"==typeof(a=t[o])&&(r[j(a)]={type:null});else if(c(t))for(var i in t)a=t[i],r[j(i)]=c(a)?a:{type:a};else 0;e.props=r}}(n),function(e,n){var t=e.inject;if(t){var o=e.inject={};if(Array.isArray(t))for(var a=0;a<t.length;a++)o[t[a]]={from:t[a]};else if(c(t))for(var r in t){var i=t[r];o[r]=c(i)?z({from:r},i):{from:i}}else 0}}(n),function(e){var n=e.directives;if(n)for(var t in n){var o=n[t];"function"==typeof o&&(n[t]={bind:o,update:o})}}(n),!n._base&&(n.extends&&(e=De(e,n.extends,t)),n.mixins))for(var o=0,a=n.mixins.length;o<a;o++)e=De(e,n.mixins[o],t);var r,i={};for(r in e)s(r);for(r in n)k(e,r)||s(r);function s(o){var a=Ce[o]||He;i[o]=a(e[o],n[o],t,o)}return i}function _e(e,n,t,o){if("string"==typeof t){var a=e[n];if(k(a,t))return a[t];var r=j(t);if(k(a,r))return a[r];var i=S(r);return k(a,i)?a[i]:a[t]||a[r]||a[i]}}function Ne(e,n,t,o){var a=n[e],r=!k(t,e),i=t[e],s=Be(Boolean,a.type);if(s>-1)if(r&&!k(a,"default"))i=!1;else if(""===i||i===I(e)){var l=Be(String,a.type);(l<0||s<l)&&(i=!0)}if(void 0===i){i=function(e,n,t){if(!k(n,"default"))return;var o=n.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[t]&&void 0!==e._props[t])return e._props[t];return"function"==typeof o&&"Function"!==Ge(n.type)?o.call(e):o}(o,a,e);var u=xe;je(!0),Te(i),je(u)}return i}var Me=/^\s*function (\w+)/;function Ge(e){var n=e&&e.toString().match(Me);return n?n[1]:""}function Re(e,n){return Ge(e)===Ge(n)}function Be(e,n){if(!Array.isArray(n))return Re(n,e)?0:-1;for(var t=0,o=n.length;t<o;t++)if(Re(n[t],e))return t;return-1}function We(e,n,t){pe();try{if(n)for(var o=n;o=o.$parent;){var a=o.$options.errorCaptured;if(a)for(var r=0;r<a.length;r++)try{if(!1===a[r].call(o,e,n,t))return}catch(e){Ue(e,o,"errorCaptured hook")}}Ue(e,n,t)}finally{ye()}}function Fe(e,n,t,o,a){var r;try{(r=t?e.apply(n,t):e.call(n))&&!r._isVue&&y(r)&&!r._handled&&(r.catch((function(e){return We(e,o,a+" (Promise/async)")})),r._handled=!0)}catch(e){We(e,o,a)}return r}function Ue(e,n,t){if(M.errorHandler)try{return M.errorHandler.call(null,e,n,t)}catch(n){n!==e&&Ye(n,null,"config.errorHandler")}Ye(e,n,t)}function Ye(e,n,t){if(!U&&!Y||"undefined"==typeof console)throw e;console.error(e)}var Ve,Je=!1,$e=[],Ke=!1;function Ze(){Ke=!1;var e=$e.slice(0);$e.length=0;for(var n=0;n<e.length;n++)e[n]()}if("undefined"!=typeof Promise&&ie(Promise)){var Xe=Promise.resolve();Ve=function(){Xe.then(Ze),X&&setTimeout(A)},Je=!0}else if($||"undefined"==typeof MutationObserver||!ie(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Ve="undefined"!=typeof setImmediate&&ie(setImmediate)?function(){setImmediate(Ze)}:function(){setTimeout(Ze,0)};else{var en=1,nn=new MutationObserver(Ze),tn=document.createTextNode(String(en));nn.observe(tn,{characterData:!0}),Ve=function(){en=(en+1)%2,tn.data=String(en)},Je=!0}function on(e,n){var t;if($e.push((function(){if(e)try{e.call(n)}catch(e){We(e,n,"nextTick")}else t&&t(n)})),Ke||(Ke=!0,Ve()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){t=e}))}var an=new se;function rn(e){!function e(n,t){var o,a,r=Array.isArray(n);if(!r&&!u(n)||Object.isFrozen(n)||n instanceof me)return;if(n.__ob__){var i=n.__ob__.dep.id;if(t.has(i))return;t.add(i)}if(r)for(o=n.length;o--;)e(n[o],t);else for(a=Object.keys(n),o=a.length;o--;)e(n[a[o]],t)}(e,an),an.clear()}var sn=q((function(e){var n="&"===e.charAt(0),t="~"===(e=n?e.slice(1):e).charAt(0),o="!"===(e=t?e.slice(1):e).charAt(0);return{name:e=o?e.slice(1):e,once:t,capture:o,passive:n}}));function ln(e,n){function t(){var e=arguments,o=t.fns;if(!Array.isArray(o))return Fe(o,null,arguments,n,"v-on handler");for(var a=o.slice(),r=0;r<a.length;r++)Fe(a[r],null,e,n,"v-on handler")}return t.fns=e,t}function un(e,n,t,o,a,i){var l,u,d,c;for(l in e)u=e[l],d=n[l],c=sn(l),r(u)||(r(d)?(r(u.fns)&&(u=e[l]=ln(u,i)),s(c.once)&&(u=e[l]=a(c.name,u,c.capture)),t(c.name,u,c.capture,c.passive,c.params)):u!==d&&(d.fns=u,e[l]=d));for(l in n)r(e[l])&&o((c=sn(l)).name,n[l],c.capture)}function dn(e,n,t){var o;e instanceof me&&(e=e.data.hook||(e.data.hook={}));var a=e[n];function l(){t.apply(this,arguments),w(o.fns,l)}r(a)?o=ln([l]):i(a.fns)&&s(a.merged)?(o=a).fns.push(l):o=ln([a,l]),o.merged=!0,e[n]=o}function cn(e,n,t,o,a){if(i(n)){if(k(n,t))return e[t]=n[t],a||delete n[t],!0;if(k(n,o))return e[t]=n[o],a||delete n[o],!0}return!1}function hn(e){return l(e)?[fe(e)]:Array.isArray(e)?function e(n,t){var o,a,u,d,c=[];for(o=0;o<n.length;o++)r(a=n[o])||"boolean"==typeof a||(u=c.length-1,d=c[u],Array.isArray(a)?a.length>0&&(pn((a=e(a,(t||"")+"_"+o))[0])&&pn(d)&&(c[u]=fe(d.text+a[0].text),a.shift()),c.push.apply(c,a)):l(a)?pn(d)?c[u]=fe(d.text+a):""!==a&&c.push(fe(a)):pn(a)&&pn(d)?c[u]=fe(d.text+a.text):(s(n._isVList)&&i(a.tag)&&r(a.key)&&i(t)&&(a.key="__vlist"+t+"_"+o+"__"),c.push(a)));return c}(e):void 0}function pn(e){return i(e)&&i(e.text)&&!1===e.isComment}function yn(e,n){if(e){for(var t=Object.create(null),o=le?Reflect.ownKeys(e):Object.keys(e),a=0;a<o.length;a++){var r=o[a];if("__ob__"!==r){for(var i=e[r].from,s=n;s;){if(s._provided&&k(s._provided,i)){t[r]=s._provided[i];break}s=s.$parent}if(!s)if("default"in e[r]){var l=e[r].default;t[r]="function"==typeof l?l.call(n):l}else 0}}return t}}function mn(e,n){if(!e||!e.length)return{};for(var t={},o=0,a=e.length;o<a;o++){var r=e[o],i=r.data;if(i&&i.attrs&&i.attrs.slot&&delete i.attrs.slot,r.context!==n&&r.fnContext!==n||!i||null==i.slot)(t.default||(t.default=[])).push(r);else{var s=i.slot,l=t[s]||(t[s]=[]);"template"===r.tag?l.push.apply(l,r.children||[]):l.push(r)}}for(var u in t)t[u].every(gn)&&delete t[u];return t}function gn(e){return e.isComment&&!e.asyncFactory||" "===e.text}function bn(e){return e.isComment&&e.asyncFactory}function fn(e,n,t){var o,r=Object.keys(n).length>0,i=e?!!e.$stable:!r,s=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(i&&t&&t!==a&&s===t.$key&&!r&&!t.$hasNormal)return t;for(var l in o={},e)e[l]&&"$"!==l[0]&&(o[l]=wn(n,l,e[l]))}else o={};for(var u in n)u in o||(o[u]=vn(n,u));return e&&Object.isExtensible(e)&&(e._normalized=o),R(o,"$stable",i),R(o,"$key",s),R(o,"$hasNormal",r),o}function wn(e,n,t){var o=function(){var e=arguments.length?t.apply(null,arguments):t({}),n=(e=e&&"object"==typeof e&&!Array.isArray(e)?[e]:hn(e))&&e[0];return e&&(!n||1===e.length&&n.isComment&&!bn(n))?void 0:e};return t.proxy&&Object.defineProperty(e,n,{get:o,enumerable:!0,configurable:!0}),o}function vn(e,n){return function(){return e[n]}}function kn(e,n){var t,o,a,r,s;if(Array.isArray(e)||"string"==typeof e)for(t=new Array(e.length),o=0,a=e.length;o<a;o++)t[o]=n(e[o],o);else if("number"==typeof e)for(t=new Array(e),o=0;o<e;o++)t[o]=n(o+1,o);else if(u(e))if(le&&e[Symbol.iterator]){t=[];for(var l=e[Symbol.iterator](),d=l.next();!d.done;)t.push(n(d.value,t.length)),d=l.next()}else for(r=Object.keys(e),t=new Array(r.length),o=0,a=r.length;o<a;o++)s=r[o],t[o]=n(e[s],s,o);return i(t)||(t=[]),t._isVList=!0,t}function qn(e,n,t,o){var a,r=this.$scopedSlots[e];r?(t=t||{},o&&(t=z(z({},o),t)),a=r(t)||("function"==typeof n?n():n)):a=this.$slots[e]||("function"==typeof n?n():n);var i=t&&t.slot;return i?this.$createElement("template",{slot:i},a):a}function xn(e){return _e(this.$options,"filters",e)||L}function jn(e,n){return Array.isArray(e)?-1===e.indexOf(n):e!==n}function Sn(e,n,t,o,a){var r=M.keyCodes[n]||t;return a&&o&&!M.keyCodes[n]?jn(a,o):r?jn(r,e):o?I(o)!==n:void 0===e}function Tn(e,n,t,o,a){if(t)if(u(t)){var r;Array.isArray(t)&&(t=C(t));var i=function(i){if("class"===i||"style"===i||f(i))r=e;else{var s=e.attrs&&e.attrs.type;r=o||M.mustUseProp(n,s,i)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var l=j(i),u=I(i);l in r||u in r||(r[i]=t[i],a&&((e.on||(e.on={}))["update:"+i]=function(e){t[i]=e}))};for(var s in t)i(s)}else;return e}function In(e,n){var t=this._staticTrees||(this._staticTrees=[]),o=t[e];return o&&!n||Qn(o=t[e]=this.$options.staticRenderFns[e].call(this._renderProxy,null,this),"__static__"+e,!1),o}function Pn(e,n,t){return Qn(e,"__once__"+n+(t?"_"+t:""),!0),e}function Qn(e,n,t){if(Array.isArray(e))for(var o=0;o<e.length;o++)e[o]&&"string"!=typeof e[o]&&zn(e[o],n+"_"+o,t);else zn(e,n,t)}function zn(e,n,t){e.isStatic=!0,e.key=n,e.isOnce=t}function Cn(e,n){if(n)if(c(n)){var t=e.on=e.on?z({},e.on):{};for(var o in n){var a=t[o],r=n[o];t[o]=a?[].concat(a,r):r}}else;return e}function An(e,n,t,o){n=n||{$stable:!t};for(var a=0;a<e.length;a++){var r=e[a];Array.isArray(r)?An(r,n,t):r&&(r.proxy&&(r.fn.proxy=!0),n[r.key]=r.fn)}return o&&(n.$key=o),n}function En(e,n){for(var t=0;t<n.length;t+=2){var o=n[t];"string"==typeof o&&o&&(e[n[t]]=n[t+1])}return e}function Ln(e,n){return"string"==typeof e?n+e:e}function On(e){e._o=Pn,e._n=g,e._s=m,e._l=kn,e._t=qn,e._q=O,e._i=H,e._m=In,e._f=xn,e._k=Sn,e._b=Tn,e._v=fe,e._e=be,e._u=An,e._g=Cn,e._d=En,e._p=Ln}function Hn(e,n,t,o,r){var i,l=this,u=r.options;k(o,"_uid")?(i=Object.create(o))._original=o:(i=o,o=o._original);var d=s(u._compiled),c=!d;this.data=e,this.props=n,this.children=t,this.parent=o,this.listeners=e.on||a,this.injections=yn(u.inject,o),this.slots=function(){return l.$slots||fn(e.scopedSlots,l.$slots=mn(t,o)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return fn(e.scopedSlots,this.slots())}}),d&&(this.$options=u,this.$slots=this.slots(),this.$scopedSlots=fn(e.scopedSlots,this.$slots)),u._scopeId?this._c=function(e,n,t,a){var r=Bn(i,e,n,t,a,c);return r&&!Array.isArray(r)&&(r.fnScopeId=u._scopeId,r.fnContext=o),r}:this._c=function(e,n,t,o){return Bn(i,e,n,t,o,c)}}function Dn(e,n,t,o,a){var r=we(e);return r.fnContext=t,r.fnOptions=o,n.slot&&((r.data||(r.data={})).slot=n.slot),r}function _n(e,n){for(var t in n)e[j(t)]=n[t]}On(Hn.prototype);var Nn={init:function(e,n){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var t=e;Nn.prepatch(t,t)}else{(e.componentInstance=function(e,n){var t={_isComponent:!0,_parentVnode:e,parent:n},o=e.data.inlineTemplate;i(o)&&(t.render=o.render,t.staticRenderFns=o.staticRenderFns);return new e.componentOptions.Ctor(t)}(e,Zn)).$mount(n?e.elm:void 0,n)}},prepatch:function(e,n){var t=n.componentOptions;!function(e,n,t,o,r){0;var i=o.data.scopedSlots,s=e.$scopedSlots,l=!!(i&&!i.$stable||s!==a&&!s.$stable||i&&e.$scopedSlots.$key!==i.$key||!i&&e.$scopedSlots.$key),u=!!(r||e.$options._renderChildren||l);e.$options._parentVnode=o,e.$vnode=o,e._vnode&&(e._vnode.parent=o);if(e.$options._renderChildren=r,e.$attrs=o.data.attrs||a,e.$listeners=t||a,n&&e.$options.props){je(!1);for(var d=e._props,c=e.$options._propKeys||[],h=0;h<c.length;h++){var p=c[h],y=e.$options.props;d[p]=Ne(p,y,n,e)}je(!0),e.$options.propsData=n}t=t||a;var m=e.$options._parentListeners;e.$options._parentListeners=t,Kn(e,t,m),u&&(e.$slots=mn(r,o.context),e.$forceUpdate());0}(n.componentInstance=e.componentInstance,t.propsData,t.listeners,n,t.children)},insert:function(e){var n,t=e.context,o=e.componentInstance;o._isMounted||(o._isMounted=!0,tt(o,"mounted")),e.data.keepAlive&&(t._isMounted?((n=o)._inactive=!1,at.push(n)):nt(o,!0))},destroy:function(e){var n=e.componentInstance;n._isDestroyed||(e.data.keepAlive?function e(n,t){if(t&&(n._directInactive=!0,et(n)))return;if(!n._inactive){n._inactive=!0;for(var o=0;o<n.$children.length;o++)e(n.$children[o]);tt(n,"deactivated")}}(n,!0):n.$destroy())}},Mn=Object.keys(Nn);function Gn(e,n,t,o,l){if(!r(e)){var d=t.$options._base;if(u(e)&&(e=d.extend(e)),"function"==typeof e){var c;if(r(e.cid)&&void 0===(e=function(e,n){if(s(e.error)&&i(e.errorComp))return e.errorComp;if(i(e.resolved))return e.resolved;var t=Fn;t&&i(e.owners)&&-1===e.owners.indexOf(t)&&e.owners.push(t);if(s(e.loading)&&i(e.loadingComp))return e.loadingComp;if(t&&!i(e.owners)){var o=e.owners=[t],a=!0,l=null,d=null;t.$on("hook:destroyed",(function(){return w(o,t)}));var c=function(e){for(var n=0,t=o.length;n<t;n++)o[n].$forceUpdate();e&&(o.length=0,null!==l&&(clearTimeout(l),l=null),null!==d&&(clearTimeout(d),d=null))},h=D((function(t){e.resolved=Un(t,n),a?o.length=0:c(!0)})),p=D((function(n){i(e.errorComp)&&(e.error=!0,c(!0))})),m=e(h,p);return u(m)&&(y(m)?r(e.resolved)&&m.then(h,p):y(m.component)&&(m.component.then(h,p),i(m.error)&&(e.errorComp=Un(m.error,n)),i(m.loading)&&(e.loadingComp=Un(m.loading,n),0===m.delay?e.loading=!0:l=setTimeout((function(){l=null,r(e.resolved)&&r(e.error)&&(e.loading=!0,c(!1))}),m.delay||200)),i(m.timeout)&&(d=setTimeout((function(){d=null,r(e.resolved)&&p(null)}),m.timeout)))),a=!1,e.loading?e.loadingComp:e.resolved}}(c=e,d)))return function(e,n,t,o,a){var r=be();return r.asyncFactory=e,r.asyncMeta={data:n,context:t,children:o,tag:a},r}(c,n,t,o,l);n=n||{},jt(e),i(n.model)&&function(e,n){var t=e.model&&e.model.prop||"value",o=e.model&&e.model.event||"input";(n.attrs||(n.attrs={}))[t]=n.model.value;var a=n.on||(n.on={}),r=a[o],s=n.model.callback;i(r)?(Array.isArray(r)?-1===r.indexOf(s):r!==s)&&(a[o]=[s].concat(r)):a[o]=s}(e.options,n);var h=function(e,n,t){var o=n.options.props;if(!r(o)){var a={},s=e.attrs,l=e.props;if(i(s)||i(l))for(var u in o){var d=I(u);cn(a,l,u,d,!0)||cn(a,s,u,d,!1)}return a}}(n,e);if(s(e.options.functional))return function(e,n,t,o,r){var s=e.options,l={},u=s.props;if(i(u))for(var d in u)l[d]=Ne(d,u,n||a);else i(t.attrs)&&_n(l,t.attrs),i(t.props)&&_n(l,t.props);var c=new Hn(t,l,r,o,e),h=s.render.call(null,c._c,c);if(h instanceof me)return Dn(h,t,c.parent,s,c);if(Array.isArray(h)){for(var p=hn(h)||[],y=new Array(p.length),m=0;m<p.length;m++)y[m]=Dn(p[m],t,c.parent,s,c);return y}}(e,h,n,t,o);var p=n.on;if(n.on=n.nativeOn,s(e.options.abstract)){var m=n.slot;n={},m&&(n.slot=m)}!function(e){for(var n=e.hook||(e.hook={}),t=0;t<Mn.length;t++){var o=Mn[t],a=n[o],r=Nn[o];a===r||a&&a._merged||(n[o]=a?Rn(r,a):r)}}(n);var g=e.options.name||l;return new me("vue-component-"+e.cid+(g?"-"+g:""),n,void 0,void 0,void 0,t,{Ctor:e,propsData:h,listeners:p,tag:l,children:o},c)}}}function Rn(e,n){var t=function(t,o){e(t,o),n(t,o)};return t._merged=!0,t}function Bn(e,n,t,o,a,d){return(Array.isArray(t)||l(t))&&(a=o,o=t,t=void 0),s(d)&&(a=2),function(e,n,t,o,a){if(i(t)&&i(t.__ob__))return be();i(t)&&i(t.is)&&(n=t.is);if(!n)return be();0;Array.isArray(o)&&"function"==typeof o[0]&&((t=t||{}).scopedSlots={default:o[0]},o.length=0);2===a?o=hn(o):1===a&&(o=function(e){for(var n=0;n<e.length;n++)if(Array.isArray(e[n]))return Array.prototype.concat.apply([],e);return e}(o));var l,d;if("string"==typeof n){var c;d=e.$vnode&&e.$vnode.ns||M.getTagNamespace(n),l=M.isReservedTag(n)?new me(M.parsePlatformTagName(n),t,o,void 0,void 0,e):t&&t.pre||!i(c=_e(e.$options,"components",n))?new me(n,t,o,void 0,void 0,e):Gn(c,t,e,o,n)}else l=Gn(n,t,e,o);return Array.isArray(l)?l:i(l)?(i(d)&&function e(n,t,o){n.ns=t,"foreignObject"===n.tag&&(t=void 0,o=!0);if(i(n.children))for(var a=0,l=n.children.length;a<l;a++){var u=n.children[a];i(u.tag)&&(r(u.ns)||s(o)&&"svg"!==u.tag)&&e(u,t,o)}}(l,d),i(t)&&function(e){u(e.style)&&rn(e.style);u(e.class)&&rn(e.class)}(t),l):be()}(e,n,t,o,a)}var Wn,Fn=null;function Un(e,n){return(e.__esModule||le&&"Module"===e[Symbol.toStringTag])&&(e=e.default),u(e)?n.extend(e):e}function Yn(e){if(Array.isArray(e))for(var n=0;n<e.length;n++){var t=e[n];if(i(t)&&(i(t.componentOptions)||bn(t)))return t}}function Vn(e,n){Wn.$on(e,n)}function Jn(e,n){Wn.$off(e,n)}function $n(e,n){var t=Wn;return function o(){var a=n.apply(null,arguments);null!==a&&t.$off(e,o)}}function Kn(e,n,t){Wn=e,un(n,t||{},Vn,Jn,$n,e),Wn=void 0}var Zn=null;function Xn(e){var n=Zn;return Zn=e,function(){Zn=n}}function et(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function nt(e,n){if(n){if(e._directInactive=!1,et(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var t=0;t<e.$children.length;t++)nt(e.$children[t]);tt(e,"activated")}}function tt(e,n){pe();var t=e.$options[n],o=n+" hook";if(t)for(var a=0,r=t.length;a<r;a++)Fe(t[a],e,null,e,o);e._hasHookEvent&&e.$emit("hook:"+n),ye()}var ot=[],at=[],rt={},it=!1,st=!1,lt=0;var ut=0,dt=Date.now;if(U&&!$){var ct=window.performance;ct&&"function"==typeof ct.now&&dt()>document.createEvent("Event").timeStamp&&(dt=function(){return ct.now()})}function ht(){var e,n;for(ut=dt(),st=!0,ot.sort((function(e,n){return e.id-n.id})),lt=0;lt<ot.length;lt++)(e=ot[lt]).before&&e.before(),n=e.id,rt[n]=null,e.run();var t=at.slice(),o=ot.slice();lt=ot.length=at.length=0,rt={},it=st=!1,function(e){for(var n=0;n<e.length;n++)e[n]._inactive=!0,nt(e[n],!0)}(t),function(e){var n=e.length;for(;n--;){var t=e[n],o=t.vm;o._watcher===t&&o._isMounted&&!o._isDestroyed&&tt(o,"updated")}}(o),re&&M.devtools&&re.emit("flush")}var pt=0,yt=function(e,n,t,o,a){this.vm=e,a&&(e._watcher=this),e._watchers.push(this),o?(this.deep=!!o.deep,this.user=!!o.user,this.lazy=!!o.lazy,this.sync=!!o.sync,this.before=o.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++pt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new se,this.newDepIds=new se,this.expression="","function"==typeof n?this.getter=n:(this.getter=function(e){if(!B.test(e)){var n=e.split(".");return function(e){for(var t=0;t<n.length;t++){if(!e)return;e=e[n[t]]}return e}}}(n),this.getter||(this.getter=A)),this.value=this.lazy?void 0:this.get()};yt.prototype.get=function(){var e;pe(this);var n=this.vm;try{e=this.getter.call(n,n)}catch(e){if(!this.user)throw e;We(e,n,'getter for watcher "'+this.expression+'"')}finally{this.deep&&rn(e),ye(),this.cleanupDeps()}return e},yt.prototype.addDep=function(e){var n=e.id;this.newDepIds.has(n)||(this.newDepIds.add(n),this.newDeps.push(e),this.depIds.has(n)||e.addSub(this))},yt.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var n=this.deps[e];this.newDepIds.has(n.id)||n.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},yt.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(e){var n=e.id;if(null==rt[n]){if(rt[n]=!0,st){for(var t=ot.length-1;t>lt&&ot[t].id>e.id;)t--;ot.splice(t+1,0,e)}else ot.push(e);it||(it=!0,on(ht))}}(this)},yt.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||u(e)||this.deep){var n=this.value;if(this.value=e,this.user){var t='callback for watcher "'+this.expression+'"';Fe(this.cb,this.vm,[e,n],this.vm,t)}else this.cb.call(this.vm,e,n)}}},yt.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},yt.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},yt.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||w(this.vm._watchers,this);for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1}};var mt={enumerable:!0,configurable:!0,get:A,set:A};function gt(e,n,t){mt.get=function(){return this[n][t]},mt.set=function(e){this[n][t]=e},Object.defineProperty(e,t,mt)}function bt(e){e._watchers=[];var n=e.$options;n.props&&function(e,n){var t=e.$options.propsData||{},o=e._props={},a=e.$options._propKeys=[];e.$parent&&je(!1);var r=function(r){a.push(r);var i=Ne(r,n,t,e);Ie(o,r,i),r in e||gt(e,"_props",r)};for(var i in n)r(i);je(!0)}(e,n.props),n.methods&&function(e,n){e.$options.props;for(var t in n)e[t]="function"!=typeof n[t]?A:P(n[t],e)}(e,n.methods),n.data?function(e){var n=e.$options.data;c(n=e._data="function"==typeof n?function(e,n){pe();try{return e.call(n,n)}catch(e){return We(e,n,"data()"),{}}finally{ye()}}(n,e):n||{})||(n={});var t=Object.keys(n),o=e.$options.props,a=(e.$options.methods,t.length);for(;a--;){var r=t[a];0,o&&k(o,r)||(i=void 0,36!==(i=(r+"").charCodeAt(0))&&95!==i&&gt(e,"_data",r))}var i;Te(n,!0)}(e):Te(e._data={},!0),n.computed&&function(e,n){var t=e._computedWatchers=Object.create(null),o=ae();for(var a in n){var r=n[a],i="function"==typeof r?r:r.get;0,o||(t[a]=new yt(e,i||A,A,ft)),a in e||wt(e,a,r)}}(e,n.computed),n.watch&&n.watch!==ne&&function(e,n){for(var t in n){var o=n[t];if(Array.isArray(o))for(var a=0;a<o.length;a++)qt(e,t,o[a]);else qt(e,t,o)}}(e,n.watch)}var ft={lazy:!0};function wt(e,n,t){var o=!ae();"function"==typeof t?(mt.get=o?vt(n):kt(t),mt.set=A):(mt.get=t.get?o&&!1!==t.cache?vt(n):kt(t.get):A,mt.set=t.set||A),Object.defineProperty(e,n,mt)}function vt(e){return function(){var n=this._computedWatchers&&this._computedWatchers[e];if(n)return n.dirty&&n.evaluate(),ce.target&&n.depend(),n.value}}function kt(e){return function(){return e.call(this,this)}}function qt(e,n,t,o){return c(t)&&(o=t,t=t.handler),"string"==typeof t&&(t=e[t]),e.$watch(n,t,o)}var xt=0;function jt(e){var n=e.options;if(e.super){var t=jt(e.super);if(t!==e.superOptions){e.superOptions=t;var o=function(e){var n,t=e.options,o=e.sealedOptions;for(var a in t)t[a]!==o[a]&&(n||(n={}),n[a]=t[a]);return n}(e);o&&z(e.extendOptions,o),(n=e.options=De(t,e.extendOptions)).name&&(n.components[n.name]=e)}}return n}function St(e){this._init(e)}function Tt(e){e.cid=0;var n=1;e.extend=function(e){e=e||{};var t=this,o=t.cid,a=e._Ctor||(e._Ctor={});if(a[o])return a[o];var r=e.name||t.options.name;var i=function(e){this._init(e)};return(i.prototype=Object.create(t.prototype)).constructor=i,i.cid=n++,i.options=De(t.options,e),i.super=t,i.options.props&&function(e){var n=e.options.props;for(var t in n)gt(e.prototype,"_props",t)}(i),i.options.computed&&function(e){var n=e.options.computed;for(var t in n)wt(e.prototype,t,n[t])}(i),i.extend=t.extend,i.mixin=t.mixin,i.use=t.use,_.forEach((function(e){i[e]=t[e]})),r&&(i.options.components[r]=i),i.superOptions=t.options,i.extendOptions=e,i.sealedOptions=z({},i.options),a[o]=i,i}}function It(e){return e&&(e.Ctor.options.name||e.tag)}function Pt(e,n){return Array.isArray(e)?e.indexOf(n)>-1:"string"==typeof e?e.split(",").indexOf(n)>-1:!!h(e)&&e.test(n)}function Qt(e,n){var t=e.cache,o=e.keys,a=e._vnode;for(var r in t){var i=t[r];if(i){var s=i.name;s&&!n(s)&&zt(t,r,o,a)}}}function zt(e,n,t,o){var a=e[n];!a||o&&a.tag===o.tag||a.componentInstance.$destroy(),e[n]=null,w(t,n)}St.prototype._init=function(e){var n=this;n._uid=xt++,n._isVue=!0,e&&e._isComponent?function(e,n){var t=e.$options=Object.create(e.constructor.options),o=n._parentVnode;t.parent=n.parent,t._parentVnode=o;var a=o.componentOptions;t.propsData=a.propsData,t._parentListeners=a.listeners,t._renderChildren=a.children,t._componentTag=a.tag,n.render&&(t.render=n.render,t.staticRenderFns=n.staticRenderFns)}(n,e):n.$options=De(jt(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(e){var n=e.$options,t=n.parent;if(t&&!n.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(e)}e.$parent=t,e.$root=t?t.$root:e,e.$children=[],e.$refs={},e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(n),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var n=e.$options._parentListeners;n&&Kn(e,n)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,t=e.$vnode=n._parentVnode,o=t&&t.context;e.$slots=mn(n._renderChildren,o),e.$scopedSlots=a,e._c=function(n,t,o,a){return Bn(e,n,t,o,a,!1)},e.$createElement=function(n,t,o,a){return Bn(e,n,t,o,a,!0)};var r=t&&t.data;Ie(e,"$attrs",r&&r.attrs||a,null,!0),Ie(e,"$listeners",n._parentListeners||a,null,!0)}(n),tt(n,"beforeCreate"),function(e){var n=yn(e.$options.inject,e);n&&(je(!1),Object.keys(n).forEach((function(t){Ie(e,t,n[t])})),je(!0))}(n),bt(n),function(e){var n=e.$options.provide;n&&(e._provided="function"==typeof n?n.call(e):n)}(n),tt(n,"created"),n.$options.el&&n.$mount(n.$options.el)},function(e){var n={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",n),Object.defineProperty(e.prototype,"$props",t),e.prototype.$set=Pe,e.prototype.$delete=Qe,e.prototype.$watch=function(e,n,t){if(c(n))return qt(this,e,n,t);(t=t||{}).user=!0;var o=new yt(this,e,n,t);if(t.immediate){var a='callback for immediate watcher "'+o.expression+'"';pe(),Fe(n,this,[o.value],this,a),ye()}return function(){o.teardown()}}}(St),function(e){var n=/^hook:/;e.prototype.$on=function(e,t){var o=this;if(Array.isArray(e))for(var a=0,r=e.length;a<r;a++)o.$on(e[a],t);else(o._events[e]||(o._events[e]=[])).push(t),n.test(e)&&(o._hasHookEvent=!0);return o},e.prototype.$once=function(e,n){var t=this;function o(){t.$off(e,o),n.apply(t,arguments)}return o.fn=n,t.$on(e,o),t},e.prototype.$off=function(e,n){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(e)){for(var o=0,a=e.length;o<a;o++)t.$off(e[o],n);return t}var r,i=t._events[e];if(!i)return t;if(!n)return t._events[e]=null,t;for(var s=i.length;s--;)if((r=i[s])===n||r.fn===n){i.splice(s,1);break}return t},e.prototype.$emit=function(e){var n=this,t=n._events[e];if(t){t=t.length>1?Q(t):t;for(var o=Q(arguments,1),a='event handler for "'+e+'"',r=0,i=t.length;r<i;r++)Fe(t[r],n,o,n,a)}return n}}(St),function(e){e.prototype._update=function(e,n){var t=this,o=t.$el,a=t._vnode,r=Xn(t);t._vnode=e,t.$el=a?t.__patch__(a,e):t.__patch__(t.$el,e,n,!1),r(),o&&(o.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){tt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var n=e.$parent;!n||n._isBeingDestroyed||e.$options.abstract||w(n.$children,e),e._watcher&&e._watcher.teardown();for(var t=e._watchers.length;t--;)e._watchers[t].teardown();e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),tt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(St),function(e){On(e.prototype),e.prototype.$nextTick=function(e){return on(e,this)},e.prototype._render=function(){var e,n=this,t=n.$options,o=t.render,a=t._parentVnode;a&&(n.$scopedSlots=fn(a.data.scopedSlots,n.$slots,n.$scopedSlots)),n.$vnode=a;try{Fn=n,e=o.call(n._renderProxy,n.$createElement)}catch(t){We(t,n,"render"),e=n._vnode}finally{Fn=null}return Array.isArray(e)&&1===e.length&&(e=e[0]),e instanceof me||(e=be()),e.parent=a,e}}(St);var Ct=[String,RegExp,Array],At={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Ct,exclude:Ct,max:[String,Number]},methods:{cacheVNode:function(){var e=this.cache,n=this.keys,t=this.vnodeToCache,o=this.keyToCache;if(t){var a=t.tag,r=t.componentInstance,i=t.componentOptions;e[o]={name:It(i),tag:a,componentInstance:r},n.push(o),this.max&&n.length>parseInt(this.max)&&zt(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)zt(this.cache,e,this.keys)},mounted:function(){var e=this;this.cacheVNode(),this.$watch("include",(function(n){Qt(e,(function(e){return Pt(n,e)}))})),this.$watch("exclude",(function(n){Qt(e,(function(e){return!Pt(n,e)}))}))},updated:function(){this.cacheVNode()},render:function(){var e=this.$slots.default,n=Yn(e),t=n&&n.componentOptions;if(t){var o=It(t),a=this.include,r=this.exclude;if(a&&(!o||!Pt(a,o))||r&&o&&Pt(r,o))return n;var i=this.cache,s=this.keys,l=null==n.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):n.key;i[l]?(n.componentInstance=i[l].componentInstance,w(s,l),s.push(l)):(this.vnodeToCache=n,this.keyToCache=l),n.data.keepAlive=!0}return n||e&&e[0]}}};!function(e){var n={get:function(){return M}};Object.defineProperty(e,"config",n),e.util={warn:ue,extend:z,mergeOptions:De,defineReactive:Ie},e.set=Pe,e.delete=Qe,e.nextTick=on,e.observable=function(e){return Te(e),e},e.options=Object.create(null),_.forEach((function(n){e.options[n+"s"]=Object.create(null)})),e.options._base=e,z(e.options.components,At),function(e){e.use=function(e){var n=this._installedPlugins||(this._installedPlugins=[]);if(n.indexOf(e)>-1)return this;var t=Q(arguments,1);return t.unshift(this),"function"==typeof e.install?e.install.apply(e,t):"function"==typeof e&&e.apply(null,t),n.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=De(this.options,e),this}}(e),Tt(e),function(e){_.forEach((function(n){e[n]=function(e,t){return t?("component"===n&&c(t)&&(t.name=t.name||e,t=this.options._base.extend(t)),"directive"===n&&"function"==typeof t&&(t={bind:t,update:t}),this.options[n+"s"][e]=t,t):this.options[n+"s"][e]}}))}(e)}(St),Object.defineProperty(St.prototype,"$isServer",{get:ae}),Object.defineProperty(St.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(St,"FunctionalRenderContext",{value:Hn}),St.version="2.6.14";var Et=b("style,class"),Lt=b("input,textarea,option,select,progress"),Ot=b("contenteditable,draggable,spellcheck"),Ht=b("events,caret,typing,plaintext-only"),Dt=b("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),_t="http://www.w3.org/1999/xlink",Nt=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},Mt=function(e){return Nt(e)?e.slice(6,e.length):""},Gt=function(e){return null==e||!1===e};function Rt(e){for(var n=e.data,t=e,o=e;i(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(n=Bt(o.data,n));for(;i(t=t.parent);)t&&t.data&&(n=Bt(n,t.data));return function(e,n){if(i(e)||i(n))return Wt(e,Ft(n));return""}(n.staticClass,n.class)}function Bt(e,n){return{staticClass:Wt(e.staticClass,n.staticClass),class:i(e.class)?[e.class,n.class]:n.class}}function Wt(e,n){return e?n?e+" "+n:e:n||""}function Ft(e){return Array.isArray(e)?function(e){for(var n,t="",o=0,a=e.length;o<a;o++)i(n=Ft(e[o]))&&""!==n&&(t&&(t+=" "),t+=n);return t}(e):u(e)?function(e){var n="";for(var t in e)e[t]&&(n&&(n+=" "),n+=t);return n}(e):"string"==typeof e?e:""}var Ut={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Yt=b("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Vt=b("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Jt=function(e){return Yt(e)||Vt(e)};var $t=Object.create(null);var Kt=b("text,number,password,search,email,tel,url");var Zt=Object.freeze({createElement:function(e,n){var t=document.createElement(e);return"select"!==e||n.data&&n.data.attrs&&void 0!==n.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(e,n){return document.createElementNS(Ut[e],n)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,n,t){e.insertBefore(n,t)},removeChild:function(e,n){e.removeChild(n)},appendChild:function(e,n){e.appendChild(n)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,n){e.textContent=n},setStyleScope:function(e,n){e.setAttribute(n,"")}}),Xt={create:function(e,n){eo(n)},update:function(e,n){e.data.ref!==n.data.ref&&(eo(e,!0),eo(n))},destroy:function(e){eo(e,!0)}};function eo(e,n){var t=e.data.ref;if(i(t)){var o=e.context,a=e.componentInstance||e.elm,r=o.$refs;n?Array.isArray(r[t])?w(r[t],a):r[t]===a&&(r[t]=void 0):e.data.refInFor?Array.isArray(r[t])?r[t].indexOf(a)<0&&r[t].push(a):r[t]=[a]:r[t]=a}}var no=new me("",{},[]),to=["create","activate","update","remove","destroy"];function oo(e,n){return e.key===n.key&&e.asyncFactory===n.asyncFactory&&(e.tag===n.tag&&e.isComment===n.isComment&&i(e.data)===i(n.data)&&function(e,n){if("input"!==e.tag)return!0;var t,o=i(t=e.data)&&i(t=t.attrs)&&t.type,a=i(t=n.data)&&i(t=t.attrs)&&t.type;return o===a||Kt(o)&&Kt(a)}(e,n)||s(e.isAsyncPlaceholder)&&r(n.asyncFactory.error))}function ao(e,n,t){var o,a,r={};for(o=n;o<=t;++o)i(a=e[o].key)&&(r[a]=o);return r}var ro={create:io,update:io,destroy:function(e){io(e,no)}};function io(e,n){(e.data.directives||n.data.directives)&&function(e,n){var t,o,a,r=e===no,i=n===no,s=lo(e.data.directives,e.context),l=lo(n.data.directives,n.context),u=[],d=[];for(t in l)o=s[t],a=l[t],o?(a.oldValue=o.value,a.oldArg=o.arg,co(a,"update",n,e),a.def&&a.def.componentUpdated&&d.push(a)):(co(a,"bind",n,e),a.def&&a.def.inserted&&u.push(a));if(u.length){var c=function(){for(var t=0;t<u.length;t++)co(u[t],"inserted",n,e)};r?dn(n,"insert",c):c()}d.length&&dn(n,"postpatch",(function(){for(var t=0;t<d.length;t++)co(d[t],"componentUpdated",n,e)}));if(!r)for(t in s)l[t]||co(s[t],"unbind",e,e,i)}(e,n)}var so=Object.create(null);function lo(e,n){var t,o,a=Object.create(null);if(!e)return a;for(t=0;t<e.length;t++)(o=e[t]).modifiers||(o.modifiers=so),a[uo(o)]=o,o.def=_e(n.$options,"directives",o.name);return a}function uo(e){return e.rawName||e.name+"."+Object.keys(e.modifiers||{}).join(".")}function co(e,n,t,o,a){var r=e.def&&e.def[n];if(r)try{r(t.elm,e,t,o,a)}catch(o){We(o,t.context,"directive "+e.name+" "+n+" hook")}}var ho=[Xt,ro];function po(e,n){var t=n.componentOptions;if(!(i(t)&&!1===t.Ctor.options.inheritAttrs||r(e.data.attrs)&&r(n.data.attrs))){var o,a,s=n.elm,l=e.data.attrs||{},u=n.data.attrs||{};for(o in i(u.__ob__)&&(u=n.data.attrs=z({},u)),u)a=u[o],l[o]!==a&&yo(s,o,a,n.data.pre);for(o in($||Z)&&u.value!==l.value&&yo(s,"value",u.value),l)r(u[o])&&(Nt(o)?s.removeAttributeNS(_t,Mt(o)):Ot(o)||s.removeAttribute(o))}}function yo(e,n,t,o){o||e.tagName.indexOf("-")>-1?mo(e,n,t):Dt(n)?Gt(t)?e.removeAttribute(n):(t="allowfullscreen"===n&&"EMBED"===e.tagName?"true":n,e.setAttribute(n,t)):Ot(n)?e.setAttribute(n,function(e,n){return Gt(n)||"false"===n?"false":"contenteditable"===e&&Ht(n)?n:"true"}(n,t)):Nt(n)?Gt(t)?e.removeAttributeNS(_t,Mt(n)):e.setAttributeNS(_t,n,t):mo(e,n,t)}function mo(e,n,t){if(Gt(t))e.removeAttribute(n);else{if($&&!K&&"TEXTAREA"===e.tagName&&"placeholder"===n&&""!==t&&!e.__ieph){var o=function(n){n.stopImmediatePropagation(),e.removeEventListener("input",o)};e.addEventListener("input",o),e.__ieph=!0}e.setAttribute(n,t)}}var go={create:po,update:po};function bo(e,n){var t=n.elm,o=n.data,a=e.data;if(!(r(o.staticClass)&&r(o.class)&&(r(a)||r(a.staticClass)&&r(a.class)))){var s=Rt(n),l=t._transitionClasses;i(l)&&(s=Wt(s,Ft(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var fo,wo={create:bo,update:bo};function vo(e,n,t){var o=fo;return function a(){var r=n.apply(null,arguments);null!==r&&xo(e,a,t,o)}}var ko=Je&&!(ee&&Number(ee[1])<=53);function qo(e,n,t,o){if(ko){var a=ut,r=n;n=r._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=a||e.timeStamp<=0||e.target.ownerDocument!==document)return r.apply(this,arguments)}}fo.addEventListener(e,n,te?{capture:t,passive:o}:t)}function xo(e,n,t,o){(o||fo).removeEventListener(e,n._wrapper||n,t)}function jo(e,n){if(!r(e.data.on)||!r(n.data.on)){var t=n.data.on||{},o=e.data.on||{};fo=n.elm,function(e){if(i(e.__r)){var n=$?"change":"input";e[n]=[].concat(e.__r,e[n]||[]),delete e.__r}i(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(t),un(t,o,qo,xo,vo,n.context),fo=void 0}}var So,To={create:jo,update:jo};function Io(e,n){if(!r(e.data.domProps)||!r(n.data.domProps)){var t,o,a=n.elm,s=e.data.domProps||{},l=n.data.domProps||{};for(t in i(l.__ob__)&&(l=n.data.domProps=z({},l)),s)t in l||(a[t]="");for(t in l){if(o=l[t],"textContent"===t||"innerHTML"===t){if(n.children&&(n.children.length=0),o===s[t])continue;1===a.childNodes.length&&a.removeChild(a.childNodes[0])}if("value"===t&&"PROGRESS"!==a.tagName){a._value=o;var u=r(o)?"":String(o);Po(a,u)&&(a.value=u)}else if("innerHTML"===t&&Vt(a.tagName)&&r(a.innerHTML)){(So=So||document.createElement("div")).innerHTML="<svg>"+o+"</svg>";for(var d=So.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;d.firstChild;)a.appendChild(d.firstChild)}else if(o!==s[t])try{a[t]=o}catch(e){}}}}function Po(e,n){return!e.composing&&("OPTION"===e.tagName||function(e,n){var t=!0;try{t=document.activeElement!==e}catch(e){}return t&&e.value!==n}(e,n)||function(e,n){var t=e.value,o=e._vModifiers;if(i(o)){if(o.number)return g(t)!==g(n);if(o.trim)return t.trim()!==n.trim()}return t!==n}(e,n))}var Qo={create:Io,update:Io},zo=q((function(e){var n={},t=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var o=e.split(t);o.length>1&&(n[o[0].trim()]=o[1].trim())}})),n}));function Co(e){var n=Ao(e.style);return e.staticStyle?z(e.staticStyle,n):n}function Ao(e){return Array.isArray(e)?C(e):"string"==typeof e?zo(e):e}var Eo,Lo=/^--/,Oo=/\s*!important$/,Ho=function(e,n,t){if(Lo.test(n))e.style.setProperty(n,t);else if(Oo.test(t))e.style.setProperty(I(n),t.replace(Oo,""),"important");else{var o=_o(n);if(Array.isArray(t))for(var a=0,r=t.length;a<r;a++)e.style[o]=t[a];else e.style[o]=t}},Do=["Webkit","Moz","ms"],_o=q((function(e){if(Eo=Eo||document.createElement("div").style,"filter"!==(e=j(e))&&e in Eo)return e;for(var n=e.charAt(0).toUpperCase()+e.slice(1),t=0;t<Do.length;t++){var o=Do[t]+n;if(o in Eo)return o}}));function No(e,n){var t=n.data,o=e.data;if(!(r(t.staticStyle)&&r(t.style)&&r(o.staticStyle)&&r(o.style))){var a,s,l=n.elm,u=o.staticStyle,d=o.normalizedStyle||o.style||{},c=u||d,h=Ao(n.data.style)||{};n.data.normalizedStyle=i(h.__ob__)?z({},h):h;var p=function(e,n){var t,o={};if(n)for(var a=e;a.componentInstance;)(a=a.componentInstance._vnode)&&a.data&&(t=Co(a.data))&&z(o,t);(t=Co(e.data))&&z(o,t);for(var r=e;r=r.parent;)r.data&&(t=Co(r.data))&&z(o,t);return o}(n,!0);for(s in c)r(p[s])&&Ho(l,s,"");for(s in p)(a=p[s])!==c[s]&&Ho(l,s,null==a?"":a)}}var Mo={create:No,update:No},Go=/\s+/;function Ro(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(Go).forEach((function(n){return e.classList.add(n)})):e.classList.add(n);else{var t=" "+(e.getAttribute("class")||"")+" ";t.indexOf(" "+n+" ")<0&&e.setAttribute("class",(t+n).trim())}}function Bo(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(Go).forEach((function(n){return e.classList.remove(n)})):e.classList.remove(n),e.classList.length||e.removeAttribute("class");else{for(var t=" "+(e.getAttribute("class")||"")+" ",o=" "+n+" ";t.indexOf(o)>=0;)t=t.replace(o," ");(t=t.trim())?e.setAttribute("class",t):e.removeAttribute("class")}}function Wo(e){if(e){if("object"==typeof e){var n={};return!1!==e.css&&z(n,Fo(e.name||"v")),z(n,e),n}return"string"==typeof e?Fo(e):void 0}}var Fo=q((function(e){return{enterClass:e+"-enter",enterToClass:e+"-enter-to",enterActiveClass:e+"-enter-active",leaveClass:e+"-leave",leaveToClass:e+"-leave-to",leaveActiveClass:e+"-leave-active"}})),Uo=U&&!K,Yo="transition",Vo="transitionend",Jo="animation",$o="animationend";Uo&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Yo="WebkitTransition",Vo="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Jo="WebkitAnimation",$o="webkitAnimationEnd"));var Ko=U?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function Zo(e){Ko((function(){Ko(e)}))}function Xo(e,n){var t=e._transitionClasses||(e._transitionClasses=[]);t.indexOf(n)<0&&(t.push(n),Ro(e,n))}function ea(e,n){e._transitionClasses&&w(e._transitionClasses,n),Bo(e,n)}function na(e,n,t){var o=oa(e,n),a=o.type,r=o.timeout,i=o.propCount;if(!a)return t();var s="transition"===a?Vo:$o,l=0,u=function(){e.removeEventListener(s,d),t()},d=function(n){n.target===e&&++l>=i&&u()};setTimeout((function(){l<i&&u()}),r+1),e.addEventListener(s,d)}var ta=/\b(transform|all)(,|$)/;function oa(e,n){var t,o=window.getComputedStyle(e),a=(o[Yo+"Delay"]||"").split(", "),r=(o[Yo+"Duration"]||"").split(", "),i=aa(a,r),s=(o[Jo+"Delay"]||"").split(", "),l=(o[Jo+"Duration"]||"").split(", "),u=aa(s,l),d=0,c=0;return"transition"===n?i>0&&(t="transition",d=i,c=r.length):"animation"===n?u>0&&(t="animation",d=u,c=l.length):c=(t=(d=Math.max(i,u))>0?i>u?"transition":"animation":null)?"transition"===t?r.length:l.length:0,{type:t,timeout:d,propCount:c,hasTransform:"transition"===t&&ta.test(o[Yo+"Property"])}}function aa(e,n){for(;e.length<n.length;)e=e.concat(e);return Math.max.apply(null,n.map((function(n,t){return ra(n)+ra(e[t])})))}function ra(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function ia(e,n){var t=e.elm;i(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var o=Wo(e.data.transition);if(!r(o)&&!i(t._enterCb)&&1===t.nodeType){for(var a=o.css,s=o.type,l=o.enterClass,d=o.enterToClass,c=o.enterActiveClass,h=o.appearClass,p=o.appearToClass,y=o.appearActiveClass,m=o.beforeEnter,b=o.enter,f=o.afterEnter,w=o.enterCancelled,v=o.beforeAppear,k=o.appear,q=o.afterAppear,x=o.appearCancelled,j=o.duration,S=Zn,T=Zn.$vnode;T&&T.parent;)S=T.context,T=T.parent;var I=!S._isMounted||!e.isRootInsert;if(!I||k||""===k){var P=I&&h?h:l,Q=I&&y?y:c,z=I&&p?p:d,C=I&&v||m,A=I&&"function"==typeof k?k:b,E=I&&q||f,L=I&&x||w,O=g(u(j)?j.enter:j);0;var H=!1!==a&&!K,_=ua(A),N=t._enterCb=D((function(){H&&(ea(t,z),ea(t,Q)),N.cancelled?(H&&ea(t,P),L&&L(t)):E&&E(t),t._enterCb=null}));e.data.show||dn(e,"insert",(function(){var n=t.parentNode,o=n&&n._pending&&n._pending[e.key];o&&o.tag===e.tag&&o.elm._leaveCb&&o.elm._leaveCb(),A&&A(t,N)})),C&&C(t),H&&(Xo(t,P),Xo(t,Q),Zo((function(){ea(t,P),N.cancelled||(Xo(t,z),_||(la(O)?setTimeout(N,O):na(t,s,N)))}))),e.data.show&&(n&&n(),A&&A(t,N)),H||_||N()}}}function sa(e,n){var t=e.elm;i(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var o=Wo(e.data.transition);if(r(o)||1!==t.nodeType)return n();if(!i(t._leaveCb)){var a=o.css,s=o.type,l=o.leaveClass,d=o.leaveToClass,c=o.leaveActiveClass,h=o.beforeLeave,p=o.leave,y=o.afterLeave,m=o.leaveCancelled,b=o.delayLeave,f=o.duration,w=!1!==a&&!K,v=ua(p),k=g(u(f)?f.leave:f);0;var q=t._leaveCb=D((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[e.key]=null),w&&(ea(t,d),ea(t,c)),q.cancelled?(w&&ea(t,l),m&&m(t)):(n(),y&&y(t)),t._leaveCb=null}));b?b(x):x()}function x(){q.cancelled||(!e.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[e.key]=e),h&&h(t),w&&(Xo(t,l),Xo(t,c),Zo((function(){ea(t,l),q.cancelled||(Xo(t,d),v||(la(k)?setTimeout(q,k):na(t,s,q)))}))),p&&p(t,q),w||v||q())}}function la(e){return"number"==typeof e&&!isNaN(e)}function ua(e){if(r(e))return!1;var n=e.fns;return i(n)?ua(Array.isArray(n)?n[0]:n):(e._length||e.length)>1}function da(e,n){!0!==n.data.show&&ia(n)}var ca=function(e){var n,t,o={},a=e.modules,u=e.nodeOps;for(n=0;n<to.length;++n)for(o[to[n]]=[],t=0;t<a.length;++t)i(a[t][to[n]])&&o[to[n]].push(a[t][to[n]]);function d(e){var n=u.parentNode(e);i(n)&&u.removeChild(n,e)}function c(e,n,t,a,r,l,d){if(i(e.elm)&&i(l)&&(e=l[d]=we(e)),e.isRootInsert=!r,!function(e,n,t,a){var r=e.data;if(i(r)){var l=i(e.componentInstance)&&r.keepAlive;if(i(r=r.hook)&&i(r=r.init)&&r(e,!1),i(e.componentInstance))return h(e,n),p(t,e.elm,a),s(l)&&function(e,n,t,a){var r,s=e;for(;s.componentInstance;)if(s=s.componentInstance._vnode,i(r=s.data)&&i(r=r.transition)){for(r=0;r<o.activate.length;++r)o.activate[r](no,s);n.push(s);break}p(t,e.elm,a)}(e,n,t,a),!0}}(e,n,t,a)){var c=e.data,m=e.children,b=e.tag;i(b)?(e.elm=e.ns?u.createElementNS(e.ns,b):u.createElement(b,e),f(e),y(e,m,n),i(c)&&g(e,n),p(t,e.elm,a)):s(e.isComment)?(e.elm=u.createComment(e.text),p(t,e.elm,a)):(e.elm=u.createTextNode(e.text),p(t,e.elm,a))}}function h(e,n){i(e.data.pendingInsert)&&(n.push.apply(n,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,m(e)?(g(e,n),f(e)):(eo(e),n.push(e))}function p(e,n,t){i(e)&&(i(t)?u.parentNode(t)===e&&u.insertBefore(e,n,t):u.appendChild(e,n))}function y(e,n,t){if(Array.isArray(n)){0;for(var o=0;o<n.length;++o)c(n[o],t,e.elm,null,!0,n,o)}else l(e.text)&&u.appendChild(e.elm,u.createTextNode(String(e.text)))}function m(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return i(e.tag)}function g(e,t){for(var a=0;a<o.create.length;++a)o.create[a](no,e);i(n=e.data.hook)&&(i(n.create)&&n.create(no,e),i(n.insert)&&t.push(e))}function f(e){var n;if(i(n=e.fnScopeId))u.setStyleScope(e.elm,n);else for(var t=e;t;)i(n=t.context)&&i(n=n.$options._scopeId)&&u.setStyleScope(e.elm,n),t=t.parent;i(n=Zn)&&n!==e.context&&n!==e.fnContext&&i(n=n.$options._scopeId)&&u.setStyleScope(e.elm,n)}function w(e,n,t,o,a,r){for(;o<=a;++o)c(t[o],r,e,n,!1,t,o)}function v(e){var n,t,a=e.data;if(i(a))for(i(n=a.hook)&&i(n=n.destroy)&&n(e),n=0;n<o.destroy.length;++n)o.destroy[n](e);if(i(n=e.children))for(t=0;t<e.children.length;++t)v(e.children[t])}function k(e,n,t){for(;n<=t;++n){var o=e[n];i(o)&&(i(o.tag)?(q(o),v(o)):d(o.elm))}}function q(e,n){if(i(n)||i(e.data)){var t,a=o.remove.length+1;for(i(n)?n.listeners+=a:n=function(e,n){function t(){0==--t.listeners&&d(e)}return t.listeners=n,t}(e.elm,a),i(t=e.componentInstance)&&i(t=t._vnode)&&i(t.data)&&q(t,n),t=0;t<o.remove.length;++t)o.remove[t](e,n);i(t=e.data.hook)&&i(t=t.remove)?t(e,n):n()}else d(e.elm)}function x(e,n,t,o){for(var a=t;a<o;a++){var r=n[a];if(i(r)&&oo(e,r))return a}}function j(e,n,t,a,l,d){if(e!==n){i(n.elm)&&i(a)&&(n=a[l]=we(n));var h=n.elm=e.elm;if(s(e.isAsyncPlaceholder))i(n.asyncFactory.resolved)?I(e.elm,n,t):n.isAsyncPlaceholder=!0;else if(s(n.isStatic)&&s(e.isStatic)&&n.key===e.key&&(s(n.isCloned)||s(n.isOnce)))n.componentInstance=e.componentInstance;else{var p,y=n.data;i(y)&&i(p=y.hook)&&i(p=p.prepatch)&&p(e,n);var g=e.children,b=n.children;if(i(y)&&m(n)){for(p=0;p<o.update.length;++p)o.update[p](e,n);i(p=y.hook)&&i(p=p.update)&&p(e,n)}r(n.text)?i(g)&&i(b)?g!==b&&function(e,n,t,o,a){var s,l,d,h=0,p=0,y=n.length-1,m=n[0],g=n[y],b=t.length-1,f=t[0],v=t[b],q=!a;for(0;h<=y&&p<=b;)r(m)?m=n[++h]:r(g)?g=n[--y]:oo(m,f)?(j(m,f,o,t,p),m=n[++h],f=t[++p]):oo(g,v)?(j(g,v,o,t,b),g=n[--y],v=t[--b]):oo(m,v)?(j(m,v,o,t,b),q&&u.insertBefore(e,m.elm,u.nextSibling(g.elm)),m=n[++h],v=t[--b]):oo(g,f)?(j(g,f,o,t,p),q&&u.insertBefore(e,g.elm,m.elm),g=n[--y],f=t[++p]):(r(s)&&(s=ao(n,h,y)),r(l=i(f.key)?s[f.key]:x(f,n,h,y))?c(f,o,e,m.elm,!1,t,p):oo(d=n[l],f)?(j(d,f,o,t,p),n[l]=void 0,q&&u.insertBefore(e,d.elm,m.elm)):c(f,o,e,m.elm,!1,t,p),f=t[++p]);h>y?w(e,r(t[b+1])?null:t[b+1].elm,t,p,b,o):p>b&&k(n,h,y)}(h,g,b,t,d):i(b)?(i(e.text)&&u.setTextContent(h,""),w(h,null,b,0,b.length-1,t)):i(g)?k(g,0,g.length-1):i(e.text)&&u.setTextContent(h,""):e.text!==n.text&&u.setTextContent(h,n.text),i(y)&&i(p=y.hook)&&i(p=p.postpatch)&&p(e,n)}}}function S(e,n,t){if(s(t)&&i(e.parent))e.parent.data.pendingInsert=n;else for(var o=0;o<n.length;++o)n[o].data.hook.insert(n[o])}var T=b("attrs,class,staticClass,staticStyle,key");function I(e,n,t,o){var a,r=n.tag,l=n.data,u=n.children;if(o=o||l&&l.pre,n.elm=e,s(n.isComment)&&i(n.asyncFactory))return n.isAsyncPlaceholder=!0,!0;if(i(l)&&(i(a=l.hook)&&i(a=a.init)&&a(n,!0),i(a=n.componentInstance)))return h(n,t),!0;if(i(r)){if(i(u))if(e.hasChildNodes())if(i(a=l)&&i(a=a.domProps)&&i(a=a.innerHTML)){if(a!==e.innerHTML)return!1}else{for(var d=!0,c=e.firstChild,p=0;p<u.length;p++){if(!c||!I(c,u[p],t,o)){d=!1;break}c=c.nextSibling}if(!d||c)return!1}else y(n,u,t);if(i(l)){var m=!1;for(var b in l)if(!T(b)){m=!0,g(n,t);break}!m&&l.class&&rn(l.class)}}else e.data!==n.text&&(e.data=n.text);return!0}return function(e,n,t,a){if(!r(n)){var l,d=!1,h=[];if(r(e))d=!0,c(n,h);else{var p=i(e.nodeType);if(!p&&oo(e,n))j(e,n,h,null,null,a);else{if(p){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),t=!0),s(t)&&I(e,n,h))return S(n,h,!0),e;l=e,e=new me(u.tagName(l).toLowerCase(),{},[],void 0,l)}var y=e.elm,g=u.parentNode(y);if(c(n,h,y._leaveCb?null:g,u.nextSibling(y)),i(n.parent))for(var b=n.parent,f=m(n);b;){for(var w=0;w<o.destroy.length;++w)o.destroy[w](b);if(b.elm=n.elm,f){for(var q=0;q<o.create.length;++q)o.create[q](no,b);var x=b.data.hook.insert;if(x.merged)for(var T=1;T<x.fns.length;T++)x.fns[T]()}else eo(b);b=b.parent}i(g)?k([e],0,0):i(e.tag)&&v(e)}}return S(n,h,d),n.elm}i(e)&&v(e)}}({nodeOps:Zt,modules:[go,wo,To,Qo,Mo,U?{create:da,activate:da,remove:function(e,n){!0!==e.data.show?sa(e,n):n()}}:{}].concat(ho)});K&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&wa(e,"input")}));var ha={inserted:function(e,n,t,o){"select"===t.tag?(o.elm&&!o.elm._vOptions?dn(t,"postpatch",(function(){ha.componentUpdated(e,n,t)})):pa(e,n,t.context),e._vOptions=[].map.call(e.options,ga)):("textarea"===t.tag||Kt(e.type))&&(e._vModifiers=n.modifiers,n.modifiers.lazy||(e.addEventListener("compositionstart",ba),e.addEventListener("compositionend",fa),e.addEventListener("change",fa),K&&(e.vmodel=!0)))},componentUpdated:function(e,n,t){if("select"===t.tag){pa(e,n,t.context);var o=e._vOptions,a=e._vOptions=[].map.call(e.options,ga);if(a.some((function(e,n){return!O(e,o[n])})))(e.multiple?n.value.some((function(e){return ma(e,a)})):n.value!==n.oldValue&&ma(n.value,a))&&wa(e,"change")}}};function pa(e,n,t){ya(e,n,t),($||Z)&&setTimeout((function(){ya(e,n,t)}),0)}function ya(e,n,t){var o=n.value,a=e.multiple;if(!a||Array.isArray(o)){for(var r,i,s=0,l=e.options.length;s<l;s++)if(i=e.options[s],a)r=H(o,ga(i))>-1,i.selected!==r&&(i.selected=r);else if(O(ga(i),o))return void(e.selectedIndex!==s&&(e.selectedIndex=s));a||(e.selectedIndex=-1)}}function ma(e,n){return n.every((function(n){return!O(n,e)}))}function ga(e){return"_value"in e?e._value:e.value}function ba(e){e.target.composing=!0}function fa(e){e.target.composing&&(e.target.composing=!1,wa(e.target,"input"))}function wa(e,n){var t=document.createEvent("HTMLEvents");t.initEvent(n,!0,!0),e.dispatchEvent(t)}function va(e){return!e.componentInstance||e.data&&e.data.transition?e:va(e.componentInstance._vnode)}var ka={model:ha,show:{bind:function(e,n,t){var o=n.value,a=(t=va(t)).data&&t.data.transition,r=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;o&&a?(t.data.show=!0,ia(t,(function(){e.style.display=r}))):e.style.display=o?r:"none"},update:function(e,n,t){var o=n.value;!o!=!n.oldValue&&((t=va(t)).data&&t.data.transition?(t.data.show=!0,o?ia(t,(function(){e.style.display=e.__vOriginalDisplay})):sa(t,(function(){e.style.display="none"}))):e.style.display=o?e.__vOriginalDisplay:"none")},unbind:function(e,n,t,o,a){a||(e.style.display=e.__vOriginalDisplay)}}},qa={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function xa(e){var n=e&&e.componentOptions;return n&&n.Ctor.options.abstract?xa(Yn(n.children)):e}function ja(e){var n={},t=e.$options;for(var o in t.propsData)n[o]=e[o];var a=t._parentListeners;for(var r in a)n[j(r)]=a[r];return n}function Sa(e,n){if(/\d-keep-alive$/.test(n.tag))return e("keep-alive",{props:n.componentOptions.propsData})}var Ta=function(e){return e.tag||bn(e)},Ia=function(e){return"show"===e.name},Pa={name:"transition",props:qa,abstract:!0,render:function(e){var n=this,t=this.$slots.default;if(t&&(t=t.filter(Ta)).length){0;var o=this.mode;0;var a=t[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return a;var r=xa(a);if(!r)return a;if(this._leaving)return Sa(e,a);var i="__transition-"+this._uid+"-";r.key=null==r.key?r.isComment?i+"comment":i+r.tag:l(r.key)?0===String(r.key).indexOf(i)?r.key:i+r.key:r.key;var s=(r.data||(r.data={})).transition=ja(this),u=this._vnode,d=xa(u);if(r.data.directives&&r.data.directives.some(Ia)&&(r.data.show=!0),d&&d.data&&!function(e,n){return n.key===e.key&&n.tag===e.tag}(r,d)&&!bn(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var c=d.data.transition=z({},s);if("out-in"===o)return this._leaving=!0,dn(c,"afterLeave",(function(){n._leaving=!1,n.$forceUpdate()})),Sa(e,a);if("in-out"===o){if(bn(r))return u;var h,p=function(){h()};dn(s,"afterEnter",p),dn(s,"enterCancelled",p),dn(c,"delayLeave",(function(e){h=e}))}}return a}}},Qa=z({tag:String,moveClass:String},qa);function za(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function Ca(e){e.data.newPos=e.elm.getBoundingClientRect()}function Aa(e){var n=e.data.pos,t=e.data.newPos,o=n.left-t.left,a=n.top-t.top;if(o||a){e.data.moved=!0;var r=e.elm.style;r.transform=r.WebkitTransform="translate("+o+"px,"+a+"px)",r.transitionDuration="0s"}}delete Qa.mode;var Ea={Transition:Pa,TransitionGroup:{props:Qa,beforeMount:function(){var e=this,n=this._update;this._update=function(t,o){var a=Xn(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,a(),n.call(e,t,o)}},render:function(e){for(var n=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),o=this.prevChildren=this.children,a=this.$slots.default||[],r=this.children=[],i=ja(this),s=0;s<a.length;s++){var l=a[s];if(l.tag)if(null!=l.key&&0!==String(l.key).indexOf("__vlist"))r.push(l),t[l.key]=l,(l.data||(l.data={})).transition=i;else;}if(o){for(var u=[],d=[],c=0;c<o.length;c++){var h=o[c];h.data.transition=i,h.data.pos=h.elm.getBoundingClientRect(),t[h.key]?u.push(h):d.push(h)}this.kept=e(n,null,u),this.removed=d}return e(n,null,r)},updated:function(){var e=this.prevChildren,n=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,n)&&(e.forEach(za),e.forEach(Ca),e.forEach(Aa),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var t=e.elm,o=t.style;Xo(t,n),o.transform=o.WebkitTransform=o.transitionDuration="",t.addEventListener(Vo,t._moveCb=function e(o){o&&o.target!==t||o&&!/transform$/.test(o.propertyName)||(t.removeEventListener(Vo,e),t._moveCb=null,ea(t,n))})}})))},methods:{hasMove:function(e,n){if(!Uo)return!1;if(this._hasMove)return this._hasMove;var t=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){Bo(t,e)})),Ro(t,n),t.style.display="none",this.$el.appendChild(t);var o=oa(t);return this.$el.removeChild(t),this._hasMove=o.hasTransform}}}};St.config.mustUseProp=function(e,n,t){return"value"===t&&Lt(e)&&"button"!==n||"selected"===t&&"option"===e||"checked"===t&&"input"===e||"muted"===t&&"video"===e},St.config.isReservedTag=Jt,St.config.isReservedAttr=Et,St.config.getTagNamespace=function(e){return Vt(e)?"svg":"math"===e?"math":void 0},St.config.isUnknownElement=function(e){if(!U)return!0;if(Jt(e))return!1;if(e=e.toLowerCase(),null!=$t[e])return $t[e];var n=document.createElement(e);return e.indexOf("-")>-1?$t[e]=n.constructor===window.HTMLUnknownElement||n.constructor===window.HTMLElement:$t[e]=/HTMLUnknownElement/.test(n.toString())},z(St.options.directives,ka),z(St.options.components,Ea),St.prototype.__patch__=U?ca:A,St.prototype.$mount=function(e,n){return function(e,n,t){var o;return e.$el=n,e.$options.render||(e.$options.render=be),tt(e,"beforeMount"),o=function(){e._update(e._render(),t)},new yt(e,o,A,{before:function(){e._isMounted&&!e._isDestroyed&&tt(e,"beforeUpdate")}},!0),t=!1,null==e.$vnode&&(e._isMounted=!0,tt(e,"mounted")),e}(this,e=e&&U?function(e){if("string"==typeof e){var n=document.querySelector(e);return n||document.createElement("div")}return e}(e):void 0,n)},U&&setTimeout((function(){M.devtools&&re&&re.emit("init",St)}),0);var La=St;
/*!
  * vue-router v3.5.1
  * (c) 2021 Evan You
  * @license MIT
  */function Oa(e,n){for(var t in n)e[t]=n[t];return e}var Ha=/[!'()*]/g,Da=function(e){return"%"+e.charCodeAt(0).toString(16)},_a=/%2C/g,Na=function(e){return encodeURIComponent(e).replace(Ha,Da).replace(_a,",")};function Ma(e){try{return decodeURIComponent(e)}catch(e){0}return e}var Ga=function(e){return null==e||"object"==typeof e?e:String(e)};function Ra(e){var n={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var t=e.replace(/\+/g," ").split("="),o=Ma(t.shift()),a=t.length>0?Ma(t.join("=")):null;void 0===n[o]?n[o]=a:Array.isArray(n[o])?n[o].push(a):n[o]=[n[o],a]})),n):n}function Ba(e){var n=e?Object.keys(e).map((function(n){var t=e[n];if(void 0===t)return"";if(null===t)return Na(n);if(Array.isArray(t)){var o=[];return t.forEach((function(e){void 0!==e&&(null===e?o.push(Na(n)):o.push(Na(n)+"="+Na(e)))})),o.join("&")}return Na(n)+"="+Na(t)})).filter((function(e){return e.length>0})).join("&"):null;return n?"?"+n:""}var Wa=/\/?$/;function Fa(e,n,t,o){var a=o&&o.options.stringifyQuery,r=n.query||{};try{r=Ua(r)}catch(e){}var i={name:n.name||e&&e.name,meta:e&&e.meta||{},path:n.path||"/",hash:n.hash||"",query:r,params:n.params||{},fullPath:Ja(n,a),matched:e?Va(e):[]};return t&&(i.redirectedFrom=Ja(t,a)),Object.freeze(i)}function Ua(e){if(Array.isArray(e))return e.map(Ua);if(e&&"object"==typeof e){var n={};for(var t in e)n[t]=Ua(e[t]);return n}return e}var Ya=Fa(null,{path:"/"});function Va(e){for(var n=[];e;)n.unshift(e),e=e.parent;return n}function Ja(e,n){var t=e.path,o=e.query;void 0===o&&(o={});var a=e.hash;return void 0===a&&(a=""),(t||"/")+(n||Ba)(o)+a}function $a(e,n,t){return n===Ya?e===n:!!n&&(e.path&&n.path?e.path.replace(Wa,"")===n.path.replace(Wa,"")&&(t||e.hash===n.hash&&Ka(e.query,n.query)):!(!e.name||!n.name)&&(e.name===n.name&&(t||e.hash===n.hash&&Ka(e.query,n.query)&&Ka(e.params,n.params))))}function Ka(e,n){if(void 0===e&&(e={}),void 0===n&&(n={}),!e||!n)return e===n;var t=Object.keys(e).sort(),o=Object.keys(n).sort();return t.length===o.length&&t.every((function(t,a){var r=e[t];if(o[a]!==t)return!1;var i=n[t];return null==r||null==i?r===i:"object"==typeof r&&"object"==typeof i?Ka(r,i):String(r)===String(i)}))}function Za(e){for(var n=0;n<e.matched.length;n++){var t=e.matched[n];for(var o in t.instances){var a=t.instances[o],r=t.enteredCbs[o];if(a&&r){delete t.enteredCbs[o];for(var i=0;i<r.length;i++)a._isBeingDestroyed||r[i](a)}}}}var Xa={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,n){var t=n.props,o=n.children,a=n.parent,r=n.data;r.routerView=!0;for(var i=a.$createElement,s=t.name,l=a.$route,u=a._routerViewCache||(a._routerViewCache={}),d=0,c=!1;a&&a._routerRoot!==a;){var h=a.$vnode?a.$vnode.data:{};h.routerView&&d++,h.keepAlive&&a._directInactive&&a._inactive&&(c=!0),a=a.$parent}if(r.routerViewDepth=d,c){var p=u[s],y=p&&p.component;return y?(p.configProps&&er(y,r,p.route,p.configProps),i(y,r,o)):i()}var m=l.matched[d],g=m&&m.components[s];if(!m||!g)return u[s]=null,i();u[s]={component:g},r.registerRouteInstance=function(e,n){var t=m.instances[s];(n&&t!==e||!n&&t===e)&&(m.instances[s]=n)},(r.hook||(r.hook={})).prepatch=function(e,n){m.instances[s]=n.componentInstance},r.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==m.instances[s]&&(m.instances[s]=e.componentInstance),Za(l)};var b=m.props&&m.props[s];return b&&(Oa(u[s],{route:l,configProps:b}),er(g,r,l,b)),i(g,r,o)}};function er(e,n,t,o){var a=n.props=function(e,n){switch(typeof n){case"undefined":return;case"object":return n;case"function":return n(e);case"boolean":return n?e.params:void 0;default:0}}(t,o);if(a){a=n.props=Oa({},a);var r=n.attrs=n.attrs||{};for(var i in a)e.props&&i in e.props||(r[i]=a[i],delete a[i])}}function nr(e,n,t){var o=e.charAt(0);if("/"===o)return e;if("?"===o||"#"===o)return n+e;var a=n.split("/");t&&a[a.length-1]||a.pop();for(var r=e.replace(/^\//,"").split("/"),i=0;i<r.length;i++){var s=r[i];".."===s?a.pop():"."!==s&&a.push(s)}return""!==a[0]&&a.unshift(""),a.join("/")}function tr(e){return e.replace(/\/\//g,"/")}var or=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},ar=fr,rr=dr,ir=function(e,n){return hr(dr(e,n),n)},sr=hr,lr=br,ur=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function dr(e,n){for(var t,o=[],a=0,r=0,i="",s=n&&n.delimiter||"/";null!=(t=ur.exec(e));){var l=t[0],u=t[1],d=t.index;if(i+=e.slice(r,d),r=d+l.length,u)i+=u[1];else{var c=e[r],h=t[2],p=t[3],y=t[4],m=t[5],g=t[6],b=t[7];i&&(o.push(i),i="");var f=null!=h&&null!=c&&c!==h,w="+"===g||"*"===g,v="?"===g||"*"===g,k=t[2]||s,q=y||m;o.push({name:p||a++,prefix:h||"",delimiter:k,optional:v,repeat:w,partial:f,asterisk:!!b,pattern:q?yr(q):b?".*":"[^"+pr(k)+"]+?"})}}return r<e.length&&(i+=e.substr(r)),i&&o.push(i),o}function cr(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function hr(e,n){for(var t=new Array(e.length),o=0;o<e.length;o++)"object"==typeof e[o]&&(t[o]=new RegExp("^(?:"+e[o].pattern+")$",gr(n)));return function(n,o){for(var a="",r=n||{},i=(o||{}).pretty?cr:encodeURIComponent,s=0;s<e.length;s++){var l=e[s];if("string"!=typeof l){var u,d=r[l.name];if(null==d){if(l.optional){l.partial&&(a+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(or(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var c=0;c<d.length;c++){if(u=i(d[c]),!t[s].test(u))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(u)+"`");a+=(0===c?l.prefix:l.delimiter)+u}}else{if(u=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):i(d),!t[s].test(u))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+u+'"');a+=l.prefix+u}}else a+=l}return a}}function pr(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function yr(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function mr(e,n){return e.keys=n,e}function gr(e){return e&&e.sensitive?"":"i"}function br(e,n,t){or(n)||(t=n||t,n=[]);for(var o=(t=t||{}).strict,a=!1!==t.end,r="",i=0;i<e.length;i++){var s=e[i];if("string"==typeof s)r+=pr(s);else{var l=pr(s.prefix),u="(?:"+s.pattern+")";n.push(s),s.repeat&&(u+="(?:"+l+u+")*"),r+=u=s.optional?s.partial?l+"("+u+")?":"(?:"+l+"("+u+"))?":l+"("+u+")"}}var d=pr(t.delimiter||"/"),c=r.slice(-d.length)===d;return o||(r=(c?r.slice(0,-d.length):r)+"(?:"+d+"(?=$))?"),r+=a?"$":o&&c?"":"(?="+d+"|$)",mr(new RegExp("^"+r,gr(t)),n)}function fr(e,n,t){return or(n)||(t=n||t,n=[]),t=t||{},e instanceof RegExp?function(e,n){var t=e.source.match(/\((?!\?)/g);if(t)for(var o=0;o<t.length;o++)n.push({name:o,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return mr(e,n)}(e,n):or(e)?function(e,n,t){for(var o=[],a=0;a<e.length;a++)o.push(fr(e[a],n,t).source);return mr(new RegExp("(?:"+o.join("|")+")",gr(t)),n)}(e,n,t):function(e,n,t){return br(dr(e,t),n,t)}(e,n,t)}ar.parse=rr,ar.compile=ir,ar.tokensToFunction=sr,ar.tokensToRegExp=lr;var wr=Object.create(null);function vr(e,n,t){n=n||{};try{var o=wr[e]||(wr[e]=ar.compile(e));return"string"==typeof n.pathMatch&&(n[0]=n.pathMatch),o(n,{pretty:!0})}catch(e){return""}finally{delete n[0]}}function kr(e,n,t,o){var a="string"==typeof e?{path:e}:e;if(a._normalized)return a;if(a.name){var r=(a=Oa({},e)).params;return r&&"object"==typeof r&&(a.params=Oa({},r)),a}if(!a.path&&a.params&&n){(a=Oa({},a))._normalized=!0;var i=Oa(Oa({},n.params),a.params);if(n.name)a.name=n.name,a.params=i;else if(n.matched.length){var s=n.matched[n.matched.length-1].path;a.path=vr(s,i,n.path)}else 0;return a}var l=function(e){var n="",t="",o=e.indexOf("#");o>=0&&(n=e.slice(o),e=e.slice(0,o));var a=e.indexOf("?");return a>=0&&(t=e.slice(a+1),e=e.slice(0,a)),{path:e,query:t,hash:n}}(a.path||""),u=n&&n.path||"/",d=l.path?nr(l.path,u,t||a.append):u,c=function(e,n,t){void 0===n&&(n={});var o,a=t||Ra;try{o=a(e||"")}catch(e){o={}}for(var r in n){var i=n[r];o[r]=Array.isArray(i)?i.map(Ga):Ga(i)}return o}(l.query,a.query,o&&o.options.parseQuery),h=a.hash||l.hash;return h&&"#"!==h.charAt(0)&&(h="#"+h),{_normalized:!0,path:d,query:c,hash:h}}var qr,xr=function(){},jr={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var n=this,t=this.$router,o=this.$route,a=t.resolve(this.to,o,this.append),r=a.location,i=a.route,s=a.href,l={},u=t.options.linkActiveClass,d=t.options.linkExactActiveClass,c=null==u?"router-link-active":u,h=null==d?"router-link-exact-active":d,p=null==this.activeClass?c:this.activeClass,y=null==this.exactActiveClass?h:this.exactActiveClass,m=i.redirectedFrom?Fa(null,kr(i.redirectedFrom),null,t):i;l[y]=$a(o,m,this.exactPath),l[p]=this.exact||this.exactPath?l[y]:function(e,n){return 0===e.path.replace(Wa,"/").indexOf(n.path.replace(Wa,"/"))&&(!n.hash||e.hash===n.hash)&&function(e,n){for(var t in n)if(!(t in e))return!1;return!0}(e.query,n.query)}(o,m);var g=l[y]?this.ariaCurrentValue:null,b=function(e){Sr(e)&&(n.replace?t.replace(r,xr):t.push(r,xr))},f={click:Sr};Array.isArray(this.event)?this.event.forEach((function(e){f[e]=b})):f[this.event]=b;var w={class:l},v=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:i,navigate:b,isActive:l[p],isExactActive:l[y]});if(v){if(1===v.length)return v[0];if(v.length>1||!v.length)return 0===v.length?e():e("span",{},v)}if("a"===this.tag)w.on=f,w.attrs={href:s,"aria-current":g};else{var k=function e(n){var t;if(n)for(var o=0;o<n.length;o++){if("a"===(t=n[o]).tag)return t;if(t.children&&(t=e(t.children)))return t}}(this.$slots.default);if(k){k.isStatic=!1;var q=k.data=Oa({},k.data);for(var x in q.on=q.on||{},q.on){var j=q.on[x];x in f&&(q.on[x]=Array.isArray(j)?j:[j])}for(var S in f)S in q.on?q.on[S].push(f[S]):q.on[S]=b;var T=k.data.attrs=Oa({},k.data.attrs);T.href=s,T["aria-current"]=g}else w.on=f}return e(this.tag,w,this.$slots.default)}};function Sr(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var n=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(n))return}return e.preventDefault&&e.preventDefault(),!0}}var Tr="undefined"!=typeof window;function Ir(e,n,t,o,a){var r=n||[],i=t||Object.create(null),s=o||Object.create(null);e.forEach((function(e){!function e(n,t,o,a,r,i){var s=a.path,l=a.name;0;var u=a.pathToRegexpOptions||{},d=function(e,n,t){t||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==n)return e;return tr(n.path+"/"+e)}(s,r,u.strict);"boolean"==typeof a.caseSensitive&&(u.sensitive=a.caseSensitive);var c={path:d,regex:Pr(d,u),components:a.components||{default:a.component},alias:a.alias?"string"==typeof a.alias?[a.alias]:a.alias:[],instances:{},enteredCbs:{},name:l,parent:r,matchAs:i,redirect:a.redirect,beforeEnter:a.beforeEnter,meta:a.meta||{},props:null==a.props?{}:a.components?a.props:{default:a.props}};a.children&&a.children.forEach((function(a){var r=i?tr(i+"/"+a.path):void 0;e(n,t,o,a,c,r)}));t[c.path]||(n.push(c.path),t[c.path]=c);if(void 0!==a.alias)for(var h=Array.isArray(a.alias)?a.alias:[a.alias],p=0;p<h.length;++p){0;var y={path:h[p],children:a.children};e(n,t,o,y,r,c.path||"/")}l&&(o[l]||(o[l]=c))}(r,i,s,e,a)}));for(var l=0,u=r.length;l<u;l++)"*"===r[l]&&(r.push(r.splice(l,1)[0]),u--,l--);return{pathList:r,pathMap:i,nameMap:s}}function Pr(e,n){return ar(e,[],n)}function Qr(e,n){var t=Ir(e),o=t.pathList,a=t.pathMap,r=t.nameMap;function i(e,t,i){var s=kr(e,t,!1,n),u=s.name;if(u){var d=r[u];if(!d)return l(null,s);var c=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var h in t.params)!(h in s.params)&&c.indexOf(h)>-1&&(s.params[h]=t.params[h]);return s.path=vr(d.path,s.params),l(d,s,i)}if(s.path){s.params={};for(var p=0;p<o.length;p++){var y=o[p],m=a[y];if(zr(m.regex,s.path,s.params))return l(m,s,i)}}return l(null,s)}function s(e,t){var o=e.redirect,a="function"==typeof o?o(Fa(e,t,null,n)):o;if("string"==typeof a&&(a={path:a}),!a||"object"!=typeof a)return l(null,t);var s=a,u=s.name,d=s.path,c=t.query,h=t.hash,p=t.params;if(c=s.hasOwnProperty("query")?s.query:c,h=s.hasOwnProperty("hash")?s.hash:h,p=s.hasOwnProperty("params")?s.params:p,u){r[u];return i({_normalized:!0,name:u,query:c,hash:h,params:p},void 0,t)}if(d){var y=function(e,n){return nr(e,n.parent?n.parent.path:"/",!0)}(d,e);return i({_normalized:!0,path:vr(y,p),query:c,hash:h},void 0,t)}return l(null,t)}function l(e,t,o){return e&&e.redirect?s(e,o||t):e&&e.matchAs?function(e,n,t){var o=i({_normalized:!0,path:vr(t,n.params)});if(o){var a=o.matched,r=a[a.length-1];return n.params=o.params,l(r,n)}return l(null,n)}(0,t,e.matchAs):Fa(e,t,o,n)}return{match:i,addRoute:function(e,n){var t="object"!=typeof e?r[e]:void 0;Ir([n||e],o,a,r,t),t&&Ir(t.alias.map((function(e){return{path:e,children:[n]}})),o,a,r,t)},getRoutes:function(){return o.map((function(e){return a[e]}))},addRoutes:function(e){Ir(e,o,a,r)}}}function zr(e,n,t){var o=n.match(e);if(!o)return!1;if(!t)return!0;for(var a=1,r=o.length;a<r;++a){var i=e.keys[a-1];i&&(t[i.name||"pathMatch"]="string"==typeof o[a]?Ma(o[a]):o[a])}return!0}var Cr=Tr&&window.performance&&window.performance.now?window.performance:Date;function Ar(){return Cr.now().toFixed(3)}var Er=Ar();function Lr(){return Er}function Or(e){return Er=e}var Hr=Object.create(null);function Dr(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,n=window.location.href.replace(e,""),t=Oa({},window.history.state);return t.key=Lr(),window.history.replaceState(t,"",n),window.addEventListener("popstate",Mr),function(){window.removeEventListener("popstate",Mr)}}function _r(e,n,t,o){if(e.app){var a=e.options.scrollBehavior;a&&e.app.$nextTick((function(){var r=function(){var e=Lr();if(e)return Hr[e]}(),i=a.call(e,n,t,o?r:null);i&&("function"==typeof i.then?i.then((function(e){Fr(e,r)})).catch((function(e){0})):Fr(i,r))}))}}function Nr(){var e=Lr();e&&(Hr[e]={x:window.pageXOffset,y:window.pageYOffset})}function Mr(e){Nr(),e.state&&e.state.key&&Or(e.state.key)}function Gr(e){return Br(e.x)||Br(e.y)}function Rr(e){return{x:Br(e.x)?e.x:window.pageXOffset,y:Br(e.y)?e.y:window.pageYOffset}}function Br(e){return"number"==typeof e}var Wr=/^#\d/;function Fr(e,n){var t,o="object"==typeof e;if(o&&"string"==typeof e.selector){var a=Wr.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(a){var r=e.offset&&"object"==typeof e.offset?e.offset:{};n=function(e,n){var t=document.documentElement.getBoundingClientRect(),o=e.getBoundingClientRect();return{x:o.left-t.left-n.x,y:o.top-t.top-n.y}}(a,r={x:Br((t=r).x)?t.x:0,y:Br(t.y)?t.y:0})}else Gr(e)&&(n=Rr(e))}else o&&Gr(e)&&(n=Rr(e));n&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:n.x,top:n.y,behavior:e.behavior}):window.scrollTo(n.x,n.y))}var Ur,Yr=Tr&&((-1===(Ur=window.navigator.userAgent).indexOf("Android 2.")&&-1===Ur.indexOf("Android 4.0")||-1===Ur.indexOf("Mobile Safari")||-1!==Ur.indexOf("Chrome")||-1!==Ur.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Vr(e,n){Nr();var t=window.history;try{if(n){var o=Oa({},t.state);o.key=Lr(),t.replaceState(o,"",e)}else t.pushState({key:Or(Ar())},"",e)}catch(t){window.location[n?"replace":"assign"](e)}}function Jr(e){Vr(e,!0)}function $r(e,n,t){var o=function(a){a>=e.length?t():e[a]?n(e[a],(function(){o(a+1)})):o(a+1)};o(0)}var Kr={redirected:2,aborted:4,cancelled:8,duplicated:16};function Zr(e,n){return ei(e,n,Kr.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var n={};return ni.forEach((function(t){t in e&&(n[t]=e[t])})),JSON.stringify(n,null,2)}(n)+'" via a navigation guard.')}function Xr(e,n){return ei(e,n,Kr.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+n.fullPath+'" with a new navigation.')}function ei(e,n,t,o){var a=new Error(o);return a._isRouter=!0,a.from=e,a.to=n,a.type=t,a}var ni=["params","query","hash"];function ti(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function oi(e,n){return ti(e)&&e._isRouter&&(null==n||e.type===n)}function ai(e){return function(n,t,o){var a=!1,r=0,i=null;ri(e,(function(e,n,t,s){if("function"==typeof e&&void 0===e.cid){a=!0,r++;var l,u=li((function(n){var a;((a=n).__esModule||si&&"Module"===a[Symbol.toStringTag])&&(n=n.default),e.resolved="function"==typeof n?n:qr.extend(n),t.components[s]=n,--r<=0&&o()})),d=li((function(e){var n="Failed to resolve async component "+s+": "+e;i||(i=ti(e)?e:new Error(n),o(i))}));try{l=e(u,d)}catch(e){d(e)}if(l)if("function"==typeof l.then)l.then(u,d);else{var c=l.component;c&&"function"==typeof c.then&&c.then(u,d)}}})),a||o()}}function ri(e,n){return ii(e.map((function(e){return Object.keys(e.components).map((function(t){return n(e.components[t],e.instances[t],e,t)}))})))}function ii(e){return Array.prototype.concat.apply([],e)}var si="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function li(e){var n=!1;return function(){for(var t=[],o=arguments.length;o--;)t[o]=arguments[o];if(!n)return n=!0,e.apply(this,t)}}var ui=function(e,n){this.router=e,this.base=function(e){if(!e)if(Tr){var n=document.querySelector("base");e=(e=n&&n.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(n),this.current=Ya,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function di(e,n,t,o){var a=ri(e,(function(e,o,a,r){var i=function(e,n){"function"!=typeof e&&(e=qr.extend(e));return e.options[n]}(e,n);if(i)return Array.isArray(i)?i.map((function(e){return t(e,o,a,r)})):t(i,o,a,r)}));return ii(o?a.reverse():a)}function ci(e,n){if(n)return function(){return e.apply(n,arguments)}}ui.prototype.listen=function(e){this.cb=e},ui.prototype.onReady=function(e,n){this.ready?e():(this.readyCbs.push(e),n&&this.readyErrorCbs.push(n))},ui.prototype.onError=function(e){this.errorCbs.push(e)},ui.prototype.transitionTo=function(e,n,t){var o,a=this;try{o=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(n){n(e)})),e}var r=this.current;this.confirmTransition(o,(function(){a.updateRoute(o),n&&n(o),a.ensureURL(),a.router.afterHooks.forEach((function(e){e&&e(o,r)})),a.ready||(a.ready=!0,a.readyCbs.forEach((function(e){e(o)})))}),(function(e){t&&t(e),e&&!a.ready&&(oi(e,Kr.redirected)&&r===Ya||(a.ready=!0,a.readyErrorCbs.forEach((function(n){n(e)}))))}))},ui.prototype.confirmTransition=function(e,n,t){var o=this,a=this.current;this.pending=e;var r,i,s=function(e){!oi(e)&&ti(e)&&(o.errorCbs.length?o.errorCbs.forEach((function(n){n(e)})):console.error(e)),t&&t(e)},l=e.matched.length-1,u=a.matched.length-1;if($a(e,a)&&l===u&&e.matched[l]===a.matched[u])return this.ensureURL(),s(((i=ei(r=a,e,Kr.duplicated,'Avoided redundant navigation to current location: "'+r.fullPath+'".')).name="NavigationDuplicated",i));var d=function(e,n){var t,o=Math.max(e.length,n.length);for(t=0;t<o&&e[t]===n[t];t++);return{updated:n.slice(0,t),activated:n.slice(t),deactivated:e.slice(t)}}(this.current.matched,e.matched),c=d.updated,h=d.deactivated,p=d.activated,y=[].concat(function(e){return di(e,"beforeRouteLeave",ci,!0)}(h),this.router.beforeHooks,function(e){return di(e,"beforeRouteUpdate",ci)}(c),p.map((function(e){return e.beforeEnter})),ai(p)),m=function(n,t){if(o.pending!==e)return s(Xr(a,e));try{n(e,a,(function(n){!1===n?(o.ensureURL(!0),s(function(e,n){return ei(e,n,Kr.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+n.fullPath+'" via a navigation guard.')}(a,e))):ti(n)?(o.ensureURL(!0),s(n)):"string"==typeof n||"object"==typeof n&&("string"==typeof n.path||"string"==typeof n.name)?(s(Zr(a,e)),"object"==typeof n&&n.replace?o.replace(n):o.push(n)):t(n)}))}catch(e){s(e)}};$r(y,m,(function(){$r(function(e){return di(e,"beforeRouteEnter",(function(e,n,t,o){return function(e,n,t){return function(o,a,r){return e(o,a,(function(e){"function"==typeof e&&(n.enteredCbs[t]||(n.enteredCbs[t]=[]),n.enteredCbs[t].push(e)),r(e)}))}}(e,t,o)}))}(p).concat(o.router.resolveHooks),m,(function(){if(o.pending!==e)return s(Xr(a,e));o.pending=null,n(e),o.router.app&&o.router.app.$nextTick((function(){Za(e)}))}))}))},ui.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},ui.prototype.setupListeners=function(){},ui.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=Ya,this.pending=null};var hi=function(e){function n(n,t){e.call(this,n,t),this._startLocation=pi(this.base)}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router,t=n.options.scrollBehavior,o=Yr&&t;o&&this.listeners.push(Dr());var a=function(){var t=e.current,a=pi(e.base);e.current===Ya&&a===e._startLocation||e.transitionTo(a,(function(e){o&&_r(n,e,t,!0)}))};window.addEventListener("popstate",a),this.listeners.push((function(){window.removeEventListener("popstate",a)}))}},n.prototype.go=function(e){window.history.go(e)},n.prototype.push=function(e,n,t){var o=this,a=this.current;this.transitionTo(e,(function(e){Vr(tr(o.base+e.fullPath)),_r(o.router,e,a,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var o=this,a=this.current;this.transitionTo(e,(function(e){Jr(tr(o.base+e.fullPath)),_r(o.router,e,a,!1),n&&n(e)}),t)},n.prototype.ensureURL=function(e){if(pi(this.base)!==this.current.fullPath){var n=tr(this.base+this.current.fullPath);e?Vr(n):Jr(n)}},n.prototype.getCurrentLocation=function(){return pi(this.base)},n}(ui);function pi(e){var n=window.location.pathname;return e&&0===n.toLowerCase().indexOf(e.toLowerCase())&&(n=n.slice(e.length)),(n||"/")+window.location.search+window.location.hash}var yi=function(e){function n(n,t,o){e.call(this,n,t),o&&function(e){var n=pi(e);if(!/^\/#/.test(n))return window.location.replace(tr(e+"/#"+n)),!0}(this.base)||mi()}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router.options.scrollBehavior,t=Yr&&n;t&&this.listeners.push(Dr());var o=function(){var n=e.current;mi()&&e.transitionTo(gi(),(function(o){t&&_r(e.router,o,n,!0),Yr||wi(o.fullPath)}))},a=Yr?"popstate":"hashchange";window.addEventListener(a,o),this.listeners.push((function(){window.removeEventListener(a,o)}))}},n.prototype.push=function(e,n,t){var o=this,a=this.current;this.transitionTo(e,(function(e){fi(e.fullPath),_r(o.router,e,a,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var o=this,a=this.current;this.transitionTo(e,(function(e){wi(e.fullPath),_r(o.router,e,a,!1),n&&n(e)}),t)},n.prototype.go=function(e){window.history.go(e)},n.prototype.ensureURL=function(e){var n=this.current.fullPath;gi()!==n&&(e?fi(n):wi(n))},n.prototype.getCurrentLocation=function(){return gi()},n}(ui);function mi(){var e=gi();return"/"===e.charAt(0)||(wi("/"+e),!1)}function gi(){var e=window.location.href,n=e.indexOf("#");return n<0?"":e=e.slice(n+1)}function bi(e){var n=window.location.href,t=n.indexOf("#");return(t>=0?n.slice(0,t):n)+"#"+e}function fi(e){Yr?Vr(bi(e)):window.location.hash=e}function wi(e){Yr?Jr(bi(e)):window.location.replace(bi(e))}var vi=function(e){function n(n,t){e.call(this,n,t),this.stack=[],this.index=-1}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.push=function(e,n,t){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index+1).concat(e),o.index++,n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index).concat(e),n&&n(e)}),t)},n.prototype.go=function(e){var n=this,t=this.index+e;if(!(t<0||t>=this.stack.length)){var o=this.stack[t];this.confirmTransition(o,(function(){var e=n.current;n.index=t,n.updateRoute(o),n.router.afterHooks.forEach((function(n){n&&n(o,e)}))}),(function(e){oi(e,Kr.duplicated)&&(n.index=t)}))}},n.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},n.prototype.ensureURL=function(){},n}(ui),ki=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Qr(e.routes||[],this);var n=e.mode||"hash";switch(this.fallback="history"===n&&!Yr&&!1!==e.fallback,this.fallback&&(n="hash"),Tr||(n="abstract"),this.mode=n,n){case"history":this.history=new hi(this,e.base);break;case"hash":this.history=new yi(this,e.base,this.fallback);break;case"abstract":this.history=new vi(this,e.base);break;default:0}},qi={currentRoute:{configurable:!0}};function xi(e,n){return e.push(n),function(){var t=e.indexOf(n);t>-1&&e.splice(t,1)}}ki.prototype.match=function(e,n,t){return this.matcher.match(e,n,t)},qi.currentRoute.get=function(){return this.history&&this.history.current},ki.prototype.init=function(e){var n=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var t=n.apps.indexOf(e);t>-1&&n.apps.splice(t,1),n.app===e&&(n.app=n.apps[0]||null),n.app||n.history.teardown()})),!this.app){this.app=e;var t=this.history;if(t instanceof hi||t instanceof yi){var o=function(e){t.setupListeners(),function(e){var o=t.current,a=n.options.scrollBehavior;Yr&&a&&"fullPath"in e&&_r(n,e,o,!1)}(e)};t.transitionTo(t.getCurrentLocation(),o,o)}t.listen((function(e){n.apps.forEach((function(n){n._route=e}))}))}},ki.prototype.beforeEach=function(e){return xi(this.beforeHooks,e)},ki.prototype.beforeResolve=function(e){return xi(this.resolveHooks,e)},ki.prototype.afterEach=function(e){return xi(this.afterHooks,e)},ki.prototype.onReady=function(e,n){this.history.onReady(e,n)},ki.prototype.onError=function(e){this.history.onError(e)},ki.prototype.push=function(e,n,t){var o=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){o.history.push(e,n,t)}));this.history.push(e,n,t)},ki.prototype.replace=function(e,n,t){var o=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){o.history.replace(e,n,t)}));this.history.replace(e,n,t)},ki.prototype.go=function(e){this.history.go(e)},ki.prototype.back=function(){this.go(-1)},ki.prototype.forward=function(){this.go(1)},ki.prototype.getMatchedComponents=function(e){var n=e?e.matched?e:this.resolve(e).route:this.currentRoute;return n?[].concat.apply([],n.matched.map((function(e){return Object.keys(e.components).map((function(n){return e.components[n]}))}))):[]},ki.prototype.resolve=function(e,n,t){var o=kr(e,n=n||this.history.current,t,this),a=this.match(o,n),r=a.redirectedFrom||a.fullPath;return{location:o,route:a,href:function(e,n,t){var o="hash"===t?"#"+n:n;return e?tr(e+"/"+o):o}(this.history.base,r,this.mode),normalizedTo:o,resolved:a}},ki.prototype.getRoutes=function(){return this.matcher.getRoutes()},ki.prototype.addRoute=function(e,n){this.matcher.addRoute(e,n),this.history.current!==Ya&&this.history.transitionTo(this.history.getCurrentLocation())},ki.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==Ya&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(ki.prototype,qi),ki.install=function e(n){if(!e.installed||qr!==n){e.installed=!0,qr=n;var t=function(e){return void 0!==e},o=function(e,n){var o=e.$options._parentVnode;t(o)&&t(o=o.data)&&t(o=o.registerRouteInstance)&&o(e,n)};n.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),n.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,o(this,this)},destroyed:function(){o(this)}}),Object.defineProperty(n.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(n.prototype,"$route",{get:function(){return this._routerRoot._route}}),n.component("RouterView",Xa),n.component("RouterLink",jr);var a=n.config.optionMergeStrategies;a.beforeRouteEnter=a.beforeRouteLeave=a.beforeRouteUpdate=a.created}},ki.version="3.5.1",ki.isNavigationFailure=oi,ki.NavigationFailureType=Kr,ki.START_LOCATION=Ya,Tr&&window.Vue&&window.Vue.use(ki);var ji=ki;t(182),t(130),t(201),t(96),t(203),t(97),t(98),t(204);function Si(e){e.locales&&Object.keys(e.locales).forEach((function(n){e.locales[n].path=n})),Object.freeze(e)}t(34),t(40),t(59);var Ti=t(42),Ii=(t(138),t(45),t(68),t(184),t(185),{NotFound:function(){return t.e(6).then(t.bind(null,378))},Layout:function(){return Promise.all([t.e(0),t.e(2)]).then(t.bind(null,377))}}),Pi={"v-47639a6e":function(){return t.e(7).then(t.bind(null,381))},"v-86b1f726":function(){return t.e(8).then(t.bind(null,382))},"v-94114ee6":function(){return t.e(9).then(t.bind(null,383))},"v-5d960c2d":function(){return t.e(10).then(t.bind(null,384))},"v-03d2d023":function(){return t.e(11).then(t.bind(null,385))},"v-4806233e":function(){return t.e(12).then(t.bind(null,386))},"v-64c45226":function(){return t.e(13).then(t.bind(null,387))},"v-19f3891b":function(){return t.e(14).then(t.bind(null,388))},"v-4f881571":function(){return t.e(15).then(t.bind(null,389))},"v-3d649b57":function(){return t.e(17).then(t.bind(null,390))},"v-f95dcc66":function(){return t.e(16).then(t.bind(null,391))},"v-46088ee7":function(){return t.e(18).then(t.bind(null,392))},"v-4f6d6333":function(){return t.e(19).then(t.bind(null,393))},"v-25b5132d":function(){return t.e(20).then(t.bind(null,394))},"v-220de14d":function(){return t.e(21).then(t.bind(null,395))},"v-8fd36766":function(){return t.e(22).then(t.bind(null,396))},"v-576a0161":function(){return t.e(23).then(t.bind(null,397))},"v-45bb1917":function(){return t.e(24).then(t.bind(null,398))},"v-04677efd":function(){return t.e(25).then(t.bind(null,399))},"v-03e13271":function(){return t.e(26).then(t.bind(null,400))},"v-306d6933":function(){return t.e(27).then(t.bind(null,401))},"v-754465ed":function(){return t.e(28).then(t.bind(null,402))},"v-756db2b5":function(){return t.e(29).then(t.bind(null,403))},"v-e30ce652":function(){return t.e(30).then(t.bind(null,404))},"v-bc8b07b2":function(){return t.e(32).then(t.bind(null,405))},"v-76f709af":function(){return t.e(31).then(t.bind(null,406))},"v-5c0f832f":function(){return t.e(33).then(t.bind(null,407))},"v-89f298e6":function(){return t.e(34).then(t.bind(null,408))},"v-65f40cc9":function(){return t.e(35).then(t.bind(null,409))},"v-4162124d":function(){return t.e(36).then(t.bind(null,410))},"v-1397d61e":function(){return t.e(37).then(t.bind(null,411))},"v-514b550d":function(){return t.e(38).then(t.bind(null,412))},"v-38986917":function(){return t.e(40).then(t.bind(null,413))},"v-7c3a4841":function(){return t.e(39).then(t.bind(null,414))},"v-06fe97ed":function(){return t.e(41).then(t.bind(null,415))},"v-56f85486":function(){return t.e(42).then(t.bind(null,416))},"v-2f18e28d":function(){return t.e(43).then(t.bind(null,417))},"v-76f64b99":function(){return t.e(44).then(t.bind(null,418))},"v-2e6d4e4d":function(){return t.e(45).then(t.bind(null,419))},"v-29564a26":function(){return t.e(46).then(t.bind(null,420))},"v-fd457426":function(){return t.e(47).then(t.bind(null,421))},"v-407a5507":function(){return t.e(48).then(t.bind(null,422))},"v-2ecb6cbd":function(){return t.e(49).then(t.bind(null,423))},"v-25105aba":function(){return t.e(50).then(t.bind(null,424))},"v-2d2f1159":function(){return t.e(52).then(t.bind(null,425))},"v-0b973857":function(){return t.e(51).then(t.bind(null,426))},"v-ee61d266":function(){return t.e(53).then(t.bind(null,427))},"v-2f06345b":function(){return t.e(54).then(t.bind(null,428))},"v-19c469bd":function(){return t.e(55).then(t.bind(null,429))},"v-6ba4f6d5":function(){return t.e(56).then(t.bind(null,430))},"v-ab158e66":function(){return t.e(57).then(t.bind(null,431))},"v-58d12b55":function(){return t.e(58).then(t.bind(null,432))},"v-76eb85ed":function(){return t.e(59).then(t.bind(null,433))},"v-1f8c8e6f":function(){return t.e(60).then(t.bind(null,434))},"v-5bbe0a6d":function(){return t.e(61).then(t.bind(null,435))},"v-1265970d":function(){return t.e(62).then(t.bind(null,436))},"v-560420fe":function(){return t.e(63).then(t.bind(null,437))},"v-56f909a6":function(){return t.e(64).then(t.bind(null,438))},"v-6a073232":function(){return t.e(65).then(t.bind(null,439))},"v-7e31bc3d":function(){return t.e(66).then(t.bind(null,440))},"v-eb9283e6":function(){return t.e(67).then(t.bind(null,441))},"v-47ab00ba":function(){return t.e(68).then(t.bind(null,442))},"v-74b235b3":function(){return t.e(69).then(t.bind(null,443))},"v-42d0237f":function(){return t.e(70).then(t.bind(null,444))},"v-73371d26":function(){return t.e(71).then(t.bind(null,445))},"v-932646e6":function(){return t.e(72).then(t.bind(null,446))},"v-4c75478d":function(){return t.e(73).then(t.bind(null,447))},"v-f3d8afa6":function(){return t.e(74).then(t.bind(null,448))},"v-7464bfe3":function(){return t.e(75).then(t.bind(null,449))},"v-331125c9":function(){return t.e(76).then(t.bind(null,450))},"v-7981e486":function(){return t.e(77).then(t.bind(null,451))},"v-5e3758ff":function(){return t.e(78).then(t.bind(null,452))},"v-eabc40a6":function(){return t.e(79).then(t.bind(null,453))},"v-79dd94fe":function(){return t.e(80).then(t.bind(null,454))},"v-26e5d623":function(){return t.e(81).then(t.bind(null,455))},"v-3977c37b":function(){return t.e(82).then(t.bind(null,456))},"v-5ee434f3":function(){return t.e(83).then(t.bind(null,457))},"v-ec4d1a0a":function(){return t.e(84).then(t.bind(null,458))},"v-7c9e504d":function(){return t.e(85).then(t.bind(null,459))},"v-98d0e0d6":function(){return t.e(86).then(t.bind(null,460))},"v-0765aae6":function(){return t.e(87).then(t.bind(null,461))},"v-2680104d":function(){return t.e(88).then(t.bind(null,462))},"v-188a7204":function(){return t.e(89).then(t.bind(null,463))},"v-1466252d":function(){return t.e(90).then(t.bind(null,464))},"v-73c98272":function(){return t.e(91).then(t.bind(null,465))},"v-1553fc1d":function(){return t.e(92).then(t.bind(null,466))},"v-6bcd2fe6":function(){return t.e(93).then(t.bind(null,467))},"v-784c5b83":function(){return t.e(94).then(t.bind(null,468))},"v-0bd47593":function(){return t.e(95).then(t.bind(null,469))},"v-a8374142":function(){return t.e(96).then(t.bind(null,470))},"v-36e11e8d":function(){return t.e(98).then(t.bind(null,471))},"v-2a342b6d":function(){return t.e(97).then(t.bind(null,472))},"v-662cece6":function(){return t.e(99).then(t.bind(null,473))},"v-1d35e80d":function(){return t.e(100).then(t.bind(null,474))},"v-0b86ffc3":function(){return t.e(101).then(t.bind(null,475))},"v-6b9934ae":function(){return t.e(102).then(t.bind(null,476))},"v-a188fcc6":function(){return t.e(103).then(t.bind(null,477))},"v-296dfe42":function(){return t.e(104).then(t.bind(null,478))},"v-f39834a6":function(){return t.e(105).then(t.bind(null,479))},"v-0ca4553e":function(){return t.e(106).then(t.bind(null,480))},"v-475b7a03":function(){return t.e(107).then(t.bind(null,481))},"v-2fc0fb5b":function(){return t.e(108).then(t.bind(null,482))},"v-ec5d765a":function(){return t.e(109).then(t.bind(null,483))},"v-16eb1adb":function(){return t.e(110).then(t.bind(null,484))},"v-0f11ce4d":function(){return t.e(111).then(t.bind(null,485))},"v-2b97a116":function(){return t.e(112).then(t.bind(null,486))},"v-444c16e6":function(){return t.e(113).then(t.bind(null,487))},"v-f2a4a7f6":function(){return t.e(117).then(t.bind(null,488))},"v-027260af":function(){return t.e(116).then(t.bind(null,489))},"v-8bf930e6":function(){return t.e(118).then(t.bind(null,490))},"v-7d2e257b":function(){return t.e(120).then(t.bind(null,491))},"v-6d65c06b":function(){return t.e(119).then(t.bind(null,492))},"v-0a98e772":function(){return t.e(121).then(t.bind(null,493))},"v-95e01226":function(){return t.e(122).then(t.bind(null,494))},"v-1738500d":function(){return t.e(123).then(t.bind(null,495))},"v-a57e89e6":function(){return t.e(124).then(t.bind(null,496))},"v-e2e0d016":function(){return t.e(125).then(t.bind(null,497))},"v-7ce0afab":function(){return t.e(126).then(t.bind(null,498))},"v-3b8d1591":function(){return t.e(127).then(t.bind(null,499))},"v-bf8e4ef6":function(){return t.e(128).then(t.bind(null,500))},"v-02e16a2d":function(){return t.e(130).then(t.bind(null,501))},"v-d5d38272":function(){return t.e(129).then(t.bind(null,502))},"v-bab9656e":function(){return t.e(131).then(t.bind(null,503))},"v-c098962a":function(){return t.e(132).then(t.bind(null,504))},"v-2f10357a":function(){return t.e(133).then(t.bind(null,505))},"v-6a0310bb":function(){return t.e(134).then(t.bind(null,506))},"v-7e8f4e7a":function(){return t.e(135).then(t.bind(null,507))},"v-062f2666":function(){return t.e(136).then(t.bind(null,508))},"v-d9acb146":function(){return t.e(137).then(t.bind(null,509))},"v-14d51344":function(){return t.e(139).then(t.bind(null,510))},"v-07038c0d":function(){return t.e(138).then(t.bind(null,511))},"v-f004ba66":function(){return t.e(140).then(t.bind(null,512))},"v-64b91fd5":function(){return t.e(141).then(t.bind(null,513))},"v-6fbdffab":function(){return t.e(142).then(t.bind(null,514))},"v-3db0e5ad":function(){return t.e(143).then(t.bind(null,515))},"v-0484fad1":function(){return t.e(144).then(t.bind(null,516))},"v-663e7921":function(){return t.e(145).then(t.bind(null,517))},"v-223c74ad":function(){return t.e(146).then(t.bind(null,518))},"v-846f9be6":function(){return t.e(147).then(t.bind(null,519))},"v-7687b4ad":function(){return t.e(149).then(t.bind(null,520))},"v-3f016ca6":function(){return t.e(148).then(t.bind(null,521))},"v-779feb9b":function(){return t.e(150).then(t.bind(null,522))},"v-65f10351":function(){return t.e(151).then(t.bind(null,523))},"v-249d6937":function(){return t.e(152).then(t.bind(null,524))},"v-b022432a":function(){return t.e(153).then(t.bind(null,525))},"v-dc503226":function(){return t.e(154).then(t.bind(null,526))},"v-166c1b0d":function(){return t.e(155).then(t.bind(null,527))},"v-5c3bceef":function(){return t.e(156).then(t.bind(null,528))},"v-2afe91d1":function(){return t.e(157).then(t.bind(null,529))},"v-45b45b2e":function(){return t.e(158).then(t.bind(null,530))},"v-72bdcd61":function(){return t.e(159).then(t.bind(null,531))},"v-850bfe2e":function(){return t.e(160).then(t.bind(null,532))},"v-8e6581a6":function(){return t.e(161).then(t.bind(null,533))},"v-4cc22903":function(){return t.e(162).then(t.bind(null,534))},"v-215f842d":function(){return t.e(163).then(t.bind(null,535))},"v-ae40a866":function(){return t.e(164).then(t.bind(null,536))},"v-a45391be":function(){return t.e(165).then(t.bind(null,537))},"v-6d299e43":function(){return t.e(166).then(t.bind(null,538))},"v-034044f3":function(){return t.e(167).then(t.bind(null,539))},"v-b6710da6":function(){return t.e(168).then(t.bind(null,540))},"v-529f2d66":function(){return t.e(169).then(t.bind(null,541))},"v-5be2726d":function(){return t.e(170).then(t.bind(null,542))},"v-31c257a6":function(){return t.e(171).then(t.bind(null,543))},"v-ecedcbe6":function(){return t.e(172).then(t.bind(null,544))},"v-562bf2a5":function(){return t.e(173).then(t.bind(null,545))},"v-bc9cfe26":function(){return t.e(174).then(t.bind(null,546))},"v-867568e6":function(){return t.e(175).then(t.bind(null,547))},"v-3bb71ffe":function(){return t.e(177).then(t.bind(null,548))},"v-d4874966":function(){return t.e(176).then(t.bind(null,549))},"v-1b3677a6":function(){return t.e(178).then(t.bind(null,550))},"v-1e91fed2":function(){return t.e(179).then(t.bind(null,551))},"v-6a2b59ed":function(){return t.e(180).then(t.bind(null,552))},"v-afcff1e6":function(){return t.e(181).then(t.bind(null,553))},"v-249ce353":function(){return t.e(182).then(t.bind(null,554))},"v-60abd363":function(){return t.e(183).then(t.bind(null,555))},"v-4a7311a2":function(){return t.e(184).then(t.bind(null,556))},"v-7ce7626d":function(){return t.e(185).then(t.bind(null,557))},"v-0258bce6":function(){return t.e(186).then(t.bind(null,558))},"v-d647e6e6":function(){return t.e(187).then(t.bind(null,559))},"v-720d45dd":function(){return t.e(188).then(t.bind(null,560))},"v-605e5d93":function(){return t.e(189).then(t.bind(null,561))},"v-1f0ac379":function(){return t.e(190).then(t.bind(null,562))},"v-0ffdfaaf":function(){return t.e(192).then(t.bind(null,563))},"v-50fd6d6d":function(){return t.e(191).then(t.bind(null,564))},"v-38af4ead":function(){return t.e(193).then(t.bind(null,565))},"v-953a799e":function(){return t.e(194).then(t.bind(null,566))},"v-0133a45a":function(){return t.e(195).then(t.bind(null,567))},"v-05c24d2b":function(){return t.e(196).then(t.bind(null,568))},"v-0df44aba":function(){return t.e(197).then(t.bind(null,569))},"v-3ba014ab":function(){return t.e(198).then(t.bind(null,570))},"v-11f6eb4d":function(){return t.e(199).then(t.bind(null,571))},"v-b42dc576":function(){return t.e(200).then(t.bind(null,572))},"v-48f9c38d":function(){return t.e(201).then(t.bind(null,573))},"v-09dfd14d":function(){return t.e(202).then(t.bind(null,574))},"v-07cf064d":function(){return t.e(203).then(t.bind(null,575))},"v-462cc17b":function(){return t.e(204).then(t.bind(null,576))},"v-05bf836b":function(){return t.e(206).then(t.bind(null,577))},"v-65affcad":function(){return t.e(205).then(t.bind(null,578))},"v-5dda9d1e":function(){return t.e(207).then(t.bind(null,579))},"v-6dcc960d":function(){return t.e(208).then(t.bind(null,580))},"v-67670071":function(){return t.e(209).then(t.bind(null,581))},"v-237fd8ed":function(){return t.e(211).then(t.bind(null,582))},"v-3a2d0547":function(){return t.e(210).then(t.bind(null,583))},"v-6e8c87ed":function(){return t.e(212).then(t.bind(null,584))},"v-30ad7ebd":function(){return t.e(213).then(t.bind(null,585))},"v-0841246e":function(){return t.e(214).then(t.bind(null,586))},"v-daa61166":function(){return t.e(215).then(t.bind(null,587))},"v-392eb7ed":function(){return t.e(216).then(t.bind(null,588))},"v-6191ba26":function(){return t.e(217).then(t.bind(null,589))},"v-420ef137":function(){return t.e(218).then(t.bind(null,590))},"v-306008ed":function(){return t.e(219).then(t.bind(null,591))},"v-21e7225a":function(){return t.e(220).then(t.bind(null,592))},"v-ada9f2f2":function(){return t.e(221).then(t.bind(null,593))},"v-7bc99189":function(){return t.e(222).then(t.bind(null,594))},"v-01b4cdcd":function(){return t.e(223).then(t.bind(null,595))},"v-9ea17eea":function(){return t.e(224).then(t.bind(null,596))},"v-7943c1f6":function(){return t.e(226).then(t.bind(null,597))},"v-47b39cfd":function(){return t.e(227).then(t.bind(null,598))},"v-77c86fed":function(){return t.e(225).then(t.bind(null,599))},"v-b128a8f6":function(){return t.e(228).then(t.bind(null,600))},"v-2ca3ee26":function(){return t.e(229).then(t.bind(null,601))},"v-bd94cac2":function(){return t.e(230).then(t.bind(null,602))},"v-3b041b6d":function(){return t.e(231).then(t.bind(null,603))},"v-e894ac04":function(){return t.e(232).then(t.bind(null,604))},"v-9cd12ba6":function(){return t.e(233).then(t.bind(null,605))},"v-62e23092":function(){return t.e(234).then(t.bind(null,606))},"v-e7c9cde6":function(){return t.e(235).then(t.bind(null,607))},"v-674aad0d":function(){return t.e(236).then(t.bind(null,608))},"v-6989e673":function(){return t.e(237).then(t.bind(null,609))},"v-fac8dafa":function(){return t.e(238).then(t.bind(null,610))},"v-8b57cb62":function(){return t.e(239).then(t.bind(null,611))},"v-3795786d":function(){return t.e(240).then(t.bind(null,612))},"v-fbfd20e6":function(){return t.e(241).then(t.bind(null,613))},"v-1809da8d":function(){return t.e(242).then(t.bind(null,614))},"v-045024ad":function(){return t.e(115).then(t.bind(null,615))},"v-2a03c37e":function(){return t.e(114).then(t.bind(null,616))},"v-40fa8299":function(){return t.e(245).then(t.bind(null,617))},"v-7636cee6":function(){return t.e(246).then(t.bind(null,618))},"v-290cdc62":function(){return t.e(247).then(t.bind(null,619))},"v-19ad78ad":function(){return t.e(248).then(t.bind(null,620))},"v-33ab2251":function(){return t.e(249).then(t.bind(null,621))},"v-7c7a576a":function(){return t.e(251).then(t.bind(null,622))},"v-3ac4ae1a":function(){return t.e(250).then(t.bind(null,623))},"v-171babcb":function(){return t.e(253).then(t.bind(null,624))},"v-258149c3":function(){return t.e(252).then(t.bind(null,625))},"v-24317c65":function(){return t.e(255).then(t.bind(null,626))},"v-131f5d4d":function(){return t.e(254).then(t.bind(null,627))},"v-73d3498d":function(){return t.e(256).then(t.bind(null,628))},"v-8fbc73c4":function(){return t.e(257).then(t.bind(null,629))},"v-4cb5e50d":function(){return t.e(258).then(t.bind(null,630))},"v-8de7f97e":function(){return t.e(259).then(t.bind(null,631))},"v-541b37d2":function(){return t.e(260).then(t.bind(null,632))},"v-026927ed":function(){return t.e(261).then(t.bind(null,633))},"v-39ce30bd":function(){return t.e(262).then(t.bind(null,634))},"v-671a44e6":function(){return t.e(263).then(t.bind(null,635))},"v-3bc0b2ce":function(){return t.e(264).then(t.bind(null,636))},"v-723b4366":function(){return t.e(265).then(t.bind(null,637))},"v-0501aa26":function(){return t.e(266).then(t.bind(null,638))},"v-d8f0d426":function(){return t.e(267).then(t.bind(null,639))},"v-44575ff2":function(){return t.e(268).then(t.bind(null,640))},"v-67b53086":function(){return t.e(269).then(t.bind(null,641))},"v-ea5c64ba":function(){return t.e(270).then(t.bind(null,642))},"v-29ed5952":function(){return t.e(271).then(t.bind(null,643))},"v-37f14c59":function(){return t.e(272).then(t.bind(null,644))},"v-50bff266":function(){return t.e(273).then(t.bind(null,645))},"v-9101a14a":function(){return t.e(274).then(t.bind(null,646))},"v-2c2604bd":function(){return t.e(275).then(t.bind(null,647))},"v-31d49c56":function(){return t.e(276).then(t.bind(null,648))},"v-5f7fb3cd":function(){return t.e(277).then(t.bind(null,649))},"v-63936655":function(){return t.e(278).then(t.bind(null,650))},"v-58f735ed":function(){return t.e(279).then(t.bind(null,651))},"v-aff4ed22":function(){return t.e(280).then(t.bind(null,652))},"v-cc84cb26":function(){return t.e(281).then(t.bind(null,653))},"v-fb63c69a":function(){return t.e(244).then(t.bind(null,654))},"v-d805f606":function(){return t.e(243).then(t.bind(null,655))}};function Qi(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var zi=/-(\w)/g,Ci=Qi((function(e){return e.replace(zi,(function(e,n){return n?n.toUpperCase():""}))})),Ai=/\B([A-Z])/g,Ei=Qi((function(e){return e.replace(Ai,"-$1").toLowerCase()})),Li=Qi((function(e){return e.charAt(0).toUpperCase()+e.slice(1)}));function Oi(e,n){if(n)return e(n)?e(n):n.includes("-")?e(Li(Ci(n))):e(Li(n))||e(Ei(n))}var Hi=Object.assign({},Ii,Pi),Di=function(e){return Hi[e]},_i=function(e){return Pi[e]},Ni=function(e){return Ii[e]},Mi=function(e){return La.component(e)};function Gi(e){return Oi(_i,e)}function Ri(e){return Oi(Ni,e)}function Bi(e){return Oi(Di,e)}function Wi(e){return Oi(Mi,e)}function Fi(){for(var e=arguments.length,n=new Array(e),t=0;t<e;t++)n[t]=arguments[t];return Promise.all(n.filter((function(e){return e})).map(function(){var e=Object(o.a)(regeneratorRuntime.mark((function e(n){var t;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:if(Wi(n)||!Bi(n)){e.next=5;break}return e.next=3,Bi(n)();case 3:t=e.sent,La.component(n,t.default);case 5:case"end":return e.stop()}}),e)})));return function(n){return e.apply(this,arguments)}}()))}function Ui(e,n){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=n)}var Yi=t(94),Vi=(t(175),t(176),t(163)),Ji=t.n(Vi),$i={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(e){return"meta"===Object(Yi.a)(e,1)[0]})).map((function(e){var n=Object(Yi.a)(e,2);n[0];return n[1]})),this.$ssrContext){var e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map((function(e){var n="<meta";return Object.keys(e).forEach((function(t){n+=" ".concat(t,'="').concat(e[t],'"')})),n+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=Zi(this.$canonicalUrl)}var n},mounted:function(){this.currentMetaTags=Object(Ti.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var e=this.getMergedMetaTags();this.currentMetaTags=Xi(e,this.currentMetaTags)},getMergedMetaTags:function(){var e=this.$page.frontmatter.meta||[];return Ji()([{name:"description",content:this.$description}],e,this.siteMeta,es)},updateCanonicalLink:function(){Ki(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",Zi(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){Xi(null,this.currentMetaTags),Ki()}};function Ki(){var e=document.querySelector("link[rel='canonical']");e&&e.remove()}function Zi(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return e?'<link href="'.concat(e,'" rel="canonical" />'):""}function Xi(e,n){if(n&&Object(Ti.a)(n).filter((function(e){return e.parentNode===document.head})).forEach((function(e){return document.head.removeChild(e)})),e)return e.map((function(e){var n=document.createElement("meta");return Object.keys(e).forEach((function(t){n.setAttribute(t,e[t])})),document.head.appendChild(n),n}))}function es(e){for(var n=0,t=["name","property","itemprop"];n<t.length;n++){var o=t[n];if(e.hasOwnProperty(o))return e[o]+o}return JSON.stringify(e)}t(95);var ns=t(164),ts={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(ns)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var e=this,n=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(e){return n.some((function(n){return n.hash===e.hash}))})),o=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),a=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),r=window.innerHeight+o,i=0;i<t.length;i++){var s=t[i],l=t[i+1],u=0===i&&0===o||o>=s.parentElement.offsetTop+10&&(!l||o<l.parentElement.offsetTop-10),d=decodeURIComponent(this.$route.hash);if(u&&d!==decodeURIComponent(s.hash)){var c=s;if(r===a)for(var h=i+1;h<t.length;h++)if(d===decodeURIComponent(t[h].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(c.hash),(function(){e.$nextTick((function(){e.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},os=(t(84),t(66)),as=t.n(os),rs=[$i,ts,{mounted:function(){var e=this;as.a.configure({showSpinner:!1}),this.$router.beforeEach((function(e,n,t){e.path===n.path||La.component(e.name)||as.a.start(),t()})),this.$router.afterEach((function(){as.a.done(),e.isSidebarOpen=!1}))}}],is={name:"GlobalLayout",computed:{layout:function(){var e=this.getLayout();return Ui("layout",e),La.component(e)}},methods:{getLayout:function(){if(this.$page.path){var e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},ss=t(44),ls=Object(ss.a)(is,(function(){var e=this.$createElement;return(this._self._c||e)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,n,t){var o;switch(n){case"components":e[n]||(e[n]={}),Object.assign(e[n],t);break;case"mixins":e[n]||(e[n]=[]),(o=e[n]).push.apply(o,Object(Ti.a)(t));break;default:throw new Error("Unknown option name.")}}(ls,"mixins",rs);var us=[{name:"v-47639a6e",path:"/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-47639a6e").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-86b1f726",path:"/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-86b1f726").then(t)}},{name:"v-94114ee6",path:"/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-94114ee6").then(t)}},{name:"v-5d960c2d",path:"/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5d960c2d").then(t)}},{name:"v-03d2d023",path:"/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-03d2d023").then(t)}},{name:"v-4806233e",path:"/de/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4806233e").then(t)}},{path:"/de/index.html",redirect:"/de/"},{name:"v-64c45226",path:"/de/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-64c45226").then(t)}},{name:"v-19f3891b",path:"/de/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-19f3891b").then(t)}},{name:"v-4f881571",path:"/de/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4f881571").then(t)}},{name:"v-3d649b57",path:"/de/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3d649b57").then(t)}},{name:"v-f95dcc66",path:"/de/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-f95dcc66").then(t)}},{name:"v-46088ee7",path:"/de/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-46088ee7").then(t)}},{name:"v-4f6d6333",path:"/de/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4f6d6333").then(t)}},{name:"v-25b5132d",path:"/de/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-25b5132d").then(t)}},{name:"v-220de14d",path:"/de/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-220de14d").then(t)}},{name:"v-8fd36766",path:"/de/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-8fd36766").then(t)}},{name:"v-576a0161",path:"/de/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-576a0161").then(t)}},{name:"v-45bb1917",path:"/de/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-45bb1917").then(t)}},{name:"v-04677efd",path:"/de/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-04677efd").then(t)}},{name:"v-03e13271",path:"/de/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-03e13271").then(t)}},{name:"v-306d6933",path:"/de/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-306d6933").then(t)}},{name:"v-754465ed",path:"/de/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-754465ed").then(t)}},{name:"v-756db2b5",path:"/de/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-756db2b5").then(t)}},{name:"v-e30ce652",path:"/de/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-e30ce652").then(t)}},{name:"v-bc8b07b2",path:"/de/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-bc8b07b2").then(t)}},{name:"v-76f709af",path:"/de/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-76f709af").then(t)}},{name:"v-5c0f832f",path:"/de/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5c0f832f").then(t)}},{name:"v-89f298e6",path:"/de/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-89f298e6").then(t)}},{name:"v-65f40cc9",path:"/de/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-65f40cc9").then(t)}},{name:"v-4162124d",path:"/de/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4162124d").then(t)}},{name:"v-1397d61e",path:"/es/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1397d61e").then(t)}},{path:"/es/index.html",redirect:"/es/"},{name:"v-514b550d",path:"/es/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-514b550d").then(t)}},{name:"v-38986917",path:"/es/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-38986917").then(t)}},{name:"v-7c3a4841",path:"/es/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7c3a4841").then(t)}},{name:"v-06fe97ed",path:"/es/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-06fe97ed").then(t)}},{name:"v-56f85486",path:"/es/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-56f85486").then(t)}},{name:"v-2f18e28d",path:"/es/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2f18e28d").then(t)}},{name:"v-76f64b99",path:"/es/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-76f64b99").then(t)}},{name:"v-2e6d4e4d",path:"/es/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2e6d4e4d").then(t)}},{name:"v-29564a26",path:"/es/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-29564a26").then(t)}},{name:"v-fd457426",path:"/es/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-fd457426").then(t)}},{name:"v-407a5507",path:"/es/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-407a5507").then(t)}},{name:"v-2ecb6cbd",path:"/es/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2ecb6cbd").then(t)}},{name:"v-25105aba",path:"/es/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-25105aba").then(t)}},{name:"v-2d2f1159",path:"/es/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2d2f1159").then(t)}},{name:"v-0b973857",path:"/es/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0b973857").then(t)}},{name:"v-ee61d266",path:"/es/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ee61d266").then(t)}},{name:"v-2f06345b",path:"/es/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2f06345b").then(t)}},{name:"v-19c469bd",path:"/es/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-19c469bd").then(t)}},{name:"v-6ba4f6d5",path:"/es/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6ba4f6d5").then(t)}},{name:"v-ab158e66",path:"/es/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ab158e66").then(t)}},{name:"v-58d12b55",path:"/es/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-58d12b55").then(t)}},{name:"v-76eb85ed",path:"/es/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-76eb85ed").then(t)}},{name:"v-1f8c8e6f",path:"/es/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1f8c8e6f").then(t)}},{name:"v-5bbe0a6d",path:"/es/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5bbe0a6d").then(t)}},{name:"v-1265970d",path:"/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1265970d").then(t)}},{name:"v-560420fe",path:"/id/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-560420fe").then(t)}},{path:"/id/index.html",redirect:"/id/"},{name:"v-56f909a6",path:"/id/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-56f909a6").then(t)}},{name:"v-6a073232",path:"/id/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6a073232").then(t)}},{name:"v-7e31bc3d",path:"/id/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7e31bc3d").then(t)}},{name:"v-eb9283e6",path:"/id/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-eb9283e6").then(t)}},{name:"v-47ab00ba",path:"/id/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-47ab00ba").then(t)}},{name:"v-74b235b3",path:"/id/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-74b235b3").then(t)}},{name:"v-42d0237f",path:"/id/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-42d0237f").then(t)}},{name:"v-73371d26",path:"/id/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-73371d26").then(t)}},{name:"v-932646e6",path:"/id/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-932646e6").then(t)}},{name:"v-4c75478d",path:"/id/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4c75478d").then(t)}},{name:"v-f3d8afa6",path:"/id/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-f3d8afa6").then(t)}},{name:"v-7464bfe3",path:"/id/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7464bfe3").then(t)}},{name:"v-331125c9",path:"/id/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-331125c9").then(t)}},{name:"v-7981e486",path:"/id/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7981e486").then(t)}},{name:"v-5e3758ff",path:"/id/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5e3758ff").then(t)}},{name:"v-eabc40a6",path:"/id/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-eabc40a6").then(t)}},{name:"v-79dd94fe",path:"/id/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-79dd94fe").then(t)}},{name:"v-26e5d623",path:"/id/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-26e5d623").then(t)}},{name:"v-3977c37b",path:"/id/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3977c37b").then(t)}},{name:"v-5ee434f3",path:"/id/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5ee434f3").then(t)}},{name:"v-ec4d1a0a",path:"/id/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ec4d1a0a").then(t)}},{name:"v-7c9e504d",path:"/id/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7c9e504d").then(t)}},{name:"v-98d0e0d6",path:"/id/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-98d0e0d6").then(t)}},{name:"v-0765aae6",path:"/id/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0765aae6").then(t)}},{name:"v-2680104d",path:"/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2680104d").then(t)}},{name:"v-188a7204",path:"/it/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-188a7204").then(t)}},{path:"/it/index.html",redirect:"/it/"},{name:"v-1466252d",path:"/it/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1466252d").then(t)}},{name:"v-73c98272",path:"/it/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-73c98272").then(t)}},{name:"v-1553fc1d",path:"/it/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1553fc1d").then(t)}},{name:"v-6bcd2fe6",path:"/it/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6bcd2fe6").then(t)}},{name:"v-784c5b83",path:"/it/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-784c5b83").then(t)}},{name:"v-0bd47593",path:"/it/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0bd47593").then(t)}},{name:"v-a8374142",path:"/it/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-a8374142").then(t)}},{name:"v-36e11e8d",path:"/it/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-36e11e8d").then(t)}},{name:"v-2a342b6d",path:"/it/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2a342b6d").then(t)}},{name:"v-662cece6",path:"/it/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-662cece6").then(t)}},{name:"v-1d35e80d",path:"/it/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1d35e80d").then(t)}},{name:"v-0b86ffc3",path:"/it/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0b86ffc3").then(t)}},{name:"v-6b9934ae",path:"/it/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6b9934ae").then(t)}},{name:"v-a188fcc6",path:"/it/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-a188fcc6").then(t)}},{name:"v-296dfe42",path:"/it/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-296dfe42").then(t)}},{name:"v-f39834a6",path:"/it/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-f39834a6").then(t)}},{name:"v-0ca4553e",path:"/it/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0ca4553e").then(t)}},{name:"v-475b7a03",path:"/it/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-475b7a03").then(t)}},{name:"v-2fc0fb5b",path:"/it/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2fc0fb5b").then(t)}},{name:"v-ec5d765a",path:"/it/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ec5d765a").then(t)}},{name:"v-16eb1adb",path:"/it/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-16eb1adb").then(t)}},{name:"v-0f11ce4d",path:"/it/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0f11ce4d").then(t)}},{name:"v-2b97a116",path:"/it/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2b97a116").then(t)}},{name:"v-444c16e6",path:"/it/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-444c16e6").then(t)}},{name:"v-f2a4a7f6",path:"/ja/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-f2a4a7f6").then(t)}},{name:"v-027260af",path:"/ja/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-027260af").then(t)}},{name:"v-8bf930e6",path:"/ja/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-8bf930e6").then(t)}},{name:"v-7d2e257b",path:"/ja/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7d2e257b").then(t)}},{name:"v-6d65c06b",path:"/ja/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6d65c06b").then(t)}},{name:"v-0a98e772",path:"/ja/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0a98e772").then(t)}},{name:"v-95e01226",path:"/ja/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-95e01226").then(t)}},{name:"v-1738500d",path:"/ja/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1738500d").then(t)}},{name:"v-a57e89e6",path:"/ja/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-a57e89e6").then(t)}},{name:"v-e2e0d016",path:"/ja/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-e2e0d016").then(t)}},{name:"v-7ce0afab",path:"/ja/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7ce0afab").then(t)}},{name:"v-3b8d1591",path:"/ja/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3b8d1591").then(t)}},{name:"v-bf8e4ef6",path:"/ja/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-bf8e4ef6").then(t)}},{name:"v-02e16a2d",path:"/ja/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-02e16a2d").then(t)}},{name:"v-d5d38272",path:"/ja/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-d5d38272").then(t)}},{name:"v-bab9656e",path:"/ja/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-bab9656e").then(t)}},{name:"v-c098962a",path:"/ja/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-c098962a").then(t)}},{name:"v-2f10357a",path:"/ja/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2f10357a").then(t)}},{name:"v-6a0310bb",path:"/ja/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6a0310bb").then(t)}},{name:"v-7e8f4e7a",path:"/ja/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7e8f4e7a").then(t)}},{name:"v-062f2666",path:"/ja/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-062f2666").then(t)}},{name:"v-d9acb146",path:"/ja/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-d9acb146").then(t)}},{name:"v-14d51344",path:"/ko/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-14d51344").then(t)}},{path:"/ko/index.html",redirect:"/ko/"},{name:"v-07038c0d",path:"/ja/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-07038c0d").then(t)}},{name:"v-f004ba66",path:"/ko/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-f004ba66").then(t)}},{name:"v-64b91fd5",path:"/ko/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-64b91fd5").then(t)}},{name:"v-6fbdffab",path:"/ko/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6fbdffab").then(t)}},{name:"v-3db0e5ad",path:"/ko/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3db0e5ad").then(t)}},{name:"v-0484fad1",path:"/ko/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0484fad1").then(t)}},{name:"v-663e7921",path:"/ko/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-663e7921").then(t)}},{name:"v-223c74ad",path:"/ko/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-223c74ad").then(t)}},{name:"v-846f9be6",path:"/ko/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-846f9be6").then(t)}},{name:"v-7687b4ad",path:"/ko/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7687b4ad").then(t)}},{name:"v-3f016ca6",path:"/ko/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3f016ca6").then(t)}},{name:"v-779feb9b",path:"/ko/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-779feb9b").then(t)}},{name:"v-65f10351",path:"/ko/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-65f10351").then(t)}},{name:"v-249d6937",path:"/ko/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-249d6937").then(t)}},{name:"v-b022432a",path:"/ko/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-b022432a").then(t)}},{name:"v-dc503226",path:"/ko/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-dc503226").then(t)}},{name:"v-166c1b0d",path:"/ko/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-166c1b0d").then(t)}},{name:"v-5c3bceef",path:"/ko/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5c3bceef").then(t)}},{name:"v-2afe91d1",path:"/ko/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2afe91d1").then(t)}},{name:"v-45b45b2e",path:"/ko/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-45b45b2e").then(t)}},{name:"v-72bdcd61",path:"/ko/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-72bdcd61").then(t)}},{name:"v-850bfe2e",path:"/ko/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-850bfe2e").then(t)}},{name:"v-8e6581a6",path:"/ko/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-8e6581a6").then(t)}},{name:"v-4cc22903",path:"/ko/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4cc22903").then(t)}},{name:"v-215f842d",path:"/ko/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-215f842d").then(t)}},{name:"v-ae40a866",path:"/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ae40a866").then(t)}},{name:"v-a45391be",path:"/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-a45391be").then(t)}},{name:"v-6d299e43",path:"/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6d299e43").then(t)}},{name:"v-034044f3",path:"/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-034044f3").then(t)}},{name:"v-b6710da6",path:"/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-b6710da6").then(t)}},{name:"v-529f2d66",path:"/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-529f2d66").then(t)}},{name:"v-5be2726d",path:"/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5be2726d").then(t)}},{name:"v-31c257a6",path:"/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-31c257a6").then(t)}},{name:"v-ecedcbe6",path:"/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ecedcbe6").then(t)}},{name:"v-562bf2a5",path:"/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-562bf2a5").then(t)}},{name:"v-bc9cfe26",path:"/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-bc9cfe26").then(t)}},{name:"v-867568e6",path:"/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-867568e6").then(t)}},{name:"v-3bb71ffe",path:"/ru/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3bb71ffe").then(t)}},{path:"/ru/index.html",redirect:"/ru/"},{name:"v-d4874966",path:"/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-d4874966").then(t)}},{name:"v-1b3677a6",path:"/ru/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1b3677a6").then(t)}},{name:"v-1e91fed2",path:"/ru/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1e91fed2").then(t)}},{name:"v-6a2b59ed",path:"/ru/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6a2b59ed").then(t)}},{name:"v-afcff1e6",path:"/ru/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-afcff1e6").then(t)}},{name:"v-249ce353",path:"/ru/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-249ce353").then(t)}},{name:"v-60abd363",path:"/ru/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-60abd363").then(t)}},{name:"v-4a7311a2",path:"/ru/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4a7311a2").then(t)}},{name:"v-7ce7626d",path:"/ru/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7ce7626d").then(t)}},{name:"v-0258bce6",path:"/ru/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0258bce6").then(t)}},{name:"v-d647e6e6",path:"/ru/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-d647e6e6").then(t)}},{name:"v-720d45dd",path:"/ru/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-720d45dd").then(t)}},{name:"v-605e5d93",path:"/ru/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-605e5d93").then(t)}},{name:"v-1f0ac379",path:"/ru/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1f0ac379").then(t)}},{name:"v-0ffdfaaf",path:"/ru/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0ffdfaaf").then(t)}},{name:"v-50fd6d6d",path:"/ru/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-50fd6d6d").then(t)}},{name:"v-38af4ead",path:"/ru/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-38af4ead").then(t)}},{name:"v-953a799e",path:"/ru/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-953a799e").then(t)}},{name:"v-0133a45a",path:"/ru/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0133a45a").then(t)}},{name:"v-05c24d2b",path:"/ru/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-05c24d2b").then(t)}},{name:"v-0df44aba",path:"/ru/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0df44aba").then(t)}},{name:"v-3ba014ab",path:"/ru/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3ba014ab").then(t)}},{name:"v-11f6eb4d",path:"/ru/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-11f6eb4d").then(t)}},{name:"v-b42dc576",path:"/ru/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-b42dc576").then(t)}},{name:"v-48f9c38d",path:"/ru/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-48f9c38d").then(t)}},{name:"v-09dfd14d",path:"/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-09dfd14d").then(t)}},{name:"v-07cf064d",path:"/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-07cf064d").then(t)}},{name:"v-462cc17b",path:"/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-462cc17b").then(t)}},{name:"v-05bf836b",path:"/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-05bf836b").then(t)}},{name:"v-65affcad",path:"/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-65affcad").then(t)}},{name:"v-5dda9d1e",path:"/uk/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5dda9d1e").then(t)}},{path:"/uk/index.html",redirect:"/uk/"},{name:"v-6dcc960d",path:"/uk/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6dcc960d").then(t)}},{name:"v-67670071",path:"/uk/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-67670071").then(t)}},{name:"v-237fd8ed",path:"/uk/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-237fd8ed").then(t)}},{name:"v-3a2d0547",path:"/uk/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3a2d0547").then(t)}},{name:"v-6e8c87ed",path:"/uk/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6e8c87ed").then(t)}},{name:"v-30ad7ebd",path:"/uk/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-30ad7ebd").then(t)}},{name:"v-0841246e",path:"/uk/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0841246e").then(t)}},{name:"v-daa61166",path:"/uk/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-daa61166").then(t)}},{name:"v-392eb7ed",path:"/uk/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-392eb7ed").then(t)}},{name:"v-6191ba26",path:"/uk/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6191ba26").then(t)}},{name:"v-420ef137",path:"/uk/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-420ef137").then(t)}},{name:"v-306008ed",path:"/uk/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-306008ed").then(t)}},{name:"v-21e7225a",path:"/uk/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-21e7225a").then(t)}},{name:"v-ada9f2f2",path:"/uk/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ada9f2f2").then(t)}},{name:"v-7bc99189",path:"/uk/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7bc99189").then(t)}},{name:"v-01b4cdcd",path:"/uk/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-01b4cdcd").then(t)}},{name:"v-9ea17eea",path:"/uk/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-9ea17eea").then(t)}},{name:"v-7943c1f6",path:"/uk/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7943c1f6").then(t)}},{name:"v-47b39cfd",path:"/uk/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-47b39cfd").then(t)}},{name:"v-77c86fed",path:"/uk/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-77c86fed").then(t)}},{name:"v-b128a8f6",path:"/uk/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-b128a8f6").then(t)}},{name:"v-2ca3ee26",path:"/uk/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2ca3ee26").then(t)}},{name:"v-bd94cac2",path:"/uk/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-bd94cac2").then(t)}},{name:"v-3b041b6d",path:"/uk/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3b041b6d").then(t)}},{name:"v-e894ac04",path:"/vi/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-e894ac04").then(t)}},{path:"/vi/index.html",redirect:"/vi/"},{name:"v-9cd12ba6",path:"/vi/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-9cd12ba6").then(t)}},{name:"v-62e23092",path:"/vi/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-62e23092").then(t)}},{name:"v-e7c9cde6",path:"/vi/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-e7c9cde6").then(t)}},{name:"v-674aad0d",path:"/vi/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-674aad0d").then(t)}},{name:"v-6989e673",path:"/vi/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-6989e673").then(t)}},{name:"v-fac8dafa",path:"/vi/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-fac8dafa").then(t)}},{name:"v-8b57cb62",path:"/vi/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-8b57cb62").then(t)}},{name:"v-3795786d",path:"/vi/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3795786d").then(t)}},{name:"v-fbfd20e6",path:"/vi/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-fbfd20e6").then(t)}},{name:"v-1809da8d",path:"/vi/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-1809da8d").then(t)}},{name:"v-045024ad",path:"/ja/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-045024ad").then(t)}},{name:"v-2a03c37e",path:"/ja/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2a03c37e").then(t)}},{path:"/ja/index.html",redirect:"/ja/"},{name:"v-40fa8299",path:"/vi/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-40fa8299").then(t)}},{name:"v-7636cee6",path:"/vi/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7636cee6").then(t)}},{name:"v-290cdc62",path:"/vi/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-290cdc62").then(t)}},{name:"v-19ad78ad",path:"/vi/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-19ad78ad").then(t)}},{name:"v-33ab2251",path:"/vi/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-33ab2251").then(t)}},{name:"v-7c7a576a",path:"/vi/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-7c7a576a").then(t)}},{name:"v-3ac4ae1a",path:"/vi/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3ac4ae1a").then(t)}},{name:"v-171babcb",path:"/vi/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-171babcb").then(t)}},{name:"v-258149c3",path:"/vi/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-258149c3").then(t)}},{name:"v-24317c65",path:"/vi/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-24317c65").then(t)}},{name:"v-131f5d4d",path:"/vi/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-131f5d4d").then(t)}},{name:"v-73d3498d",path:"/vi/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-73d3498d").then(t)}},{name:"v-8fbc73c4",path:"/zh/",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-8fbc73c4").then(t)}},{path:"/zh/index.html",redirect:"/zh/"},{name:"v-4cb5e50d",path:"/zh/create/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-4cb5e50d").then(t)}},{name:"v-8de7f97e",path:"/zh/create/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-8de7f97e").then(t)}},{name:"v-541b37d2",path:"/zh/create/manifest.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-541b37d2").then(t)}},{name:"v-026927ed",path:"/zh/create/mapping.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-026927ed").then(t)}},{name:"v-39ce30bd",path:"/zh/faqs/faqs.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-39ce30bd").then(t)}},{name:"v-671a44e6",path:"/zh/install/install.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-671a44e6").then(t)}},{name:"v-3bc0b2ce",path:"/zh/miscellaneous/ambassadors.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-3bc0b2ce").then(t)}},{name:"v-723b4366",path:"/zh/miscellaneous/branding.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-723b4366").then(t)}},{name:"v-0501aa26",path:"/zh/miscellaneous/contributing.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-0501aa26").then(t)}},{name:"v-d8f0d426",path:"/zh/miscellaneous/social_media.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-d8f0d426").then(t)}},{name:"v-44575ff2",path:"/zh/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-44575ff2").then(t)}},{name:"v-67b53086",path:"/zh/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-67b53086").then(t)}},{name:"v-ea5c64ba",path:"/zh/publish/upgrade.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-ea5c64ba").then(t)}},{name:"v-29ed5952",path:"/zh/query/graphql.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-29ed5952").then(t)}},{name:"v-37f14c59",path:"/zh/query/query.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-37f14c59").then(t)}},{name:"v-50bff266",path:"/zh/quickstart/helloworld-hosted.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-50bff266").then(t)}},{name:"v-9101a14a",path:"/zh/quickstart/helloworld-localhost.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-9101a14a").then(t)}},{name:"v-2c2604bd",path:"/zh/quickstart/quickstart.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-2c2604bd").then(t)}},{name:"v-31d49c56",path:"/zh/quickstart/understanding-helloworld.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-31d49c56").then(t)}},{name:"v-5f7fb3cd",path:"/zh/run/run.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-5f7fb3cd").then(t)}},{name:"v-63936655",path:"/zh/run/sandbox.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-63936655").then(t)}},{name:"v-58f735ed",path:"/zh/tutorials_examples/howto.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-58f735ed").then(t)}},{name:"v-aff4ed22",path:"/zh/tutorials_examples/introduction.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-aff4ed22").then(t)}},{name:"v-cc84cb26",path:"/zh/tutorials_examples/terminology.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-cc84cb26").then(t)}},{name:"v-fb63c69a",path:"/vi/publish/publish.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-fb63c69a").then(t)}},{name:"v-d805f606",path:"/vi/publish/connect.html",component:ls,beforeEnter:function(e,n,t){Fi("Layout","v-d805f606").then(t)}},{path:"*",component:ls}],ds={title:"",description:"",base:"/",headTags:[],pages:[{title:"Welcome to SubQuery",frontmatter:{},regularPath:"/",relativePath:"README.md",key:"v-47639a6e",path:"/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/create/graphql.html",relativePath:"create/graphql.md",key:"v-86b1f726",path:"/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/create/introduction.html",relativePath:"create/introduction.md",key:"v-94114ee6",path:"/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/create/manifest.html",relativePath:"create/manifest.md",key:"v-5d960c2d",path:"/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/create/mapping.html",relativePath:"create/mapping.md",key:"v-03d2d023",path:"/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243},{level:3,title:"Custom chain rpc calls",slug:"custom-chain-rpc-calls",normalizedTitle:"custom chain rpc calls",charIndex:11465}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage Custom chain rpc calls",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above. And to use custom rpc call, please see section Custom chain rpc calls\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.\n\n\n# Custom chain rpc calls\n\nTo support customised chain RPC calls, we must manually inject RPC definitions for typesBundle, allowing per-spec configuration. You can define the typesBundle in the project.yml. And please remember only isHistoric type of calls are supported.\n\n...\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]",\n  }\n  typesBundle: {\n    spec: {\n      chainname: {\n        rpc: {\n          kitties: {\n            getKittyPrice:{\n                description: string,\n                params: [\n                  {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                  },\n                  {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                  }\n                ],\n                type: "Balance",\n            }\n          }\n        }\n      }\n    }\n  }\n\n',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above. and to use custom rpc call, please see section custom chain rpc calls\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.\n\n\n# custom chain rpc calls\n\nto support customised chain rpc calls, we must manually inject rpc definitions for typesbundle, allowing per-spec configuration. you can define the typesbundle in the project.yml. and please remember only ishistoric type of calls are supported.\n\n...\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]",\n  }\n  typesbundle: {\n    spec: {\n      chainname: {\n        rpc: {\n          kitties: {\n            getkittyprice:{\n                description: string,\n                params: [\n                  {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                  },\n                  {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                  }\n                ],\n                type: "balance",\n            }\n          }\n        }\n      }\n    }\n  }\n\n',charsets:{cjk:!0}},{frontmatter:{},regularPath:"/de/",relativePath:"de/README.md",key:"v-4806233e",path:"/de/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/de/create/graphql.html",relativePath:"de/create/graphql.md",key:"v-64c45226",path:"/de/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/de/create/introduction.html",relativePath:"de/create/introduction.md",key:"v-19f3891b",path:"/de/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/de/create/manifest.html",relativePath:"de/create/manifest.md",key:"v-4f881571",path:"/de/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/de/faqs/faqs.html",relativePath:"de/faqs/faqs.md",key:"v-3d649b57",path:"/de/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/de/create/mapping.html",relativePath:"de/create/mapping.md",key:"v-f95dcc66",path:"/de/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/de/install/install.html",relativePath:"de/install/install.md",key:"v-46088ee7",path:"/de/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/de/miscellaneous/ambassadors.html",relativePath:"de/miscellaneous/ambassadors.md",key:"v-4f6d6333",path:"/de/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/de/miscellaneous/branding.html",relativePath:"de/miscellaneous/branding.md",key:"v-25b5132d",path:"/de/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/de/miscellaneous/contributing.html",relativePath:"de/miscellaneous/contributing.md",key:"v-220de14d",path:"/de/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/de/miscellaneous/social_media.html",relativePath:"de/miscellaneous/social_media.md",key:"v-8fd36766",path:"/de/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/de/publish/connect.html",relativePath:"de/publish/connect.md",key:"v-576a0161",path:"/de/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/de/publish/publish.html",relativePath:"de/publish/publish.md",key:"v-45bb1917",path:"/de/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/de/publish/upgrade.html",relativePath:"de/publish/upgrade.md",key:"v-04677efd",path:"/de/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/de/query/graphql.html",relativePath:"de/query/graphql.md",key:"v-03e13271",path:"/de/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/de/query/query.html",relativePath:"de/query/query.md",key:"v-306d6933",path:"/de/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/de/quickstart/helloworld-hosted.html",relativePath:"de/quickstart/helloworld-hosted.md",key:"v-754465ed",path:"/de/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/de/quickstart/helloworld-localhost.html",relativePath:"de/quickstart/helloworld-localhost.md",key:"v-756db2b5",path:"/de/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/de/quickstart/quickstart.html",relativePath:"de/quickstart/quickstart.md",key:"v-e30ce652",path:"/de/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/de/run/run.html",relativePath:"de/run/run.md",key:"v-bc8b07b2",path:"/de/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/de/quickstart/understanding-helloworld.html",relativePath:"de/quickstart/understanding-helloworld.md",key:"v-76f709af",path:"/de/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"The Sandbox",frontmatter:{},regularPath:"/de/run/sandbox.html",relativePath:"de/run/sandbox.md",key:"v-5c0f832f",path:"/de/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/de/tutorials_examples/howto.html",relativePath:"de/tutorials_examples/howto.md",key:"v-89f298e6",path:"/de/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/de/tutorials_examples/introduction.html",relativePath:"de/tutorials_examples/introduction.md",key:"v-65f40cc9",path:"/de/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/de/tutorials_examples/terminology.html",relativePath:"de/tutorials_examples/terminology.md",key:"v-4162124d",path:"/de/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{frontmatter:{},regularPath:"/es/",relativePath:"es/README.md",key:"v-1397d61e",path:"/es/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/es/create/graphql.html",relativePath:"es/create/graphql.md",key:"v-514b550d",path:"/es/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/es/create/manifest.html",relativePath:"es/create/manifest.md",key:"v-38986917",path:"/es/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/es/create/introduction.html",relativePath:"es/create/introduction.md",key:"v-7c3a4841",path:"/es/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/es/create/mapping.html",relativePath:"es/create/mapping.md",key:"v-06fe97ed",path:"/es/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/es/faqs/faqs.html",relativePath:"es/faqs/faqs.md",key:"v-56f85486",path:"/es/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/es/install/install.html",relativePath:"es/install/install.md",key:"v-2f18e28d",path:"/es/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/es/miscellaneous/ambassadors.html",relativePath:"es/miscellaneous/ambassadors.md",key:"v-76f64b99",path:"/es/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/es/miscellaneous/branding.html",relativePath:"es/miscellaneous/branding.md",key:"v-2e6d4e4d",path:"/es/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/es/miscellaneous/contributing.html",relativePath:"es/miscellaneous/contributing.md",key:"v-29564a26",path:"/es/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/es/miscellaneous/social_media.html",relativePath:"es/miscellaneous/social_media.md",key:"v-fd457426",path:"/es/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/es/publish/connect.html",relativePath:"es/publish/connect.md",key:"v-407a5507",path:"/es/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/es/publish/publish.html",relativePath:"es/publish/publish.md",key:"v-2ecb6cbd",path:"/es/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/es/publish/upgrade.html",relativePath:"es/publish/upgrade.md",key:"v-25105aba",path:"/es/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/es/query/query.html",relativePath:"es/query/query.md",key:"v-2d2f1159",path:"/es/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/es/query/graphql.html",relativePath:"es/query/graphql.md",key:"v-0b973857",path:"/es/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/es/quickstart/helloworld-hosted.html",relativePath:"es/quickstart/helloworld-hosted.md",key:"v-ee61d266",path:"/es/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/es/quickstart/helloworld-localhost.html",relativePath:"es/quickstart/helloworld-localhost.md",key:"v-2f06345b",path:"/es/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/es/quickstart/quickstart.html",relativePath:"es/quickstart/quickstart.md",key:"v-19c469bd",path:"/es/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/es/quickstart/understanding-helloworld.html",relativePath:"es/quickstart/understanding-helloworld.md",key:"v-6ba4f6d5",path:"/es/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/es/run/run.html",relativePath:"es/run/run.md",key:"v-ab158e66",path:"/es/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/es/run/sandbox.html",relativePath:"es/run/sandbox.md",key:"v-58d12b55",path:"/es/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/es/tutorials_examples/howto.html",relativePath:"es/tutorials_examples/howto.md",key:"v-76eb85ed",path:"/es/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/es/tutorials_examples/introduction.html",relativePath:"es/tutorials_examples/introduction.md",key:"v-1f8c8e6f",path:"/es/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/es/tutorials_examples/terminology.html",relativePath:"es/tutorials_examples/terminology.md",key:"v-5bbe0a6d",path:"/es/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/faqs/faqs.html",relativePath:"faqs/faqs.md",key:"v-1265970d",path:"/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{frontmatter:{},regularPath:"/id/",relativePath:"id/README.md",key:"v-560420fe",path:"/id/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/id/create/graphql.html",relativePath:"id/create/graphql.md",key:"v-56f909a6",path:"/id/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/id/create/introduction.html",relativePath:"id/create/introduction.md",key:"v-6a073232",path:"/id/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/id/create/manifest.html",relativePath:"id/create/manifest.md",key:"v-7e31bc3d",path:"/id/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/id/create/mapping.html",relativePath:"id/create/mapping.md",key:"v-eb9283e6",path:"/id/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/id/faqs/faqs.html",relativePath:"id/faqs/faqs.md",key:"v-47ab00ba",path:"/id/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/id/install/install.html",relativePath:"id/install/install.md",key:"v-74b235b3",path:"/id/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/id/miscellaneous/ambassadors.html",relativePath:"id/miscellaneous/ambassadors.md",key:"v-42d0237f",path:"/id/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/id/miscellaneous/branding.html",relativePath:"id/miscellaneous/branding.md",key:"v-73371d26",path:"/id/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/id/miscellaneous/contributing.html",relativePath:"id/miscellaneous/contributing.md",key:"v-932646e6",path:"/id/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/id/miscellaneous/social_media.html",relativePath:"id/miscellaneous/social_media.md",key:"v-4c75478d",path:"/id/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/id/publish/connect.html",relativePath:"id/publish/connect.md",key:"v-f3d8afa6",path:"/id/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/id/publish/publish.html",relativePath:"id/publish/publish.md",key:"v-7464bfe3",path:"/id/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/id/publish/upgrade.html",relativePath:"id/publish/upgrade.md",key:"v-331125c9",path:"/id/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/id/query/graphql.html",relativePath:"id/query/graphql.md",key:"v-7981e486",path:"/id/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/id/query/query.html",relativePath:"id/query/query.md",key:"v-5e3758ff",path:"/id/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/id/quickstart/helloworld-hosted.html",relativePath:"id/quickstart/helloworld-hosted.md",key:"v-eabc40a6",path:"/id/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/id/quickstart/helloworld-localhost.html",relativePath:"id/quickstart/helloworld-localhost.md",key:"v-79dd94fe",path:"/id/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/id/quickstart/quickstart.html",relativePath:"id/quickstart/quickstart.md",key:"v-26e5d623",path:"/id/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/id/quickstart/understanding-helloworld.html",relativePath:"id/quickstart/understanding-helloworld.md",key:"v-3977c37b",path:"/id/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/id/run/run.html",relativePath:"id/run/run.md",key:"v-5ee434f3",path:"/id/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/id/run/sandbox.html",relativePath:"id/run/sandbox.md",key:"v-ec4d1a0a",path:"/id/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/id/tutorials_examples/howto.html",relativePath:"id/tutorials_examples/howto.md",key:"v-7c9e504d",path:"/id/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/id/tutorials_examples/introduction.html",relativePath:"id/tutorials_examples/introduction.md",key:"v-98d0e0d6",path:"/id/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/id/tutorials_examples/terminology.html",relativePath:"id/tutorials_examples/terminology.md",key:"v-0765aae6",path:"/id/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/install/install.html",relativePath:"install/install.md",key:"v-2680104d",path:"/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{frontmatter:{},regularPath:"/it/",relativePath:"it/README.md",key:"v-188a7204",path:"/it/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/it/create/graphql.html",relativePath:"it/create/graphql.md",key:"v-1466252d",path:"/it/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/it/create/introduction.html",relativePath:"it/create/introduction.md",key:"v-73c98272",path:"/it/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/it/create/manifest.html",relativePath:"it/create/manifest.md",key:"v-1553fc1d",path:"/it/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/it/create/mapping.html",relativePath:"it/create/mapping.md",key:"v-6bcd2fe6",path:"/it/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/it/faqs/faqs.html",relativePath:"it/faqs/faqs.md",key:"v-784c5b83",path:"/it/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/it/install/install.html",relativePath:"it/install/install.md",key:"v-0bd47593",path:"/it/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/it/miscellaneous/ambassadors.html",relativePath:"it/miscellaneous/ambassadors.md",key:"v-a8374142",path:"/it/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/it/miscellaneous/contributing.html",relativePath:"it/miscellaneous/contributing.md",key:"v-36e11e8d",path:"/it/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/it/miscellaneous/branding.html",relativePath:"it/miscellaneous/branding.md",key:"v-2a342b6d",path:"/it/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/it/miscellaneous/social_media.html",relativePath:"it/miscellaneous/social_media.md",key:"v-662cece6",path:"/it/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/it/publish/connect.html",relativePath:"it/publish/connect.md",key:"v-1d35e80d",path:"/it/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/it/publish/publish.html",relativePath:"it/publish/publish.md",key:"v-0b86ffc3",path:"/it/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/it/publish/upgrade.html",relativePath:"it/publish/upgrade.md",key:"v-6b9934ae",path:"/it/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/it/query/graphql.html",relativePath:"it/query/graphql.md",key:"v-a188fcc6",path:"/it/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/it/query/query.html",relativePath:"it/query/query.md",key:"v-296dfe42",path:"/it/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/it/quickstart/helloworld-hosted.html",relativePath:"it/quickstart/helloworld-hosted.md",key:"v-f39834a6",path:"/it/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/it/quickstart/helloworld-localhost.html",relativePath:"it/quickstart/helloworld-localhost.md",key:"v-0ca4553e",path:"/it/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/it/quickstart/quickstart.html",relativePath:"it/quickstart/quickstart.md",key:"v-475b7a03",path:"/it/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/it/quickstart/understanding-helloworld.html",relativePath:"it/quickstart/understanding-helloworld.md",key:"v-2fc0fb5b",path:"/it/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/it/run/run.html",relativePath:"it/run/run.md",key:"v-ec5d765a",path:"/it/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/it/run/sandbox.html",relativePath:"it/run/sandbox.md",key:"v-16eb1adb",path:"/it/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/it/tutorials_examples/howto.html",relativePath:"it/tutorials_examples/howto.md",key:"v-0f11ce4d",path:"/it/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/it/tutorials_examples/introduction.html",relativePath:"it/tutorials_examples/introduction.md",key:"v-2b97a116",path:"/it/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/it/tutorials_examples/terminology.html",relativePath:"it/tutorials_examples/terminology.md",key:"v-444c16e6",path:"/it/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/ja/create/manifest.html",relativePath:"ja/create/manifest.md",key:"v-f2a4a7f6",path:"/ja/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/ja/create/introduction.html",relativePath:"ja/create/introduction.md",key:"v-027260af",path:"/ja/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/ja/create/mapping.html",relativePath:"ja/create/mapping.md",key:"v-8bf930e6",path:"/ja/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/ja/install/install.html",relativePath:"ja/install/install.md",key:"v-7d2e257b",path:"/ja/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/ja/faqs/faqs.html",relativePath:"ja/faqs/faqs.md",key:"v-6d65c06b",path:"/ja/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/ja/miscellaneous/ambassadors.html",relativePath:"ja/miscellaneous/ambassadors.md",key:"v-0a98e772",path:"/ja/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/ja/miscellaneous/branding.html",relativePath:"ja/miscellaneous/branding.md",key:"v-95e01226",path:"/ja/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/ja/miscellaneous/contributing.html",relativePath:"ja/miscellaneous/contributing.md",key:"v-1738500d",path:"/ja/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/ja/miscellaneous/social_media.html",relativePath:"ja/miscellaneous/social_media.md",key:"v-a57e89e6",path:"/ja/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/ja/publish/connect.html",relativePath:"ja/publish/connect.md",key:"v-e2e0d016",path:"/ja/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/ja/publish/publish.html",relativePath:"ja/publish/publish.md",key:"v-7ce0afab",path:"/ja/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/ja/publish/upgrade.html",relativePath:"ja/publish/upgrade.md",key:"v-3b8d1591",path:"/ja/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/ja/query/graphql.html",relativePath:"ja/query/graphql.md",key:"v-bf8e4ef6",path:"/ja/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/ja/quickstart/helloworld-hosted.html",relativePath:"ja/quickstart/helloworld-hosted.md",key:"v-02e16a2d",path:"/ja/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/ja/query/query.html",relativePath:"ja/query/query.md",key:"v-d5d38272",path:"/ja/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/ja/quickstart/helloworld-localhost.html",relativePath:"ja/quickstart/helloworld-localhost.md",key:"v-bab9656e",path:"/ja/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/ja/quickstart/quickstart.html",relativePath:"ja/quickstart/quickstart.md",key:"v-c098962a",path:"/ja/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/ja/quickstart/understanding-helloworld.html",relativePath:"ja/quickstart/understanding-helloworld.md",key:"v-2f10357a",path:"/ja/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/ja/run/run.html",relativePath:"ja/run/run.md",key:"v-6a0310bb",path:"/ja/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/ja/run/sandbox.html",relativePath:"ja/run/sandbox.md",key:"v-7e8f4e7a",path:"/ja/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/ja/tutorials_examples/howto.html",relativePath:"ja/tutorials_examples/howto.md",key:"v-062f2666",path:"/ja/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/ja/tutorials_examples/introduction.html",relativePath:"ja/tutorials_examples/introduction.md",key:"v-d9acb146",path:"/ja/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{frontmatter:{},regularPath:"/ko/",relativePath:"ko/README.md",key:"v-14d51344",path:"/ko/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/ja/tutorials_examples/terminology.html",relativePath:"ja/tutorials_examples/terminology.md",key:"v-07038c0d",path:"/ja/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/ko/create/graphql.html",relativePath:"ko/create/graphql.md",key:"v-f004ba66",path:"/ko/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/ko/create/introduction.html",relativePath:"ko/create/introduction.md",key:"v-64b91fd5",path:"/ko/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/ko/create/manifest.html",relativePath:"ko/create/manifest.md",key:"v-6fbdffab",path:"/ko/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/ko/create/mapping.html",relativePath:"ko/create/mapping.md",key:"v-3db0e5ad",path:"/ko/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/ko/faqs/faqs.html",relativePath:"ko/faqs/faqs.md",key:"v-0484fad1",path:"/ko/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/ko/install/install.html",relativePath:"ko/install/install.md",key:"v-663e7921",path:"/ko/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/ko/miscellaneous/ambassadors.html",relativePath:"ko/miscellaneous/ambassadors.md",key:"v-223c74ad",path:"/ko/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/ko/miscellaneous/branding.html",relativePath:"ko/miscellaneous/branding.md",key:"v-846f9be6",path:"/ko/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/ko/miscellaneous/social_media.html",relativePath:"ko/miscellaneous/social_media.md",key:"v-7687b4ad",path:"/ko/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/ko/miscellaneous/contributing.html",relativePath:"ko/miscellaneous/contributing.md",key:"v-3f016ca6",path:"/ko/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/ko/publish/connect.html",relativePath:"ko/publish/connect.md",key:"v-779feb9b",path:"/ko/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/ko/publish/publish.html",relativePath:"ko/publish/publish.md",key:"v-65f10351",path:"/ko/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/ko/publish/upgrade.html",relativePath:"ko/publish/upgrade.md",key:"v-249d6937",path:"/ko/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/ko/query/graphql.html",relativePath:"ko/query/graphql.md",key:"v-b022432a",path:"/ko/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/ko/query/query.html",relativePath:"ko/query/query.md",key:"v-dc503226",path:"/ko/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/ko/quickstart/helloworld-hosted.html",relativePath:"ko/quickstart/helloworld-hosted.md",key:"v-166c1b0d",path:"/ko/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/ko/quickstart/helloworld-localhost.html",relativePath:"ko/quickstart/helloworld-localhost.md",key:"v-5c3bceef",path:"/ko/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/ko/quickstart/quickstart.html",relativePath:"ko/quickstart/quickstart.md",key:"v-2afe91d1",path:"/ko/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/ko/quickstart/understanding-helloworld.html",relativePath:"ko/quickstart/understanding-helloworld.md",key:"v-45b45b2e",path:"/ko/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/ko/run/run.html",relativePath:"ko/run/run.md",key:"v-72bdcd61",path:"/ko/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/ko/run/sandbox.html",relativePath:"ko/run/sandbox.md",key:"v-850bfe2e",path:"/ko/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/ko/tutorials_examples/howto.html",relativePath:"ko/tutorials_examples/howto.md",key:"v-8e6581a6",path:"/ko/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/ko/tutorials_examples/introduction.html",relativePath:"ko/tutorials_examples/introduction.md",key:"v-4cc22903",path:"/ko/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/ko/tutorials_examples/terminology.html",relativePath:"ko/tutorials_examples/terminology.md",key:"v-215f842d",path:"/ko/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/miscellaneous/ambassadors.html",relativePath:"miscellaneous/ambassadors.md",key:"v-ae40a866",path:"/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/miscellaneous/branding.html",relativePath:"miscellaneous/branding.md",key:"v-a45391be",path:"/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/miscellaneous/contributing.html",relativePath:"miscellaneous/contributing.md",key:"v-6d299e43",path:"/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/miscellaneous/social_media.html",relativePath:"miscellaneous/social_media.md",key:"v-034044f3",path:"/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/publish/connect.html",relativePath:"publish/connect.md",key:"v-b6710da6",path:"/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/publish/publish.html",relativePath:"publish/publish.md",key:"v-529f2d66",path:"/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/publish/upgrade.html",relativePath:"publish/upgrade.md",key:"v-5be2726d",path:"/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/query/graphql.html",relativePath:"query/graphql.md",key:"v-31c257a6",path:"/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/query/query.html",relativePath:"query/query.md",key:"v-ecedcbe6",path:"/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/quickstart/helloworld-hosted.html",relativePath:"quickstart/helloworld-hosted.md",key:"v-562bf2a5",path:"/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/quickstart/helloworld-localhost.html",relativePath:"quickstart/helloworld-localhost.md",key:"v-bc9cfe26",path:"/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/quickstart/quickstart.html",relativePath:"quickstart/quickstart.md",key:"v-867568e6",path:"/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{frontmatter:{},regularPath:"/ru/",relativePath:"ru/README.md",key:"v-3bb71ffe",path:"/ru/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/quickstart/understanding-helloworld.html",relativePath:"quickstart/understanding-helloworld.md",key:"v-d4874966",path:"/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/ru/create/graphql.html",relativePath:"ru/create/graphql.md",key:"v-1b3677a6",path:"/ru/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/ru/create/introduction.html",relativePath:"ru/create/introduction.md",key:"v-1e91fed2",path:"/ru/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/ru/create/manifest.html",relativePath:"ru/create/manifest.md",key:"v-6a2b59ed",path:"/ru/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/ru/create/mapping.html",relativePath:"ru/create/mapping.md",key:"v-afcff1e6",path:"/ru/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/ru/faqs/faqs.html",relativePath:"ru/faqs/faqs.md",key:"v-249ce353",path:"/ru/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/ru/install/install.html",relativePath:"ru/install/install.md",key:"v-60abd363",path:"/ru/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/ru/miscellaneous/ambassadors.html",relativePath:"ru/miscellaneous/ambassadors.md",key:"v-4a7311a2",path:"/ru/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/ru/miscellaneous/branding.html",relativePath:"ru/miscellaneous/branding.md",key:"v-7ce7626d",path:"/ru/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/ru/miscellaneous/contributing.html",relativePath:"ru/miscellaneous/contributing.md",key:"v-0258bce6",path:"/ru/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/ru/miscellaneous/social_media.html",relativePath:"ru/miscellaneous/social_media.md",key:"v-d647e6e6",path:"/ru/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/ru/publish/connect.html",relativePath:"ru/publish/connect.md",key:"v-720d45dd",path:"/ru/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/ru/publish/publish.html",relativePath:"ru/publish/publish.md",key:"v-605e5d93",path:"/ru/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/ru/publish/upgrade.html",relativePath:"ru/publish/upgrade.md",key:"v-1f0ac379",path:"/ru/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/ru/query/query.html",relativePath:"ru/query/query.md",key:"v-0ffdfaaf",path:"/ru/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/ru/query/graphql.html",relativePath:"ru/query/graphql.md",key:"v-50fd6d6d",path:"/ru/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/ru/quickstart/helloworld-hosted.html",relativePath:"ru/quickstart/helloworld-hosted.md",key:"v-38af4ead",path:"/ru/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/ru/quickstart/helloworld-localhost.html",relativePath:"ru/quickstart/helloworld-localhost.md",key:"v-953a799e",path:"/ru/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/ru/quickstart/quickstart.html",relativePath:"ru/quickstart/quickstart.md",key:"v-0133a45a",path:"/ru/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/ru/quickstart/understanding-helloworld.html",relativePath:"ru/quickstart/understanding-helloworld.md",key:"v-05c24d2b",path:"/ru/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/ru/run/run.html",relativePath:"ru/run/run.md",key:"v-0df44aba",path:"/ru/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/ru/run/sandbox.html",relativePath:"ru/run/sandbox.md",key:"v-3ba014ab",path:"/ru/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/ru/tutorials_examples/howto.html",relativePath:"ru/tutorials_examples/howto.md",key:"v-11f6eb4d",path:"/ru/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/ru/tutorials_examples/introduction.html",relativePath:"ru/tutorials_examples/introduction.md",key:"v-b42dc576",path:"/ru/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/ru/tutorials_examples/terminology.html",relativePath:"ru/tutorials_examples/terminology.md",key:"v-48f9c38d",path:"/ru/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/run/run.html",relativePath:"run/run.md",key:"v-09dfd14d",path:"/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\nexport DB_HOST=localhost\nsubql-query --name <project_name> --playground\n\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\nexport db_host=localhost\nsubql-query --name <project_name> --playground\n\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/run/sandbox.html",relativePath:"run/sandbox.md",key:"v-07cf064d",path:"/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/tutorials_examples/howto.html",relativePath:"tutorials_examples/howto.md",key:"v-462cc17b",path:"/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/tutorials_examples/terminology.html",relativePath:"tutorials_examples/terminology.md",key:"v-05bf836b",path:"/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/tutorials_examples/introduction.html",relativePath:"tutorials_examples/introduction.md",key:"v-65affcad",path:"/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{frontmatter:{},regularPath:"/uk/",relativePath:"uk/README.md",key:"v-5dda9d1e",path:"/uk/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/uk/create/graphql.html",relativePath:"uk/create/graphql.md",key:"v-6dcc960d",path:"/uk/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/uk/create/introduction.html",relativePath:"uk/create/introduction.md",key:"v-67670071",path:"/uk/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/uk/create/mapping.html",relativePath:"uk/create/mapping.md",key:"v-237fd8ed",path:"/uk/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Manifest File",frontmatter:{},regularPath:"/uk/create/manifest.html",relativePath:"uk/create/manifest.md",key:"v-3a2d0547",path:"/uk/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/uk/faqs/faqs.html",relativePath:"uk/faqs/faqs.md",key:"v-6e8c87ed",path:"/uk/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/uk/install/install.html",relativePath:"uk/install/install.md",key:"v-30ad7ebd",path:"/uk/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/uk/miscellaneous/ambassadors.html",relativePath:"uk/miscellaneous/ambassadors.md",key:"v-0841246e",path:"/uk/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/uk/miscellaneous/branding.html",relativePath:"uk/miscellaneous/branding.md",key:"v-daa61166",path:"/uk/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/uk/miscellaneous/contributing.html",relativePath:"uk/miscellaneous/contributing.md",key:"v-392eb7ed",path:"/uk/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/uk/miscellaneous/social_media.html",relativePath:"uk/miscellaneous/social_media.md",key:"v-6191ba26",path:"/uk/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/uk/publish/connect.html",relativePath:"uk/publish/connect.md",key:"v-420ef137",path:"/uk/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/uk/publish/publish.html",relativePath:"uk/publish/publish.md",key:"v-306008ed",path:"/uk/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/uk/publish/upgrade.html",relativePath:"uk/publish/upgrade.md",key:"v-21e7225a",path:"/uk/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/uk/query/graphql.html",relativePath:"uk/query/graphql.md",key:"v-ada9f2f2",path:"/uk/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/uk/query/query.html",relativePath:"uk/query/query.md",key:"v-7bc99189",path:"/uk/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/uk/quickstart/helloworld-hosted.html",relativePath:"uk/quickstart/helloworld-hosted.md",key:"v-01b4cdcd",path:"/uk/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/uk/quickstart/helloworld-localhost.html",relativePath:"uk/quickstart/helloworld-localhost.md",key:"v-9ea17eea",path:"/uk/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Hello World Explained",frontmatter:{},regularPath:"/uk/quickstart/understanding-helloworld.html",relativePath:"uk/quickstart/understanding-helloworld.md",key:"v-7943c1f6",path:"/uk/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/uk/run/run.html",relativePath:"uk/run/run.md",key:"v-47b39cfd",path:"/uk/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/uk/quickstart/quickstart.html",relativePath:"uk/quickstart/quickstart.md",key:"v-77c86fed",path:"/uk/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/uk/run/sandbox.html",relativePath:"uk/run/sandbox.md",key:"v-b128a8f6",path:"/uk/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/uk/tutorials_examples/howto.html",relativePath:"uk/tutorials_examples/howto.md",key:"v-2ca3ee26",path:"/uk/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/uk/tutorials_examples/introduction.html",relativePath:"uk/tutorials_examples/introduction.md",key:"v-bd94cac2",path:"/uk/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/uk/tutorials_examples/terminology.html",relativePath:"uk/tutorials_examples/terminology.md",key:"v-3b041b6d",path:"/uk/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{frontmatter:{},regularPath:"/vi/",relativePath:"vi/README.md",key:"v-e894ac04",path:"/vi/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/vi/create/graphql.html",relativePath:"vi/create/graphql.md",key:"v-9cd12ba6",path:"/vi/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/vi/create/introduction.html",relativePath:"vi/create/introduction.md",key:"v-62e23092",path:"/vi/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/vi/create/manifest.html",relativePath:"vi/create/manifest.md",key:"v-e7c9cde6",path:"/vi/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/vi/create/mapping.html",relativePath:"vi/create/mapping.md",key:"v-674aad0d",path:"/vi/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/vi/faqs/faqs.html",relativePath:"vi/faqs/faqs.md",key:"v-6989e673",path:"/vi/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/vi/install/install.html",relativePath:"vi/install/install.md",key:"v-fac8dafa",path:"/vi/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/vi/miscellaneous/ambassadors.html",relativePath:"vi/miscellaneous/ambassadors.md",key:"v-8b57cb62",path:"/vi/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/vi/miscellaneous/branding.html",relativePath:"vi/miscellaneous/branding.md",key:"v-3795786d",path:"/vi/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/vi/miscellaneous/contributing.html",relativePath:"vi/miscellaneous/contributing.md",key:"v-fbfd20e6",path:"/vi/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/vi/miscellaneous/social_media.html",relativePath:"vi/miscellaneous/social_media.md",key:"v-1809da8d",path:"/vi/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/ja/create/graphql.html",relativePath:"ja/create/graphql.md",key:"v-045024ad",path:"/ja/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{frontmatter:{},regularPath:"/ja/",relativePath:"ja/README.md",key:"v-2a03c37e",path:"/ja/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/vi/publish/upgrade.html",relativePath:"vi/publish/upgrade.md",key:"v-40fa8299",path:"/vi/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/vi/query/graphql.html",relativePath:"vi/query/graphql.md",key:"v-7636cee6",path:"/vi/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/vi/query/query.html",relativePath:"vi/query/query.md",key:"v-290cdc62",path:"/vi/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/vi/quickstart/helloworld-hosted.html",relativePath:"vi/quickstart/helloworld-hosted.md",key:"v-19ad78ad",path:"/vi/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/vi/quickstart/helloworld-localhost.html",relativePath:"vi/quickstart/helloworld-localhost.md",key:"v-33ab2251",path:"/vi/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Hello World Explained",frontmatter:{},regularPath:"/vi/quickstart/understanding-helloworld.html",relativePath:"vi/quickstart/understanding-helloworld.md",key:"v-7c7a576a",path:"/vi/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/vi/quickstart/quickstart.html",relativePath:"vi/quickstart/quickstart.md",key:"v-3ac4ae1a",path:"/vi/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/vi/run/sandbox.html",relativePath:"vi/run/sandbox.md",key:"v-171babcb",path:"/vi/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/vi/run/run.html",relativePath:"vi/run/run.md",key:"v-258149c3",path:"/vi/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/vi/tutorials_examples/introduction.html",relativePath:"vi/tutorials_examples/introduction.md",key:"v-24317c65",path:"/vi/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/vi/tutorials_examples/howto.html",relativePath:"vi/tutorials_examples/howto.md",key:"v-131f5d4d",path:"/vi/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/vi/tutorials_examples/terminology.html",relativePath:"vi/tutorials_examples/terminology.md",key:"v-73d3498d",path:"/vi/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{frontmatter:{},regularPath:"/zh/",relativePath:"zh/README.md",key:"v-8fbc73c4",path:"/zh/",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"Welcome to SubQueryâs Docs\n\nExplore and transform your chain data to build intuitive dApps faster!\n\n\nQuick Start Guide\n\nUnderstand SubQuery by getting hands on with a traditional Hello World example. Using a template project within a Docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nGet started\n * Tutorials and Examples\n   \n   Learning by doing. Tutorials and examples on how to build various SubQuery projects.\n\n * Technical Reference Docs\n   \n   Written by developers for developers. Find what you need to build awesome dApps quickly.\n\n * The SubQuery Network\n   \n   SubQueryâs decentralised future. Read more about how indexers and consumers are rewarded.\n\n\nFAQ\n\n * What is SubQuery?\n   \n   SubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n   \n   READ MORE\n * What is the best way to get started with SubQuery?\n   \n   The best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n * How can I contribute or give feedback to SubQuery?\n   \n   We love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guidelines (coming soon).\n   \n   READ MORE\n * How much does it cost to host my project in SubQuery Projects?\n   \n   Hosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery Hosted) tutorial.\n   \n   HOSTING YOUR PROJECT\n\n\nFor further frequently asked questions, please see our FAQ's page.\n\nIntegrating with your Custom Chain?\n\nWhether you're building a new parachain or an entirely new blockchain on Substrate - SubQuery can help you index and troubleshoot your chain's data. SubQuery is designed to easily integrate with a custom Substrate based chain.\n\nLEARN HOW TO INTEGRATE WITH YOUR CHAIN\n\nSupport and Contribute\n\nHave a question or interested to know more or how you can contribute? Weâd love to hear from you. Please contact us via email or social media from the links below. Need technical expertise? Join our Discord community and receive support from our passionate community members.\n\nJOIN THE CONVERSATION ON DISCORD\nContact us hello@subquery.network\nFollow us on social\ndiscord twitter medium telegram github matrix linkedin\nSubQuery Â© 2021",normalizedContent:"welcome to subqueryâs docs\n\nexplore and transform your chain data to build intuitive dapps faster!\n\n\nquick start guide\n\nunderstand subquery by getting hands on with a traditional hello world example. using a template project within a docker environment, you can quickly get a node up and running and start querying a blockchain in just a few minutes with a few simple commands.\n\nget started\n * tutorials and examples\n   \n   learning by doing. tutorials and examples on how to build various subquery projects.\n\n * technical reference docs\n   \n   written by developers for developers. find what you need to build awesome dapps quickly.\n\n * the subquery network\n   \n   subqueryâs decentralised future. read more about how indexers and consumers are rewarded.\n\n\nfaq\n\n * what is subquery?\n   \n   subquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n   \n   read more\n * what is the best way to get started with subquery?\n   \n   the best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n * how can i contribute or give feedback to subquery?\n   \n   we love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guidelines (coming soon).\n   \n   read more\n * how much does it cost to host my project in subquery projects?\n   \n   hosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n   \n   hosting your project\n\n\nfor further frequently asked questions, please see our faq's page.\n\nintegrating with your custom chain?\n\nwhether you're building a new parachain or an entirely new blockchain on substrate - subquery can help you index and troubleshoot your chain's data. subquery is designed to easily integrate with a custom substrate based chain.\n\nlearn how to integrate with your chain\n\nsupport and contribute\n\nhave a question or interested to know more or how you can contribute? weâd love to hear from you. please contact us via email or social media from the links below. need technical expertise? join our discord community and receive support from our passionate community members.\n\njoin the conversation on discord\ncontact us hello@subquery.network\nfollow us on social\ndiscord twitter medium telegram github matrix linkedin\nsubquery Â© 2021",charsets:{}},{title:"GraphQL Schema",frontmatter:{},regularPath:"/zh/create/graphql.html",relativePath:"zh/create/graphql.md",key:"v-4cb5e50d",path:"/zh/create/graphql.html",headers:[{level:2,title:"Defining Entities",slug:"defining-entities",normalizedTitle:"defining entities",charIndex:21},{level:3,title:"Entities",slug:"entities",normalizedTitle:"entities",charIndex:30},{level:3,title:"Supported scalars and types",slug:"supported-scalars-and-types",normalizedTitle:"supported scalars and types",charIndex:919},{level:2,title:"Indexing by non-primary-key field",slug:"indexing-by-non-primary-key-field",normalizedTitle:"indexing by non-primary-key field",charIndex:1270},{level:2,title:"Entity Relationships",slug:"entity-relationships",normalizedTitle:"entity relationships",charIndex:1174},{level:3,title:"One-to-One Relationships",slug:"one-to-one-relationships",normalizedTitle:"one-to-one relationships",charIndex:3229},{level:3,title:"One-to-Many relationships",slug:"one-to-many-relationships",normalizedTitle:"one-to-many relationships",charIndex:3660},{level:3,title:"Many-to-Many relationships",slug:"many-to-many-relationships",normalizedTitle:"many-to-many relationships",charIndex:3944},{level:3,title:"Reverse Lookups",slug:"reverse-lookups",normalizedTitle:"reverse lookups",charIndex:4908},{level:2,title:"JSON type",slug:"json-type",normalizedTitle:"json type",charIndex:1256},{level:3,title:"Define JSON directive",slug:"define-json-directive",normalizedTitle:"define json directive",charIndex:6225},{level:3,title:"Querying JSON fields",slug:"querying-json-fields",normalizedTitle:"querying json fields",charIndex:6890}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Defining Entities Entities Supported scalars and types Indexing by non-primary-key field Entity Relationships One-to-One Relationships One-to-Many relationships Many-to-Many relationships Reverse Lookups JSON type Define JSON directive Querying JSON fields",content:"# GraphQL Schema\n\n\n# Defining Entities\n\nThe schema.graphql file defines the various GraphQL schemas. Due to the way that the GraphQL query language works, the schema file essentially dictates the shape of your data from SubQuery. To learn more about how to write in GraphQL schema language, we recommend checking out Schemas and Types.\n\nImportant: When you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# Entities\n\nEach entity must define its required fields id with the type of ID!. It is used as the primary key and unique among all entities of the same type.\n\nNon-nullable fields in the entity are indicated by !. Please see the example below:\n\ntype Example @entity {\n  id: ID! # id field is always required and must look like this\n  name: String! # This is a required field\n  address: String # This is an optional field\n}\n\n\n\n# Supported scalars and types\n\nWe currently supporting flowing scalars types:\n\n * ID\n * Int\n * String\n * BigInt\n * Date\n * Boolean\n * <EntityName> for nested relationship entities, you might use the defined entity's name as one of the fields. Please see in Entity Relationships.\n * JSON can alternatively store structured data, please see JSON type\n\n\n# Indexing by non-primary-key field\n\nTo improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nHowever, we don't allow users to add @index annotation on any JSON object. By default, indexes are automatically added to foreign keys and for JSON fields in the database, but only to enhance query service performance.\n\nHere is an example.\n\ntype User @entity {\n  id: ID!\n  name: String! @index(unique: true) # unique can be set to true or false\n  title: Title! # Indexes are automatically added to foreign key field \n}\n\ntype Title @entity {\n  id: ID!  \n  name: String! @index(unique:true)\n}\n\n\nAssuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. This makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nIf a field is not unique, the maximum result set size is 100\n\nWhen code generation is run, this will automatically create a getByName under the User model, and the foreign key field title will create a getByTitleId method, which both can directly be accessed in the mapping function.\n\n/* Prepare a record for title entity */\nINSERT INTO titles (id, name) VALUES ('id_1', 'Captain')\n\n\n// Handler in mapping function\nimport {User} from \"../types/models/User\"\nimport {Title} from \"../types/models/Title\"\n\nconst jack = await User.getByName('Jack Sparrow');\n\nconst captainTitle = await Title.getByName('Captain');\n\nconst pirateLords = await User.getByTitleId(captainTitle.id); // List of all Captains\n\n\n\n# Entity Relationships\n\nAn entity often has nested relationships with other entities. Setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\nDifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# One-to-One Relationships\n\nOne-to-one relationships are the default when only a single entity is mapped to another.\n\nExample: A passport will only belong to one person and a person only has one passport (in this example):\n\ntype Person @entity {\n  id: ID!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\nor\n\ntype Person @entity {\n  id: ID!\n  passport: Passport!\n}\n\ntype Passport @entity {\n  id: ID!\n  owner: Person!\n}\n\n\n\n# One-to-Many relationships\n\nYou can use square brackets to indicate that a field type includes multiple entities.\n\nExample: A person can have multiple accounts.\n\ntype Person @entity {\n  id: ID!\n  accounts: [Account] \n}\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\n\n\n# Many-to-Many relationships\n\nA many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nExample: Each person is a part of multiple groups (PersonGroup) and groups have multiple different people (PersonGroup).\n\ntype Person @entity {\n  id: ID!\n  name: String!\n  groups: [PersonGroup]\n}\n\ntype PersonGroup @entity {\n  id: ID!\n  person: Person!\n  Group: Group!\n}\n\ntype Group @entity {\n  id: ID!\n  name: String!\n  persons: [PersonGroup]\n}\n\n\nAlso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nFor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nThis will establish a bi-directional relationship between two Accounts (from and to) through Transfer table.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# Reverse Lookups\n\nTo enable a reverse lookup on an entity to a relation, attach @derivedFrom to the field and point to its reverse lookup field of another entity.\n\nThis creates a virtual field on the entity that can be queried.\n\nThe Transfer \"from\" an Account is accessible from the Account entity by setting the sentTransfer or receivedTransfer as having their value derived from the respective from or to fields.\n\ntype Account @entity {\n  id: ID!\n  publicAddress: String!\n  sentTransfers: [Transfer] @derivedFrom(field: \"from\")\n  receivedTransfers: [Transfer] @derivedFrom(field: \"to\")\n}\n\ntype Transfer @entity {\n  id: ID!\n  amount: BigInt\n  from: Account!\n  to: Account!\n}\n\n\n\n# JSON type\n\nWe are supporting saving data as a JSON type, which is a fast way to store structured data. We'll automatically generate corresponding JSON interfaces for querying this data and save you time defining and managing entities.\n\nWe recommend users use the JSON type in the following scenarios:\n\n * When storing structured data in a single field is more manageable than creating multiple separate entities.\n * Saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * The schema is volatile and changes frequently\n\n\n# Define JSON directive\n\nDefine the property as a JSON type by adding the jsonField annotation in the entity. This will automatically generate interfaces for all JSON objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nUnlike the entity, the jsonField directive object does not require any id field. A JSON object is also able to nest with other JSON objects.\n\ntype AddressDetail @jsonField {\n  street: String!\n  district: String!\n}\n\ntype ContactCard @jsonField {\n  phone: String!\n  address: AddressDetail # Nested JSON\n}\n\ntype User @entity {\n  id: ID! \n  contact: [ContactCard] # Store a list of JSON objects\n}\n\n\n\n# Querying JSON fields\n\nThe drawback of using JSON types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nHowever, the impact is still acceptable in our query service. Here is an example of how to use the contains operator in the GraphQL query on a JSON field to find the first 5 users who own a phone number that contains '0064'.\n\n#To find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactCard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactCard\n    }\n  }\n}\n",normalizedContent:"# graphql schema\n\n\n# defining entities\n\nthe schema.graphql file defines the various graphql schemas. due to the way that the graphql query language works, the schema file essentially dictates the shape of your data from subquery. to learn more about how to write in graphql schema language, we recommend checking out schemas and types.\n\nimportant: when you make any changes to the schema file, please ensure that you regenerate your types directory with the following command yarn codegen\n\n\n# entities\n\neach entity must define its required fields id with the type of id!. it is used as the primary key and unique among all entities of the same type.\n\nnon-nullable fields in the entity are indicated by !. please see the example below:\n\ntype example @entity {\n  id: id! # id field is always required and must look like this\n  name: string! # this is a required field\n  address: string # this is an optional field\n}\n\n\n\n# supported scalars and types\n\nwe currently supporting flowing scalars types:\n\n * id\n * int\n * string\n * bigint\n * date\n * boolean\n * <entityname> for nested relationship entities, you might use the defined entity's name as one of the fields. please see in entity relationships.\n * json can alternatively store structured data, please see json type\n\n\n# indexing by non-primary-key field\n\nto improve query performance, index an entity field simply by implementing the @index annotation on a non-primary-key field.\n\nhowever, we don't allow users to add @index annotation on any json object. by default, indexes are automatically added to foreign keys and for json fields in the database, but only to enhance query service performance.\n\nhere is an example.\n\ntype user @entity {\n  id: id!\n  name: string! @index(unique: true) # unique can be set to true or false\n  title: title! # indexes are automatically added to foreign key field \n}\n\ntype title @entity {\n  id: id!  \n  name: string! @index(unique:true)\n}\n\n\nassuming we knew this user's name, but we don't know the exact id value, rather than extract all users and then filtering by name we can add @index behind the name field. this makes querying much faster and we can additionally pass the unique: true to ensure uniqueness.\n\nif a field is not unique, the maximum result set size is 100\n\nwhen code generation is run, this will automatically create a getbyname under the user model, and the foreign key field title will create a getbytitleid method, which both can directly be accessed in the mapping function.\n\n/* prepare a record for title entity */\ninsert into titles (id, name) values ('id_1', 'captain')\n\n\n// handler in mapping function\nimport {user} from \"../types/models/user\"\nimport {title} from \"../types/models/title\"\n\nconst jack = await user.getbyname('jack sparrow');\n\nconst captaintitle = await title.getbyname('captain');\n\nconst piratelords = await user.getbytitleid(captaintitle.id); // list of all captains\n\n\n\n# entity relationships\n\nan entity often has nested relationships with other entities. setting the field value to another entity name will define a one-to-one relationship between these two entities by default.\n\ndifferent entity relationships (one-to-one, one-to-many, and many-to-many) can be configured using the examples below.\n\n\n# one-to-one relationships\n\none-to-one relationships are the default when only a single entity is mapped to another.\n\nexample: a passport will only belong to one person and a person only has one passport (in this example):\n\ntype person @entity {\n  id: id!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\nor\n\ntype person @entity {\n  id: id!\n  passport: passport!\n}\n\ntype passport @entity {\n  id: id!\n  owner: person!\n}\n\n\n\n# one-to-many relationships\n\nyou can use square brackets to indicate that a field type includes multiple entities.\n\nexample: a person can have multiple accounts.\n\ntype person @entity {\n  id: id!\n  accounts: [account] \n}\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\n\n\n# many-to-many relationships\n\na many-to-many relationship can be achieved by implementing a mapping entity to connect the other two entities.\n\nexample: each person is a part of multiple groups (persongroup) and groups have multiple different people (persongroup).\n\ntype person @entity {\n  id: id!\n  name: string!\n  groups: [persongroup]\n}\n\ntype persongroup @entity {\n  id: id!\n  person: person!\n  group: group!\n}\n\ntype group @entity {\n  id: id!\n  name: string!\n  persons: [persongroup]\n}\n\n\nalso, it is possible to create a connection of the same entity in multiple fields of the middle entity.\n\nfor example, an account can have multiple transfers, and each transfer has a source and destination account.\n\nthis will establish a bi-directional relationship between two accounts (from and to) through transfer table.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# reverse lookups\n\nto enable a reverse lookup on an entity to a relation, attach @derivedfrom to the field and point to its reverse lookup field of another entity.\n\nthis creates a virtual field on the entity that can be queried.\n\nthe transfer \"from\" an account is accessible from the account entity by setting the senttransfer or receivedtransfer as having their value derived from the respective from or to fields.\n\ntype account @entity {\n  id: id!\n  publicaddress: string!\n  senttransfers: [transfer] @derivedfrom(field: \"from\")\n  receivedtransfers: [transfer] @derivedfrom(field: \"to\")\n}\n\ntype transfer @entity {\n  id: id!\n  amount: bigint\n  from: account!\n  to: account!\n}\n\n\n\n# json type\n\nwe are supporting saving data as a json type, which is a fast way to store structured data. we'll automatically generate corresponding json interfaces for querying this data and save you time defining and managing entities.\n\nwe recommend users use the json type in the following scenarios:\n\n * when storing structured data in a single field is more manageable than creating multiple separate entities.\n * saving arbitrary key/value user preferences (where the value can be boolean, textual, or numeric, and you don't want to have separate columns for different data types)\n * the schema is volatile and changes frequently\n\n\n# define json directive\n\ndefine the property as a json type by adding the jsonfield annotation in the entity. this will automatically generate interfaces for all json objects in your project under types/interfaces.ts, and you can access them in your mapping function.\n\nunlike the entity, the jsonfield directive object does not require any id field. a json object is also able to nest with other json objects.\n\ntype addressdetail @jsonfield {\n  street: string!\n  district: string!\n}\n\ntype contactcard @jsonfield {\n  phone: string!\n  address: addressdetail # nested json\n}\n\ntype user @entity {\n  id: id! \n  contact: [contactcard] # store a list of json objects\n}\n\n\n\n# querying json fields\n\nthe drawback of using json types is a slight impact on query efficiency when filtering, as each time it performs a text search, it is on the entire entity.\n\nhowever, the impact is still acceptable in our query service. here is an example of how to use the contains operator in the graphql query on a json field to find the first 5 users who own a phone number that contains '0064'.\n\n#to find the the first 5 users own phone numbers contains '0064'.\n\nquery{\n  user(\n    first: 5,\n    filter: {\n      contactcard: {\n        contains: [{ phone: \"0064\" }]\n    }\n}){\n    nodes{\n      id\n      contactcard\n    }\n  }\n}\n",charsets:{}},{title:"Creating a SubQuery Project",frontmatter:{},regularPath:"/zh/create/introduction.html",relativePath:"zh/create/introduction.md",key:"v-8de7f97e",path:"/zh/create/introduction.html",headers:[{level:2,title:"The Basic Workflow",slug:"the-basic-workflow",normalizedTitle:"the basic workflow",charIndex:273},{level:2,title:"Directory Structure",slug:"directory-structure",normalizedTitle:"directory structure",charIndex:1236},{level:2,title:"Code Generation",slug:"code-generation",normalizedTitle:"code generation",charIndex:1600},{level:2,title:"Build",slug:"build",normalizedTitle:"build",charIndex:2057},{level:2,title:"Logging",slug:"logging",normalizedTitle:"logging",charIndex:2282}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"The Basic Workflow Directory Structure Code Generation Build Logging",content:"# Creating a SubQuery Project\n\nIn the quick start guide, we very quickly ran through an example to give you a taste of what SubQuery is and how it works. Here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# The Basic Workflow\n\nSome of the following examples will assume you have successfully initialized the starter package in the Quick start section. From that starter package, we'll walk through the standard process to customise and implement your SubQuery project.\n\n 1. Initialise your project using subql init PROJECT_NAME\n 2. Update the Manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see Manifest File\n 3. Create GraphQL entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see GraphQL Schema\n 4. Add all the mapping functions (eg mappingHandlers.ts) you wish to invoke to transform chain data to the GraphQL entities that you have defined - see Mapping\n 5. Generate, build, and publish your code to SubQuery Projects (or run in your own local node) - see Running and Querying your Starter Project in our quick start guide.\n\n\n# Directory Structure\n\nThe following map provides an overview of the directory structure of a SubQuery project when the init command is run.\n\n- project-name\n  L package.json\n  L project.yaml\n  L README.md\n  L schema.graphql\n  L tsconfig.json\n  L docker-compose.yml\n  L src\n    L index.ts\n    L mappings\n      L mappingHandlers.ts\n  L .gitignore\n\n\nFor example:\n\n\n\n\n# Code Generation\n\nWhenever you change your GraphQL entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nThis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. These classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the GraphQL Schema.\n\n\n# Build\n\nIn order to run your SubQuery Project on a locally hosted SubQuery Node, you need to first build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Logging\n\nThe console.log method is no longer supported. Instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('Info level message');\nlogger.debug('Debugger level message');\nlogger.warn('Warning level message');\n\n\nTo use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nTo use logger.debug, an additional step is required. Add --log-level=debug to your command line.\n\nIf you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nYou should now see the new logging in the terminal screen.\n\n",normalizedContent:"# creating a subquery project\n\nin the quick start guide, we very quickly ran through an example to give you a taste of what subquery is and how it works. here we'll take a closer look at the workflow when creating your project and the key files you'll be working with.\n\n\n# the basic workflow\n\nsome of the following examples will assume you have successfully initialized the starter package in the quick start section. from that starter package, we'll walk through the standard process to customise and implement your subquery project.\n\n 1. initialise your project using subql init project_name\n 2. update the manifest file (project.yaml) to include information about your blockchain, and the entities that you will map - see manifest file\n 3. create graphql entities in your schema (schema.graphql) that define the shape of the data that you will extract and persist for querying - see graphql schema\n 4. add all the mapping functions (eg mappinghandlers.ts) you wish to invoke to transform chain data to the graphql entities that you have defined - see mapping\n 5. generate, build, and publish your code to subquery projects (or run in your own local node) - see running and querying your starter project in our quick start guide.\n\n\n# directory structure\n\nthe following map provides an overview of the directory structure of a subquery project when the init command is run.\n\n- project-name\n  l package.json\n  l project.yaml\n  l readme.md\n  l schema.graphql\n  l tsconfig.json\n  l docker-compose.yml\n  l src\n    l index.ts\n    l mappings\n      l mappinghandlers.ts\n  l .gitignore\n\n\nfor example:\n\n\n\n\n# code generation\n\nwhenever you change your graphql entities, you must regenerate your types directory with the following command.\n\nyarn codegen\n\n\nthis will create a new directory (or update the existing) src/types which contain generated entity classes for each type you have defined previously in schema.graphql. these classes provide type-safe entity loading, read and write access to entity fields - see more about this process in the graphql schema.\n\n\n# build\n\nin order to run your subquery project on a locally hosted subquery node, you need to first build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# logging\n\nthe console.log method is no longer supported. instead, a logger module has been injected in the types, which means we can support a logger that can accept various logging levels.\n\nlogger.info('info level message');\nlogger.debug('debugger level message');\nlogger.warn('warning level message');\n\n\nto use logger.info or logger.warn, just place the line into your mapping file.\n\n\n\nto use logger.debug, an additional step is required. add --log-level=debug to your command line.\n\nif you are running a docker container, add this line to your docker-compose.yaml file.\n\n\n\nyou should now see the new logging in the terminal screen.\n\n",charsets:{}},{title:"Manifest File",frontmatter:{},regularPath:"/zh/create/manifest.html",relativePath:"zh/create/manifest.md",key:"v-541b37d2",path:"/zh/create/manifest.html",headers:[{level:2,title:"Network Filters",slug:"network-filters",normalizedTitle:"network filters",charIndex:1928},{level:2,title:"Mapping Filters",slug:"mapping-filters",normalizedTitle:"mapping filters",charIndex:3088},{level:2,title:"Custom Chains",slug:"custom-chains",normalizedTitle:"custom chains",charIndex:4383}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Network Filters Mapping Filters Custom Chains",content:'# Manifest File\n\nThe Manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how SubQuery will index and transform the chain data.\n\nThe Manifest can be in either YAML or JSON format. In this document, we will use YAML in all the examples. Below is a standard example of a basic project.yaml.\n\nspecVersion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # Optionally provide the HTTP endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n        - handler: handleEvent\n          kind: substrate/EventHandler\n          filter: #Filter is optional but suggested to speed up event processing\n            module: balances\n            method: Deposit\n        - handler: handleCall\n          kind: substrate/CallHandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - This must be a full archive node.\n * network.dictionary optionally provides the HTTP endpoint of a full chain dictionary to speed up processing - see Running an Indexer\n * dataSources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/Runtime for now.\n   * startBlock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# Network Filters\n\nUsually the user will create a SubQuery and expect to reuse it for both their testnet and mainnet environments (e.g Polkadot and Kusama). Between networks, various options are likely to be different (e.g. index start block). Therefore, we allow users to define different details for each data source which means that one SubQuery project can still be used across multiple networks.\n\nUsers can add a filter on dataSources to decide which data source to run on each network.\n\nBelow is an example that shows different data sources for both the Polkadot and Kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#Create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleBlock\n        kind: substrate/BlockHandler\n\ndataSources:\n  - name: polkadotRuntime\n    kind: substrate/Runtime\n    filter:  #Optional\n        specName: polkadot\n    startBlock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaRuntime\n    kind: substrate/Runtime\n    filter: \n        specName: kusama\n    startBlock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# Mapping Filters\n\nMapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nOnly incoming data that satisfy the filter conditions will be processed by the mapping functions. Mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your SubQuery project and will improve indexing performance.\n\n#Example filter from callHandler\nfilter: \n   module: balances\n   method: Deposit\n   success: true\n\n\nThe following table explains filters supported by different handlers.\n\nHANDLER        SUPPORTED FILTER\nBlockHandler   specVersion\nEventHandler   module,method\nCallHandler    module,method ,success\n\n * Module and method filters are supported on any substrate-based chain.\n * The success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * The specVersion filter specifies the spec version range for a substrate block. The following examples describe how to set version ranges.\n\nfilter:\n  specVersion: [23, 24]   #Index block with specVersion in between 23 and 24 (inclusive).\n  specVersion: [100]      #Index block with specVersion greater than or equal 100.\n  specVersion: [null, 23] #Index block with specVersion less than or equal 23.\n\n\n\n# Custom Chains\n\nYou can index data from custom chains by also including chain types in the project.yaml. Declare the specific types supported by this blockchain in network.types. We support the additional types used by substrate runtime modules.\n\ntypesAlias, typesBundle, typesChain, and typesSpec are also supported.\n\nspecVersion: "0.0.1"\ndescription: "This subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "KittyIndex": "u32",\n    "Kitty": "[u8; 16]"\n  }\n# typesChain: { chain: { Type5: \'example\' } }\n# typesSpec: { spec: { Type6: \'example\' } }\ndataSources:\n  - name: runtime\n    kind: substrate/Runtime\n    startBlock: 1\n    filter:  #Optional\n      specName: kitty-chain \n    mapping:\n      handlers:\n        - handler: handleKittyBred\n          kind: substrate/CallHandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',normalizedContent:'# manifest file\n\nthe manifest project.yaml file can be seen as an entry point of your project and it defines most of the details on how subquery will index and transform the chain data.\n\nthe manifest can be in either yaml or json format. in this document, we will use yaml in all the examples. below is a standard example of a basic project.yaml.\n\nspecversion: "0.0.1"\ndescription: ""\nrepository: "https://github.com/subquery/subql-starter"\n\nschema: "./schema.graphql"\n\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n  # optionally provide the http endpoint of a full chain dictionary to speed up processing\n  dictionary: "https://api.subquery.network/sq/subquery/dictionary-polkadot"\n\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n        - handler: handleevent\n          kind: substrate/eventhandler\n          filter: #filter is optional but suggested to speed up event processing\n            module: balances\n            method: deposit\n        - handler: handlecall\n          kind: substrate/callhandler\n\n\n * network.endpoint defines the wss or ws endpoint of the blockchain to be indexed - this must be a full archive node.\n * network.dictionary optionally provides the http endpoint of a full chain dictionary to speed up processing - see running an indexer\n * datasources defines the data that will be filtered and extracted and the location of the mapping function handler for the data transformation to be applied.\n   * kind only supports substrate/runtime for now.\n   * startblock specifies the block height to start indexing from.\n   * filter will filter the data source to execute by the network endpoint spec name, see network filters\n   * mapping.handlers will list all the mapping functions and their corresponding handler types, with additional mapping filters.\n\n\n# network filters\n\nusually the user will create a subquery and expect to reuse it for both their testnet and mainnet environments (e.g polkadot and kusama). between networks, various options are likely to be different (e.g. index start block). therefore, we allow users to define different details for each data source which means that one subquery project can still be used across multiple networks.\n\nusers can add a filter on datasources to decide which data source to run on each network.\n\nbelow is an example that shows different data sources for both the polkadot and kusama networks.\n\n...\nnetwork:\n  endpoint: "wss://polkadot.api.onfinality.io/public-ws"\n\n#create a template to avoid redundancy\ndefinitions:\n  mapping: &mymapping\n    handlers:\n      - handler: handleblock\n        kind: substrate/blockhandler\n\ndatasources:\n  - name: polkadotruntime\n    kind: substrate/runtime\n    filter:  #optional\n        specname: polkadot\n    startblock: 1000\n    mapping: *mymapping #use template here\n  - name: kusamaruntime\n    kind: substrate/runtime\n    filter: \n        specname: kusama\n    startblock: 12000 \n    mapping: *mymapping # can reuse or change\n\n\n\n# mapping filters\n\nmapping filters are an extremely useful feature to decide what block, event, or extrinsic will trigger a mapping handler.\n\nonly incoming data that satisfy the filter conditions will be processed by the mapping functions. mapping filters are optional but are recommended as they significantly reduce the amount of data processed by your subquery project and will improve indexing performance.\n\n#example filter from callhandler\nfilter: \n   module: balances\n   method: deposit\n   success: true\n\n\nthe following table explains filters supported by different handlers.\n\nhandler        supported filter\nblockhandler   specversion\neventhandler   module,method\ncallhandler    module,method ,success\n\n * module and method filters are supported on any substrate-based chain.\n * the success filter takes a boolean value and can be used to filter the extrinsic by its success status.\n * the specversion filter specifies the spec version range for a substrate block. the following examples describe how to set version ranges.\n\nfilter:\n  specversion: [23, 24]   #index block with specversion in between 23 and 24 (inclusive).\n  specversion: [100]      #index block with specversion greater than or equal 100.\n  specversion: [null, 23] #index block with specversion less than or equal 23.\n\n\n\n# custom chains\n\nyou can index data from custom chains by also including chain types in the project.yaml. declare the specific types supported by this blockchain in network.types. we support the additional types used by substrate runtime modules.\n\ntypesalias, typesbundle, typeschain, and typesspec are also supported.\n\nspecversion: "0.0.1"\ndescription: "this subquery indexes kitty\'s birth info"\nrepository: "https://github.com/onfinality-io/subql-examples"\nschema: "./schema.graphql"\nnetwork:\n  endpoint: "ws://host.kittychain.io/public-ws"\n  types: {\n    "kittyindex": "u32",\n    "kitty": "[u8; 16]"\n  }\n# typeschain: { chain: { type5: \'example\' } }\n# typesspec: { spec: { type6: \'example\' } }\ndatasources:\n  - name: runtime\n    kind: substrate/runtime\n    startblock: 1\n    filter:  #optional\n      specname: kitty-chain \n    mapping:\n      handlers:\n        - handler: handlekittybred\n          kind: substrate/callhandler\n          filter:\n            module: kitties\n            method: breed\n            success: true\n',charsets:{}},{title:"Mapping",frontmatter:{},regularPath:"/zh/create/mapping.html",relativePath:"zh/create/mapping.md",key:"v-026927ed",path:"/zh/create/mapping.html",headers:[{level:2,title:"Block Handler",slug:"block-handler",normalizedTitle:"block handler",charIndex:598},{level:2,title:"Event Handler",slug:"event-handler",normalizedTitle:"event handler",charIndex:559},{level:2,title:"Call Handler",slug:"call-handler",normalizedTitle:"call handler",charIndex:579},{level:2,title:"Query States",slug:"query-states",normalizedTitle:"query states",charIndex:3091},{level:2,title:"RPC calls",slug:"rpc-calls",normalizedTitle:"rpc calls",charIndex:4101},{level:2,title:"Modules and Libraries",slug:"modules-and-libraries",normalizedTitle:"modules and libraries",charIndex:5044},{level:3,title:"Built-in modules",slug:"built-in-modules",normalizedTitle:"built-in modules",charIndex:5474},{level:3,title:"Third-party libraries",slug:"third-party-libraries",normalizedTitle:"third-party libraries",charIndex:6124},{level:2,title:"Custom Substrate Chains",slug:"custom-substrate-chains",normalizedTitle:"custom substrate chains",charIndex:4994},{level:3,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:6841},{level:4,title:"Metadata",slug:"metadata",normalizedTitle:"metadata",charIndex:7083},{level:4,title:"Type definitions",slug:"type-definitions",normalizedTitle:"type definitions",charIndex:7789},{level:4,title:"Packages",slug:"packages",normalizedTitle:"packages",charIndex:8923},{level:3,title:"Type generation",slug:"type-generation",normalizedTitle:"type generation",charIndex:10049},{level:3,title:"Usage",slug:"usage",normalizedTitle:"usage",charIndex:11243}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Block Handler Event Handler Call Handler Query States RPC calls Modules and Libraries Built-in modules Third-party libraries Custom Substrate Chains Preparation Metadata Type definitions Packages Type generation Usage",content:'# Mapping\n\nMapping functions define how chain data is transformed into the optimised GraphQL entities that we have previously defined in the schema.graphql file.\n\nMappings are written in a subset of TypeScript called AssemblyScript which can be compiled to WASM (WebAssembly).\n\n * Mappings are defined in the src/mappings directory and are exported as a function\n * These mappings are also exported in src/index.ts\n * The mappings files are reference in project.yaml under the mapping handlers.\n\nThere are three classes of mappings functions; Block handlers, Event Handlers, and Call Handlers.\n\n\n# Block Handler\n\nYou can use block handlers to capture information each time a new block is attached to the Substrate chain, e.g. block number. To achieve this, a defined BlockHandler will be called once for every block.\n\nimport {SubstrateBlock} from "@subql/types";\n\nexport async function handleBlock(block: SubstrateBlock): Promise<void> {\n    // Create a new StarterEntity with the block hash as it\'s ID\n    const record = new starterEntity(block.block.header.hash.toString());\n    record.field1 = block.block.header.number.toNumber();\n    await record.save();\n}\n\n\nA SubstrateBlock is an extended interface type of signedBlock, but also includes the specVersion and timestamp.\n\n\n# Event Handler\n\nYou can use event handlers to capture information when certain events are included on a new block. The events that are part of the default Substrate runtime and a block may contain multiple events.\n\nDuring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. Any type of event will trigger the mapping, allowing activity with the data source to be captured. You should use Mapping Filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {SubstrateEvent} from "@subql/types";\n\nexport async function handleEvent(event: SubstrateEvent): Promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // Retrieve the record by its ID\n    const record = new starterEntity(event.extrinsic.block.block.header.hash.toString());\n    record.field2 = account.toString();\n    record.field3 = (balance as Balance).toBigInt();\n    await record.save();\n\n\nA SubstrateEvent is an extended interface type of the EventRecord. Besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# Call Handler\n\nCall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nThe SubstrateExtrinsic extends GenericExtrinsic. It is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. Additionally, it records the success status of this extrinsic.\n\n\n# Query States\n\nOur goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). Therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nThese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.queryMulti() will make multiple queries of different types at the current block.\n\nThese are the interfaces we do NOT support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesAt\n * api.query.<module>.<method>.entriesPaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysAt\n * api.query.<module>.<method>.keysPaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeAt\n\nSee an example of using this API in our validator-threshold example use case.\n\n\n# RPC calls\n\nWe also support some API RPC methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. A core premise of SubQuery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical RPC calls.\n\nDocuments in JSON-RPC provide some methods that take BlockHash as an input parameter (e.g. at?: BlockHash), which are now permitted. We have also modified these methods to take the current indexing block hash by default.\n\n// Let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// Original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getBlock(blockhash);\n\n// It will use the current block has by default like so\nconst b2 = await api.rpc.chain.getBlock();\n\n\n * For Custom Substrate Chains RPC calls, see usage.\n\n\n# Modules and Libraries\n\nTo improve SubQuery\'s data processing capabilities, we have allowed some of the NodeJS\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nPlease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. Please report any bugs you find by creating an issue in GitHub.\n\n\n# Built-in modules\n\nCurrently, we allow the following NodeJS modules: assert, buffer, crypto, util, and path.\n\nRather than importing the whole module, we recommend only importing the required method(s) that you need. Some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashMessage} from "ethers/lib/utils"; //Good way\nimport {utils} from "ethers" //Bad way\n\nexport async function handleCall(extrinsic: SubstrateExtrinsic): Promise<void> {\n    const record = new starterEntity(extrinsic.block.block.header.hash.toString());\n    record.field1 = hashMessage(\'Hello\');\n    await record.save();\n}\n\n\n\n# Third-party libraries\n\nDue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by CommonJS.\n\nWe also support a hybrid library like @polkadot/* that uses ESM as default. However, if any other libraries depend on any modules in ESM format, the virtual machine will NOT compile and return an error.\n\n\n# Custom Substrate Chains\n\nSubQuery can be used on any Substrate-based chain, not just Polkadot or Kusama.\n\nYou can use a custom Substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nIn the following sections, we use our kitty example to explain the integration process.\n\n\n# Preparation\n\nCreate a new directory api-interfaces under the project src folder to store all required and generated files. We also create an api-interfaces/kitties directory as we want to add decoration in the API from the kitties module.\n\n# Metadata\n\nWe need metadata to generate the actual API endpoints. In the kitty example, we use an endpoint from a local testnet, and it provides additional types. Follow the steps in PolkadotJS metadata setup to retrieve a node\'s metadata from its HTTP endpoint.\n\ncurl -H "Content-Type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getMetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//Install the websocat\nbrew install websocat\n\n//Get metadata\necho state_getMetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nNext, copy and paste the output to a JSON file. In our kitty example, we have created api-interface/kitty.json.\n\n# Type definitions\n\nWe assume that the user knows the specific types and RPC support from the chain, and it is defined in the Manifest.\n\nFollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        Address: "AccountId",\n        LookupSource: "AccountId",\n        KittyIndex: "u32",\n        Kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getKittyPrice\n    rpc: {\n        getKittyPrice:{\n            description: \'Get Kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'BlockHash\',\n                    isHistoric: true,\n                    isOptional: false\n                },\n                {\n                    name: \'kittyIndex\',\n                    type: \'KittyIndex\',\n                    isOptional: false\n                }\n            ],\n            type: \'Balance\'\n        }\n    }\n}\n\n\n# Packages\n\n * In the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). We also need ts-node as a development dependency to help us run the scripts.\n * We add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nHere is a simplified version of package.json. Make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devDependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# Type generation\n\nNow that preparation is completed, we are ready to generate types and metadata. Run the commands below:\n\n# Yarn to install new dependencies\nyarn\n\n# Generate types\nyarn generate:defs\n\n\nIn each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# Generate metadata\nyarn generate:meta\n\n\nThis command will generate the metadata and a new api-augment for the APIs. As we don\'t want to use the built-in API, we will need to replace them by adding an explicit override in our tsconfig.json. After the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compilerOptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# Usage\n\nNow in the mapping function, we can show how the metadata and types actually decorate the API. The RPC endpoint will support the modules and methods we declared above.\n\nexport async function kittyApiHandler(): Promise<void> {\n    //return the KittyIndex type\n    const nextKittyId = await api.query.kitties.nextKittyId();\n    // return the Kitty type, input parameters types are AccountId and KittyIndex\n    const allKitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`Next kitty id ${nextKittyId}`)\n    //Custom rpc, set undefined to blockhash\n    const kittyPrice = await api.rpc.kitties.getKittyPrice(undefined,nextKittyId);\n}\n\n\nIf you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',normalizedContent:'# mapping\n\nmapping functions define how chain data is transformed into the optimised graphql entities that we have previously defined in the schema.graphql file.\n\nmappings are written in a subset of typescript called assemblyscript which can be compiled to wasm (webassembly).\n\n * mappings are defined in the src/mappings directory and are exported as a function\n * these mappings are also exported in src/index.ts\n * the mappings files are reference in project.yaml under the mapping handlers.\n\nthere are three classes of mappings functions; block handlers, event handlers, and call handlers.\n\n\n# block handler\n\nyou can use block handlers to capture information each time a new block is attached to the substrate chain, e.g. block number. to achieve this, a defined blockhandler will be called once for every block.\n\nimport {substrateblock} from "@subql/types";\n\nexport async function handleblock(block: substrateblock): promise<void> {\n    // create a new starterentity with the block hash as it\'s id\n    const record = new starterentity(block.block.header.hash.tostring());\n    record.field1 = block.block.header.number.tonumber();\n    await record.save();\n}\n\n\na substrateblock is an extended interface type of signedblock, but also includes the specversion and timestamp.\n\n\n# event handler\n\nyou can use event handlers to capture information when certain events are included on a new block. the events that are part of the default substrate runtime and a block may contain multiple events.\n\nduring the processing, the event handler will receive a substrate event as an argument with the event\'s typed inputs and outputs. any type of event will trigger the mapping, allowing activity with the data source to be captured. you should use mapping filters in your manifest to filter events to reduce the time it takes to index data and improve mapping performance.\n\nimport {substrateevent} from "@subql/types";\n\nexport async function handleevent(event: substrateevent): promise<void> {\n    const {event: {data: [account, balance]}} = event;\n    // retrieve the record by its id\n    const record = new starterentity(event.extrinsic.block.block.header.hash.tostring());\n    record.field2 = account.tostring();\n    record.field3 = (balance as balance).tobigint();\n    await record.save();\n\n\na substrateevent is an extended interface type of the eventrecord. besides the event data, it also includes an id (the block to which this event belongs) and the extrinsic inside of this block.\n\n\n# call handler\n\ncall handlers are used when you want to capture information on certain substrate extrinsics.\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field4 = extrinsic.block.timestamp;\n    await record.save();\n}\n\n\nthe substrateextrinsic extends genericextrinsic. it is assigned an id (the block to which this extrinsic belongs) and provides an extrinsic property that extends the events among this block. additionally, it records the success status of this extrinsic.\n\n\n# query states\n\nour goal is to cover all data sources for users for mapping handlers (more than just the three interface event types above). therefore, we have exposed some of the @polkadot/api interfaces to increase capabilities.\n\nthese are the interfaces we currently support:\n\n * api.query.<module>.<method>() will query the current block.\n * api.query.<module>.<method>.multi() will make multiple queries of the same type at the current block.\n * api.querymulti() will make multiple queries of different types at the current block.\n\nthese are the interfaces we do not support currently:\n\n * api.tx.*\n * api.derive.*\n * api.query.<module>.<method>.at\n * api.query.<module>.<method>.entriesat\n * api.query.<module>.<method>.entriespaged\n * api.query.<module>.<method>.hash\n * api.query.<module>.<method>.keysat\n * api.query.<module>.<method>.keyspaged\n * api.query.<module>.<method>.range\n * api.query.<module>.<method>.sizeat\n\nsee an example of using this api in our validator-threshold example use case.\n\n\n# rpc calls\n\nwe also support some api rpc methods that are remote calls that allow the mapping function to interact with the actual node, query, and submission. a core premise of subquery is that it\'s deterministic, and therefore, to keep the results consistent we only allow historical rpc calls.\n\ndocuments in json-rpc provide some methods that take blockhash as an input parameter (e.g. at?: blockhash), which are now permitted. we have also modified these methods to take the current indexing block hash by default.\n\n// let\'s say we are currently indexing a block with this hash number\nconst blockhash = `0x844047c4cf1719ba6d54891e92c071a41e3dfe789d064871148e9d41ef086f6a`;\n\n// original method has an optional input is block hash\nconst b1 = await api.rpc.chain.getblock(blockhash);\n\n// it will use the current block has by default like so\nconst b2 = await api.rpc.chain.getblock();\n\n\n * for custom substrate chains rpc calls, see usage.\n\n\n# modules and libraries\n\nto improve subquery\'s data processing capabilities, we have allowed some of the nodejs\'s built-in modules for running mapping functions in the sandbox, and have allowed users to call third-party libraries.\n\nplease note this is an experimental feature and you may encounter bugs or issues that may negatively impact your mapping functions. please report any bugs you find by creating an issue in github.\n\n\n# built-in modules\n\ncurrently, we allow the following nodejs modules: assert, buffer, crypto, util, and path.\n\nrather than importing the whole module, we recommend only importing the required method(s) that you need. some methods in these modules may have dependencies that are unsupported and will fail on import.\n\nimport {hashmessage} from "ethers/lib/utils"; //good way\nimport {utils} from "ethers" //bad way\n\nexport async function handlecall(extrinsic: substrateextrinsic): promise<void> {\n    const record = new starterentity(extrinsic.block.block.header.hash.tostring());\n    record.field1 = hashmessage(\'hello\');\n    await record.save();\n}\n\n\n\n# third-party libraries\n\ndue to the limitations of the virtual machine in our sandbox, currently, we only support third-party libraries written by commonjs.\n\nwe also support a hybrid library like @polkadot/* that uses esm as default. however, if any other libraries depend on any modules in esm format, the virtual machine will not compile and return an error.\n\n\n# custom substrate chains\n\nsubquery can be used on any substrate-based chain, not just polkadot or kusama.\n\nyou can use a custom substrate-based chain and we provide tools to import types, interfaces, and additional methods automatically using @polkadot/typegen.\n\nin the following sections, we use our kitty example to explain the integration process.\n\n\n# preparation\n\ncreate a new directory api-interfaces under the project src folder to store all required and generated files. we also create an api-interfaces/kitties directory as we want to add decoration in the api from the kitties module.\n\n# metadata\n\nwe need metadata to generate the actual api endpoints. in the kitty example, we use an endpoint from a local testnet, and it provides additional types. follow the steps in polkadotjs metadata setup to retrieve a node\'s metadata from its http endpoint.\n\ncurl -h "content-type: application/json" -d \'{"id":"1", "jsonrpc":"2.0", "method": "state_getmetadata", "params":[]}\' http://localhost:9933\n\n\nor from its websocket endpoint with help from websocat:\n\n//install the websocat\nbrew install websocat\n\n//get metadata\necho state_getmetadata | websocat \'ws://127.0.0.1:9944\' --jsonrpc\n\n\nnext, copy and paste the output to a json file. in our kitty example, we have created api-interface/kitty.json.\n\n# type definitions\n\nwe assume that the user knows the specific types and rpc support from the chain, and it is defined in the manifest.\n\nfollowing types setup, we create :\n\n * src/api-interfaces/definitions.ts - this exports all the sub-folder definitions\n\nexport { default as kitties } from \'./kitties/definitions\';\n\n\n * src/api-interfaces/kitties/definitions.ts - type definitions for the kitties module\n\nexport default {\n    // custom types\n    types: {\n        address: "accountid",\n        lookupsource: "accountid",\n        kittyindex: "u32",\n        kitty: "[u8; 16]"\n    },\n    // custom rpc : api.rpc.kitties.getkittyprice\n    rpc: {\n        getkittyprice:{\n            description: \'get kitty price\',\n            params: [\n                {\n                    name: \'at\',\n                    type: \'blockhash\',\n                    ishistoric: true,\n                    isoptional: false\n                },\n                {\n                    name: \'kittyindex\',\n                    type: \'kittyindex\',\n                    isoptional: false\n                }\n            ],\n            type: \'balance\'\n        }\n    }\n}\n\n\n# packages\n\n * in the package.json file, make sure to add @polkadot/typegen as a development dependency and @polkadot/api as a regular dependency (ideally the same version). we also need ts-node as a development dependency to help us run the scripts.\n * we add scripts to run both types; generate:defs and metadata generate:meta generators (in that order, so metadata can use the types).\n\nhere is a simplified version of package.json. make sure in the scripts section the package name is correct and the directories are valid.\n\n{\n  "name": "kitty-birthinfo",\n  "scripts": {\n    "generate:defs": "ts-node --skip-project node_modules/.bin/polkadot-types-from-defs --package kitty-birthinfo/api-interfaces --input ./src/api-interfaces",\n    "generate:meta": "ts-node --skip-project node_modules/.bin/polkadot-types-from-chain --package kitty-birthinfo/api-interfaces --endpoint ./src/api-interfaces/kitty.json --output ./src/api-interfaces --strict"\n  },\n  "dependencies": {\n    "@polkadot/api": "^4.9.2"\n  },\n  "devdependencies": {\n    "typescript": "^4.1.3",\n    "@polkadot/typegen": "^4.9.2",\n    "ts-node": "^8.6.2"\n  }\n}\n\n\n\n# type generation\n\nnow that preparation is completed, we are ready to generate types and metadata. run the commands below:\n\n# yarn to install new dependencies\nyarn\n\n# generate types\nyarn generate:defs\n\n\nin each modules folder (eg /kitties), there should now be a generated types.ts that defines all interfaces from this modules\' definitions, also a file index.ts that exports them all.\n\n# generate metadata\nyarn generate:meta\n\n\nthis command will generate the metadata and a new api-augment for the apis. as we don\'t want to use the built-in api, we will need to replace them by adding an explicit override in our tsconfig.json. after the updates, the paths in the config will look like this (without the comments):\n\n{\n  "compileroptions": {\n      // this is the package name we use (in the interface imports, --package for generators) */\n      "kitty-birthinfo/*": ["src/*"],\n      // here we replace the @polkadot/api augmentation with our own, generated from chain\n      "@polkadot/api/augment": ["src/interfaces/augment-api.ts"],\n      // replace the augmented types with our own, as generated from definitions\n      "@polkadot/types/augment": ["src/interfaces/augment-types.ts"]\n    }\n}\n\n\n\n# usage\n\nnow in the mapping function, we can show how the metadata and types actually decorate the api. the rpc endpoint will support the modules and methods we declared above.\n\nexport async function kittyapihandler(): promise<void> {\n    //return the kittyindex type\n    const nextkittyid = await api.query.kitties.nextkittyid();\n    // return the kitty type, input parameters types are accountid and kittyindex\n    const allkitties  = await api.query.kitties.kitties(\'xxxxxxxxx\',123)\n    logger.info(`next kitty id ${nextkittyid}`)\n    //custom rpc, set undefined to blockhash\n    const kittyprice = await api.rpc.kitties.getkittyprice(undefined,nextkittyid);\n}\n\n\nif you wish to publish this project to our explorer, please include the generated files in src/api-interfaces.',charsets:{cjk:!0}},{title:"Frequently Asked Questions",frontmatter:{},regularPath:"/zh/faqs/faqs.html",relativePath:"zh/faqs/faqs.md",key:"v-39ce30bd",path:"/zh/faqs/faqs.html",headers:[{level:2,title:"What is SubQuery?",slug:"what-is-subquery",normalizedTitle:"what is subquery?",charIndex:33},{level:2,title:"What is the best way to get started with SubQuery?",slug:"what-is-the-best-way-to-get-started-with-subquery",normalizedTitle:"what is the best way to get started with subquery?",charIndex:384},{level:2,title:"How can I contribute or give feedback to SubQuery?",slug:"how-can-i-contribute-or-give-feedback-to-subquery",normalizedTitle:"how can i contribute or give feedback to subquery?",charIndex:699},{level:2,title:"How much does it cost to host my project in SubQuery Projects?",slug:"how-much-does-it-cost-to-host-my-project-in-subquery-projects",normalizedTitle:"how much does it cost to host my project in subquery projects?",charIndex:1094},{level:2,title:"What are deployment slots?",slug:"what-are-deployment-slots",normalizedTitle:"what are deployment slots?",charIndex:1378},{level:2,title:"What is the advantage of a staging slot?",slug:"what-is-the-advantage-of-a-staging-slot",normalizedTitle:"what is the advantage of a staging slot?",charIndex:2061},{level:2,title:"What are extrinsics?",slug:"what-are-extrinsics",normalizedTitle:"what are extrinsics?",charIndex:2566}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What is SubQuery? What is the best way to get started with SubQuery? How can I contribute or give feedback to SubQuery? How much does it cost to host my project in SubQuery Projects? What are deployment slots? What is the advantage of a staging slot? What are extrinsics?",content:"# Frequently Asked Questions\n\n\n# What is SubQuery?\n\nSubQuery is an open source project that allows developers to index, transform, and query Substrate chain data to power their applications.\n\nSubQuery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# What is the best way to get started with SubQuery?\n\nThe best way to get started with SubQuery is to try out our Hello World tutorial. This is a simple 5 min walk through of downloading the starter template, building the project, and then using Docker to run a node on your localhost and running a simple query.\n\n\n# How can I contribute or give feedback to SubQuery?\n\nWe love contributions and feedback from the community. To contribute code, fork the repository of interest and make your changes. Then submit a PR or Pull Request. Oh, don't forget to test as well! Also check out our contributions guide lines (TBA).\n\nTo give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# How much does it cost to host my project in SubQuery Projects?\n\nHosting your project in SubQuery Projects is absolutely free - it's is our way of giving back to the community. To learn how to host your project with us, please check out the Hello World (SubQuery hosted) tutorial.\n\n\n# What are deployment slots?\n\nDeployment slots are a feature in SubQuery Projects that is the equivalent of a development environment. For example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). Typically additional environments such as staging and pre-prod or even QA are included depending on the needs of the organisation and their development set up.\n\nSubQuery currently has two slots available. A staging slot and a production slot. This allows developers to deploy their SubQuery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# What is the advantage of a staging slot?\n\nThe main benefit of using a staging slot is that it allows you to prepare a new release of your SubQuery project without exposing it publicly. You can wait for the staging slot to reindex all data without affecting your production applications.\n\nThe staging slot is not shown to the public in the Explorer and has a unique URL that is visible only to you. And of course, the separate environment allows you to test your new code without affecting production.\n\n\n# What are extrinsics?\n\nIf you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. More formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. There are three categories of extrinsics. They are inherents, signed transactions, and unsigned transactions.\n\nInherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nSigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. They stands to pay a fee to have the transaction included on chain.\n\nUnsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. Unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. Because of this, the transaction queue lacks economic logic to prevent spam.\n\nFor more information, click here.",normalizedContent:"# frequently asked questions\n\n\n# what is subquery?\n\nsubquery is an open source project that allows developers to index, transform, and query substrate chain data to power their applications.\n\nsubquery also provides free, production grade hosting of projects for developers removing the responsiblity of manging infrastructure, and letting developers do what they do best - build.\n\n\n# what is the best way to get started with subquery?\n\nthe best way to get started with subquery is to try out our hello world tutorial. this is a simple 5 min walk through of downloading the starter template, building the project, and then using docker to run a node on your localhost and running a simple query.\n\n\n# how can i contribute or give feedback to subquery?\n\nwe love contributions and feedback from the community. to contribute code, fork the repository of interest and make your changes. then submit a pr or pull request. oh, don't forget to test as well! also check out our contributions guide lines (tba).\n\nto give feedback, contact us at hello@subquery.network or jump onto our discord channel\n\n\n# how much does it cost to host my project in subquery projects?\n\nhosting your project in subquery projects is absolutely free - it's is our way of giving back to the community. to learn how to host your project with us, please check out the hello world (subquery hosted) tutorial.\n\n\n# what are deployment slots?\n\ndeployment slots are a feature in subquery projects that is the equivalent of a development environment. for example, in any software organisation there is normally a development environment and a production environment as a minimum (ignoring localhost that is). typically additional environments such as staging and pre-prod or even qa are included depending on the needs of the organisation and their development set up.\n\nsubquery currently has two slots available. a staging slot and a production slot. this allows developers to deploy their subquery to the staging environment and all going well, \"promote to production\" at the click of a button.\n\n\n# what is the advantage of a staging slot?\n\nthe main benefit of using a staging slot is that it allows you to prepare a new release of your subquery project without exposing it publicly. you can wait for the staging slot to reindex all data without affecting your production applications.\n\nthe staging slot is not shown to the public in the explorer and has a unique url that is visible only to you. and of course, the separate environment allows you to test your new code without affecting production.\n\n\n# what are extrinsics?\n\nif you are already familiar with blockchain concepts, you can think of extrinsics as comparable to transactions. more formally though, an extrinsic is a piece of information that comes from outside the chain and is included in a block. there are three categories of extrinsics. they are inherents, signed transactions, and unsigned transactions.\n\ninherent extrinsics are pieces of information that are not signed and only inserted into a block by the block author.\n\nsigned transaction extrinsics are transactions that contain a signature of the account that issued the transaction. they stands to pay a fee to have the transaction included on chain.\n\nunsigned transactions extrinsics are transactions that do not contain a signature of the account that issued the transaction. unsigned transactions extrinsics should be used with care because there is nobody paying a fee, becaused it is signed. because of this, the transaction queue lacks economic logic to prevent spam.\n\nfor more information, click here.",charsets:{}},{title:"Installing SubQuery",frontmatter:{},regularPath:"/zh/install/install.html",relativePath:"zh/install/install.md",key:"v-671a44e6",path:"/zh/install/install.html",headers:[{level:2,title:"Install @subql/cli",slug:"install-subql-cli",normalizedTitle:"install @subql/cli",charIndex:214},{level:2,title:"Install @subql/node",slug:"install-subql-node",normalizedTitle:"install @subql/node",charIndex:582},{level:2,title:"Install @subql/query",slug:"install-subql-query",normalizedTitle:"install @subql/query",charIndex:1183}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Install @subql/cli Install @subql/node Install @subql/query",content:'# Installing SubQuery\n\nThere are various components required when creating a SubQuery project. The @subql/node component is required to run an indexer. The @subql/query library is required to generate queries.\n\n\n# Install @subql/cli\n\nThe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\nInstall SubQuery CLI globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/cli\n\n# NPM\nnpm install -g @subql/cli\n\n\nYou can then run help to see available commands and usage provide by CLI:\n\nsubql help\n\n\n\n# Install @subql/node\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\nInstall SubQuery node globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/node\n\n# NPM\nnpm install -g @subql/node\n\n\nOnce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.\n\n\n# Install @subql/query\n\nThe SubQuery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\nInstall SubQuery query globally on your terminal by using Yarn or NPM:\n\n# Yarn\nyarn global add @subql/query\n\n# NPM\nnpm install -g @subql/query\n\n\n> Note: If you are using Docker or hosting your project in SubQuery Projects, you do can skip this step also. This is because the SubQuery node is already provided in the Docker container and the hosting infrastructure.',normalizedContent:'# installing subquery\n\nthere are various components required when creating a subquery project. the @subql/node component is required to run an indexer. the @subql/query library is required to generate queries.\n\n\n# install @subql/cli\n\nthe @subql/cli library helps to create a project framework or scaffold meaning you don\'t have to start from scratch.\n\ninstall subquery cli globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/cli\n\n# npm\nnpm install -g @subql/cli\n\n\nyou can then run help to see available commands and usage provide by cli:\n\nsubql help\n\n\n\n# install @subql/node\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\ninstall subquery node globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/node\n\n# npm\nnpm install -g @subql/node\n\n\nonce installed, you can can start a node with:\n\nsubql-node <command>\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step. this is because the subquery node is already provided in the docker container and the hosting infrastructure.\n\n\n# install @subql/query\n\nthe subquery query library provides a service that allows you to query your project in a "playground" environment via your browser.\n\ninstall subquery query globally on your terminal by using yarn or npm:\n\n# yarn\nyarn global add @subql/query\n\n# npm\nnpm install -g @subql/query\n\n\n> note: if you are using docker or hosting your project in subquery projects, you do can skip this step also. this is because the subquery node is already provided in the docker container and the hosting infrastructure.',charsets:{}},{title:"Ambassador Program",frontmatter:{},regularPath:"/zh/miscellaneous/ambassadors.html",relativePath:"zh/miscellaneous/ambassadors.md",key:"v-3bc0b2ce",path:"/zh/miscellaneous/ambassadors.html",headers:[{level:2,title:"What we Believe In",slug:"what-we-believe-in",normalizedTitle:"what we believe in",charIndex:208},{level:2,title:"Our Ambassador Program",slug:"our-ambassador-program",normalizedTitle:"our ambassador program",charIndex:1327},{level:3,title:"Ambassador Benefits",slug:"ambassador-benefits",normalizedTitle:"ambassador benefits",charIndex:1663},{level:2,title:"How does it work",slug:"how-does-it-work",normalizedTitle:"how does it work",charIndex:2855},{level:2,title:"Ambassador Activities",slug:"ambassador-activities",normalizedTitle:"ambassador activities",charIndex:3770}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"What we Believe In Our Ambassador Program Ambassador Benefits How does it work Ambassador Activities",content:"# Ambassador Program\n\n\n\nWe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\nApply Now!\n\n\n# What we Believe In\n\nOur team came together with the shared vision to build the foundations of a flexible and inclusive data service for the Polkadot ecosystem.\n\nBuilt by developers, for developers: SubQuery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. SubQuery is only successful if the Polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nIntegrity and Accountability: We have team members in Auckland, Shanghai, and Sydney so remote work is important to us. We expect that our team is empowered and works autonomously together to achieve our goals. A key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\nInclusive Guidance and Support: Blockchain is hard, and everyone needs help sometimes. There is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. We learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# Our Ambassador Program\n\nOur SubQuery Ambassador program aims to find community leaders passionate about Polkadot and SubQuery. Weâre looking for self-starters that can spread the word about SubQuery in their local areas and provide support to new developers that want to use SubQuery to build amazing apps and services on Polkadot.\n\n\n# Ambassador Benefits\n\nAt SubQuery, we work hard to achieve what we do. Similarly, Ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nFunding and Support: You may be rewarded for good work with early opportunities into private sales and bounties. Additionally, weâll be providing funding grants for you to run community meetups.\n\nSubQuery Team Access: Youâll have direct access to the core SubQuery team with opportunities for hands-on training, exclusive AMAs with our leaders and developers, and insight into our roadmap.\n\nNetwork Development: Expect to grow your professional network by being an Ambassador for one of the top Polkadot projects. Meet other ambassadors around the world and receive introductions to local Polkadot projects that we need to support locally. You might even get free entry to represent SubQuery in events in your local area.\n\nSwag and other free stuff: Everyone likes free stuff! Receive an annual allocation of SubQuery swag thatâll make you stand out in the crowd. Plus additional allocation that you can share around at community events. Youâll also receive an exclusive NFT for your Ambassador status.\n\n\n# How does it work\n\nOur Ambassador program has multiple tiers, each tier has different benefits and capabilities. You can move up tiers by participating in Ambassador activities and working hard for us.\n\nOnce you have sent through an application, we will select candidates that align with our values. If selected you are placed in our trainee program and will receive an information package, expanding your understanding of SubQuery. After this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a SubQuery Project). We will host workshops throughout this process to support you.\n\nOnce you pass the trainee program, you can call yourself a SubQuery ambassador and will be accepted into our full program. From here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\nApply Now!\n\n\n# Ambassador Activities\n\nSubQuery Ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. You can participate in as many areas as you want, youâre not bound to any single one.\n\nEvent Management: Build local communities by hosting, organising, and managing different events. Building a local community will be a key part of growing the SubQuery community. SubQuery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending Q&As or online events as speakers or in AMA sessions.\n\nContent Creation: We have a long list of content and support material that we need help creating. Remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. Content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the SubQuery Ecosystem. SubQuery will support Content Creators by providing branding assets and expertise. Weâll also use SubQueryâs marketing channels to increase awareness of your content (and yourself).\n\nTranslation: Our customers donât just speak English! We need your help making SubQuery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\nCommunity Moderation: Moderators will help grow the SubQuery community by ensuring that official community channels are active and engaging. SubQuery will support Moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\nApply Now!",normalizedContent:"# ambassador program\n\n\n\nwe understand that one of our biggest strengths is our community, and with your help, we want to grow and establish local ambassadors for communities around the world.\n\napply now!\n\n\n# what we believe in\n\nour team came together with the shared vision to build the foundations of a flexible and inclusive data service for the polkadot ecosystem.\n\nbuilt by developers, for developers: subquery is a growing community that focuses on providing the best products and services for our developers, and builders in our ecosystem. subquery is only successful if the polkadot ecosystem is successful, and so everything we do is with our customers in mind.\n\nintegrity and accountability: we have team members in auckland, shanghai, and sydney so remote work is important to us. we expect that our team is empowered and works autonomously together to achieve our goals. a key requirement for this is for our team to be accountable for their actions and maintain their integrity.\n\ninclusive guidance and support: blockchain is hard, and everyone needs help sometimes. there is no such thing as a stupid question in our community and everyone in our team is expected to help support our users. we learn some of the most valuable insights about our service (and how we can improve it) directly from our community.\n\n\n# our ambassador program\n\nour subquery ambassador program aims to find community leaders passionate about polkadot and subquery. weâre looking for self-starters that can spread the word about subquery in their local areas and provide support to new developers that want to use subquery to build amazing apps and services on polkadot.\n\n\n# ambassador benefits\n\nat subquery, we work hard to achieve what we do. similarly, ambassadors are expected to commit some time when joining our team but will be rewarded with benefits.\n\nfunding and support: you may be rewarded for good work with early opportunities into private sales and bounties. additionally, weâll be providing funding grants for you to run community meetups.\n\nsubquery team access: youâll have direct access to the core subquery team with opportunities for hands-on training, exclusive amas with our leaders and developers, and insight into our roadmap.\n\nnetwork development: expect to grow your professional network by being an ambassador for one of the top polkadot projects. meet other ambassadors around the world and receive introductions to local polkadot projects that we need to support locally. you might even get free entry to represent subquery in events in your local area.\n\nswag and other free stuff: everyone likes free stuff! receive an annual allocation of subquery swag thatâll make you stand out in the crowd. plus additional allocation that you can share around at community events. youâll also receive an exclusive nft for your ambassador status.\n\n\n# how does it work\n\nour ambassador program has multiple tiers, each tier has different benefits and capabilities. you can move up tiers by participating in ambassador activities and working hard for us.\n\nonce you have sent through an application, we will select candidates that align with our values. if selected you are placed in our trainee program and will receive an information package, expanding your understanding of subquery. after this, you can start to work through the trainee program by completing certain onboarding tasks (e.g. creating a subquery project). we will host workshops throughout this process to support you.\n\nonce you pass the trainee program, you can call yourself a subquery ambassador and will be accepted into our full program. from here on you can continue to work through the program and progress up the tiers, earning more rewards and benefits as you climb the ranks.\n\napply now!\n\n\n# ambassador activities\n\nsubquery ambassadors are able to contribute through four main areas, including event management, content creation, translation, and community moderation. you can participate in as many areas as you want, youâre not bound to any single one.\n\nevent management: build local communities by hosting, organising, and managing different events. building a local community will be a key part of growing the subquery community. subquery will support you by providing funding for events, sending swag/merchandise to be given away, as well as attending q&as or online events as speakers or in ama sessions.\n\ncontent creation: we have a long list of content and support material that we need help creating. remember, our success relies on the ability of our customers to build amazing things on our service, so we need your help to make that easier. content includes videos, infographics, tutorials, animations, or any other related material, to inform, educate, or inspire community members within the subquery ecosystem. subquery will support content creators by providing branding assets and expertise. weâll also use subqueryâs marketing channels to increase awareness of your content (and yourself).\n\ntranslation: our customers donât just speak english! we need your help making subquery more accessible by translating our content into your own language, as well as helping sharing the word to our international community.\n\ncommunity moderation: moderators will help grow the subquery community by ensuring that official community channels are active and engaging. subquery will support moderators by promoting the channels that they monitor, as well as provide guidelines for our expectations.\n\napply now!",charsets:{}},{title:"Branding Materials",frontmatter:{},regularPath:"/zh/miscellaneous/branding.html",relativePath:"zh/miscellaneous/branding.md",key:"v-723b4366",path:"/zh/miscellaneous/branding.html",headers:[{level:2,title:"Exportable Figma File",slug:"exportable-figma-file",normalizedTitle:"exportable figma file",charIndex:319},{level:2,title:"Brand Assets Package",slug:"brand-assets-package",normalizedTitle:"brand assets package",charIndex:486}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Exportable Figma File Brand Assets Package",content:"# Branding Materials\n\nAll of SubQueryâs brand features are proprietary and we take our brand extremely seriously.\n\nIf you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nIf in doubt, please ask!\n\n\n# Exportable Figma File\n\nOur Figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nFigma - SubQuery Brand Resources\n\n\n# Brand Assets Package\n\nA smaller ZIP package of brand assets\n\npublic_branding.zip",normalizedContent:"# branding materials\n\nall of subqueryâs brand features are proprietary and we take our brand extremely seriously.\n\nif you opt to use any trademarks, logos, designs, or other brand features, please carefully follow the guidelines here or reach out to us via social media for clarification.\n\nif in doubt, please ask!\n\n\n# exportable figma file\n\nour figma file has a full collection of all brand assets (logos, fonts, colours, imagery etc) for export.\n\nfigma - subquery brand resources\n\n\n# brand assets package\n\na smaller zip package of brand assets\n\npublic_branding.zip",charsets:{}},{title:"Contributing To SubQuery",frontmatter:{},regularPath:"/zh/miscellaneous/contributing.html",relativePath:"zh/miscellaneous/contributing.md",key:"v-0501aa26",path:"/zh/miscellaneous/contributing.html",headers:[{level:2,title:"Code of Conduct",slug:"code-of-conduct",normalizedTitle:"code of conduct",charIndex:873},{level:2,title:"Getting started",slug:"getting-started",normalizedTitle:"getting started",charIndex:1136},{level:2,title:"How to Contribute",slug:"how-to-contribute",normalizedTitle:"how to contribute",charIndex:1619},{level:3,title:"Reporting Bugs",slug:"reporting-bugs",normalizedTitle:"reporting bugs",charIndex:1641},{level:3,title:"Submitting Pull Requests",slug:"submitting-pull-requests",normalizedTitle:"submitting pull requests",charIndex:2108},{level:2,title:"Coding Conventions",slug:"coding-conventions",normalizedTitle:"coding conventions",charIndex:2510},{level:3,title:"Git Commit Messages",slug:"git-commit-messages",normalizedTitle:"git commit messages",charIndex:2533},{level:3,title:"JavaScript Styleguide",slug:"javascript-styleguide",normalizedTitle:"javascript styleguide",charIndex:2742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Code of Conduct Getting started How to Contribute Reporting Bugs Submitting Pull Requests Coding Conventions Git Commit Messages JavaScript Styleguide",content:'# Contributing To SubQuery\n\nWelcome and a big thank you for considering contributing to this SubQuery project! Together we can pave the way to a more decentralised future.\n\n> This documentation is actively maintained by the SubQuery team. We welcome your contributions, you can do so by forking our GitHub project and making changes to all the documentation markdown files under the docs directory.\n\nWhat follows is a set of guidelines (not rules) for contributing to SubQuery. Following these guidelines will help us make the contribution process easy and effective for everyone involved. It also communicates that you agree to respect the time of the developers managing and developing this project. In return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# Code of Conduct\n\nWe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. By participating and contributing to this project, you agree to uphold our Code of Conduct.\n\n\n# Getting started\n\nContributions to our repositories are made through Issues and Pull Requests (PRs). A few general guidelines that cover both:\n\n * Search for existing Issues and PRs before creating your own.\n * We work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. A friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# How to Contribute\n\n\n# Reporting Bugs\n\nBugs are tracked as GitHub issues. When logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * Use a clear and descriptive title for the issue to identify the problem.\n * Describe the exact steps to reproduce the problem.\n * Describe the behavior you observed after following the steps.\n * Explain which behavior you expected to see instead and why.\n * Include screenshots if possible.\n\n\n# Submitting Pull Requests\n\nIn general, we follow the "fork-and-pull" Git workflow\n\n * Fork the repository to your own Github account\n * Clone the project to your machine\n * Create a branch locally with a succinct but descriptive name\n * Commit changes to the branch\n * Following any formatting and testing guidelines specific to this repo\n * Push changes to your fork\n * Open a PR in our repository\n\n\n# Coding Conventions\n\n\n# Git Commit Messages\n\n * Use the present tense ("Add feature" not "Added feature")\n * Use the imperative mood ("Move cursor to..." not "Moves cursor to...")\n * Limit the first line to 72 characters or less\n\n\n# JavaScript Styleguide\n\n * All JavaScript code is linted with Prettier and ESLint',normalizedContent:'# contributing to subquery\n\nwelcome and a big thank you for considering contributing to this subquery project! together we can pave the way to a more decentralised future.\n\n> this documentation is actively maintained by the subquery team. we welcome your contributions, you can do so by forking our github project and making changes to all the documentation markdown files under the docs directory.\n\nwhat follows is a set of guidelines (not rules) for contributing to subquery. following these guidelines will help us make the contribution process easy and effective for everyone involved. it also communicates that you agree to respect the time of the developers managing and developing this project. in return, we will reciprocate that respect by addressing your issue, considering changes, collaborating on improvements, and helping you finalise your pull requests.\n\n\n# code of conduct\n\nwe take our open source community projects and responsibility seriously and hold ourselves and other contributors to high standards of communication. by participating and contributing to this project, you agree to uphold our code of conduct.\n\n\n# getting started\n\ncontributions to our repositories are made through issues and pull requests (prs). a few general guidelines that cover both:\n\n * search for existing issues and prs before creating your own.\n * we work hard to makes sure issues are handled in promptly but, depending on the impact, it could take a while to investigate the root cause. a friendly @ mention in the comment thread to the submitter or a contributor can help draw attention if your issue is blocking.\n\n\n# how to contribute\n\n\n# reporting bugs\n\nbugs are tracked as github issues. when logging an issue, explain the problem and include additional details to help maintainers reproduce the problem:\n\n * use a clear and descriptive title for the issue to identify the problem.\n * describe the exact steps to reproduce the problem.\n * describe the behavior you observed after following the steps.\n * explain which behavior you expected to see instead and why.\n * include screenshots if possible.\n\n\n# submitting pull requests\n\nin general, we follow the "fork-and-pull" git workflow\n\n * fork the repository to your own github account\n * clone the project to your machine\n * create a branch locally with a succinct but descriptive name\n * commit changes to the branch\n * following any formatting and testing guidelines specific to this repo\n * push changes to your fork\n * open a pr in our repository\n\n\n# coding conventions\n\n\n# git commit messages\n\n * use the present tense ("add feature" not "added feature")\n * use the imperative mood ("move cursor to..." not "moves cursor to...")\n * limit the first line to 72 characters or less\n\n\n# javascript styleguide\n\n * all javascript code is linted with prettier and eslint',charsets:{}},{title:"Social Media Links",frontmatter:{},regularPath:"/zh/miscellaneous/social_media.html",relativePath:"zh/miscellaneous/social_media.md",key:"v-d8f0d426",path:"/zh/miscellaneous/social_media.html",headers:[{level:2,title:"Official SubQuery Communities",slug:"official-subquery-communities",normalizedTitle:"official subquery communities",charIndex:280},{level:2,title:"Unofficial SubQuery Communities",slug:"unofficial-subquery-communities",normalizedTitle:"unofficial subquery communities",charIndex:529}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Official SubQuery Communities Unofficial SubQuery Communities",content:"# Social Media Links\n\nSubQuery is an active project that maintains and communicates with our followers through many social media channels.\n\nIt is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# Official SubQuery Communities\n\n * Discord (Primary Community with dedicated technical support channels)\n * Medium (Primary announcements channel)\n * Twitter\n * WeChat\n * Telegram (Announcements channel only)\n * GitHub\n * Matrix/Riot\n * LinkedIn\n\n\n# Unofficial SubQuery Communities\n\nThese communities are not moderated by the SubQuery team, but our ambassadors may be there to provide support. Please be careful of scams as SubQuery is not responsible for what happens within them.",normalizedContent:"# social media links\n\nsubquery is an active project that maintains and communicates with our followers through many social media channels.\n\nit is our aim to always listen and engage with our loyal community so please join the conversation and send us your ideas or questions!\n\n\n# official subquery communities\n\n * discord (primary community with dedicated technical support channels)\n * medium (primary announcements channel)\n * twitter\n * wechat\n * telegram (announcements channel only)\n * github\n * matrix/riot\n * linkedin\n\n\n# unofficial subquery communities\n\nthese communities are not moderated by the subquery team, but our ambassadors may be there to provide support. please be careful of scams as subquery is not responsible for what happens within them.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/zh/publish/connect.html",relativePath:"zh/publish/connect.md",key:"v-44575ff2",path:"/zh/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/zh/publish/publish.html",relativePath:"zh/publish/publish.md",key:"v-67b53086",path:"/zh/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Deploy a New Version of your SubQuery Project",frontmatter:{},regularPath:"/zh/publish/upgrade.html",relativePath:"zh/publish/upgrade.md",key:"v-ea5c64ba",path:"/zh/publish/upgrade.html",headers:[{level:2,title:"Guidelines",slug:"guidelines",normalizedTitle:"guidelines",charIndex:52},{level:2,title:"Deploy Changes",slug:"deploy-changes",normalizedTitle:"deploy changes",charIndex:604},{level:4,title:"Upgrade to the Latest Indexer and Query Service",slug:"upgrade-to-the-latest-indexer-and-query-service",normalizedTitle:"upgrade to the latest indexer and query service",charIndex:821},{level:4,title:"Deploy New Version of your SubQuery Project",slug:"deploy-new-version-of-your-subquery-project",normalizedTitle:"deploy new version of your subquery project",charIndex:1145},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:1470}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Guidelines Deploy Changes Upgrade to the Latest Indexer and Query Service Deploy New Version of your SubQuery Project Next Steps - Connect to your Project",content:"# Deploy a New Version of your SubQuery Project\n\n\n# Guidelines\n\nAlthough you have the freedom to always upgrade and deploy new versions of your SubQuery project, please be considerate during this process if your SubQuery project is public for the world. Some key points to note:\n\n * If your upgrade is a breaking change, either create a new project (e.g. My SubQuery Project V2) or give your community plenty of warning of the change through social media channels.\n * Deploying a new SubQuery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# Deploy Changes\n\nLogin to SubQuery Projects, and find the project that you want to deploy a new version of. Under Deployment Details you'll see three dots in the top right, click on the Deploy New Version button.\n\n\n\n# Upgrade to the Latest Indexer and Query Service\n\nIf you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. This will cause only a few minutes of downtime.\n\n# Deploy New Version of your SubQuery Project\n\nFill in the Commit Hash from GitHub (copy the full commit hash) of the version of your SubQuery project codebase that you want deployed. This will cause a longer downtime depending on the time it takes to index the current chain. You can always report back here for progress.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started - read more about how to user our Explorer here.",normalizedContent:"# deploy a new version of your subquery project\n\n\n# guidelines\n\nalthough you have the freedom to always upgrade and deploy new versions of your subquery project, please be considerate during this process if your subquery project is public for the world. some key points to note:\n\n * if your upgrade is a breaking change, either create a new project (e.g. my subquery project v2) or give your community plenty of warning of the change through social media channels.\n * deploying a new subquery project version causes some downtime as the new version indexes the complete chain from the genesis block.\n\n\n# deploy changes\n\nlogin to subquery projects, and find the project that you want to deploy a new version of. under deployment details you'll see three dots in the top right, click on the deploy new version button.\n\n\n\n# upgrade to the latest indexer and query service\n\nif you just want to upgrade to the latest indexer (@subql/node) or query service (@subql/query) to take advantage of our regular performance and stability improvements, just select a newer versions of our packages and save. this will cause only a few minutes of downtime.\n\n# deploy new version of your subquery project\n\nfill in the commit hash from github (copy the full commit hash) of the version of your subquery project codebase that you want deployed. this will cause a longer downtime depending on the time it takes to index the current chain. you can always report back here for progress.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started - read more about how to user our explorer here.",charsets:{}},{title:"Learn more about GraphQL",frontmatter:{},regularPath:"/zh/query/graphql.html",relativePath:"zh/query/graphql.md",key:"v-29ed5952",path:"/zh/query/graphql.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}},{title:"Query your Project in SubQuery Explorer",frontmatter:{},regularPath:"/zh/query/query.html",relativePath:"zh/query/query.md",key:"v-37f14c59",path:"/zh/query/query.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Query your Project in SubQuery Explorer\n\nSubQuery Explorer is an online hosted service (at explorer.subquery.network) that provides access to published SubQuery projects made by contributors in our community and managed by the SubQuery team. You can publish your own SubQuery projects to our explorer by following our guide to Publish your SubQuery Project.\n\n\n\nThe SubQuery explorer makes getting started easy. Weâre hosting these SubQuery projects online and allow anyone to query each for free. These managed nodes will be monitored and run by the SubQuery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nYouâll also note that the SubQuery Explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. Additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs Polkadot data.\n\nOn the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query. In the example below we're using the Sum Rewards SubQuery to get the top 5 most rewarded accounts (in terms of staking revenue) on Polkadot that have never been slashed.\n\n\n\nLearn more about the GraphQL Query language.",normalizedContent:"# query your project in subquery explorer\n\nsubquery explorer is an online hosted service (at explorer.subquery.network) that provides access to published subquery projects made by contributors in our community and managed by the subquery team. you can publish your own subquery projects to our explorer by following our guide to publish your subquery project.\n\n\n\nthe subquery explorer makes getting started easy. weâre hosting these subquery projects online and allow anyone to query each for free. these managed nodes will be monitored and run by the subquery team at a performance level that will allow production apps to use and rely on them.\n\n\n\nyouâll also note that the subquery explorer provides a playground for discovering available data with example queries - you can test queries directly in your browser without implementing code. additionally, weâve made some small improvements to our documentation to better support developers on their journey to better query and analyse the worldâs polkadot data.\n\non the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query. in the example below we're using the sum rewards subquery to get the top 5 most rewarded accounts (in terms of staking revenue) on polkadot that have never been slashed.\n\n\n\nlearn more about the graphql query language.",charsets:{}},{title:"Hello World (SubQuery hosted)",frontmatter:{},regularPath:"/zh/quickstart/helloworld-hosted.html",relativePath:"zh/quickstart/helloworld-hosted.md",key:"v-50bff266",path:"/zh/quickstart/helloworld-hosted.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:495},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:830},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:986},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:1002},{level:2,title:"Step 1: Create your project",slug:"step-1-create-your-project",normalizedTitle:"step 1: create your project",charIndex:1058},{level:2,title:"Step 2: Create a GitHub repo",slug:"step-2-create-a-github-repo",normalizedTitle:"step 2: create a github repo",charIndex:1344},{level:2,title:"Step 3: Push to GitHub",slug:"step-3-push-to-github",normalizedTitle:"step 3: push to github",charIndex:1602},{level:2,title:"Step 4: Create your project",slug:"step-4-create-your-project",normalizedTitle:"step 4: create your project",charIndex:3278},{level:2,title:"Step 5: Deploy your project",slug:"step-5-deploy-your-project",normalizedTitle:"step 5: deploy your project",charIndex:4496},{level:2,title:"Step 6: Testing your project",slug:"step-6-testing-your-project",normalizedTitle:"step 6: testing your project",charIndex:6010},{level:2,title:"Step 7: Bonus step",slug:"step-7-bonus-step",normalizedTitle:"step 7: bonus step",charIndex:6258},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:7705}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Create your project Step 2: Create a GitHub repo Step 3: Push to GitHub Step 4: Create your project Step 5: Deploy your project Step 6: Testing your project Step 7: Bonus step Summary",content:'# Hello World (SubQuery hosted)\n\nThe aim of this quick start is to show how you can get the default starter project running in SubQuery Projects (our managed service) in a few easy steps.\n\nWe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within Docker, we\'ll take advantage of SubQuery\'s managed hosting infrastructure. In other words, we let SubQuery do all the heavy lifting, running and managing production infrastructure.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in SubQuery Projects\n * run a simple query to get the block height of the Polkadot mainnet using the playground\n * run a simple GET query to get the block height of the Polkadot mainnet using cURL\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * a GitHub account\n\n\n# Step 1: Create your project\n\nLet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlHelloWorld\nyarn install\nyarn codegen\nyarn build\n\n\nDo NOT run the docker commands though.\n\n\n# Step 2: Create a GitHub repo\n\nIn GitHub, create a new public repository. Provide a name and set your visibility to public. Here, everything is kept as the default for now.\n\n\n\nTake note of your GitHub URL, this must be public for SubQuery to access it.\n\n\n\n\n# Step 3: Push to GitHub\n\nBack in your project directory, initialise it as a git directory. Otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nThen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlHelloWorld.git\n\n\nThis basically sets your remote repository to âhttps://github.com/seandotau/subqlHelloWorld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in GitHub.\n\nNext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "First commit"\n[master (root-commit) a999d88] First commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 README.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappingHandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (14/14), 59.35 KiB | 8.48 MiB/s, done.\nTotal 14 (delta 0), reused 0 (delta 0)\nTo https://github.com/seandotau/subqlHelloWorld.git\n * [new branch]      master -> master\n\n\n\nThe push command means "please push my code TO the origin repo FROM my master local repo". Refreshing GitHub should show all the code in GitHub.\n\n\n\nNow that you have got your code into GitHub, let\'s look at how we can host it in SubQuery Projects.\n\n\n# Step 4: Create your project\n\nNavigate to https://project.subquery.network and log in with your GitHub account.\n\n\n\nThen create a new project,\n\n\n\nAnd fill in the various fields with the appropriate details.\n\n * GitHub account: If you have more than one GitHub account, select what account this project will be created under. Projects created in an GitHub organisation account are shared between members in that organisation.\n * Project Name: Give your project a name here.\n * Subtitle: Provide a subtitle for your project.\n * Description: Explain what your SubQuery project does.\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that contains your SubQuery project. The schema.graphql file must be in the root of your directory.\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\n\n\nWhen you click create, you\'ll be taken to your dashboard.\n\n\n\nThe dashboard contains lots of useful information such as the network it is using, the GitHub repository URL of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# Step 5: Deploy your project\n\nNow that you have created your project within SubQuery Projects, setting up the display behaviour, the next step is to deploy your project making it operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nYou can choose to deploy to various environments such as a production slot or a staging slot. Here we\'ll deploy to a production slot. Clicking on the "Deploy" button brings up a screen with the following fields:\n\n\n\n * Commit Hash of new Version: From GitHub select the correct commit of the SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery\'s node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery\'s query service that you want to run this SubQuery on. See @subql/query\n\nBecause we only have one commit, there is only a single option in the drop down. We\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "Deploy Update".\n\nYouâll then see your deployment in âProcessingâ status. Here, your code is getting deployed onto the SubQuery\'s managed infrastructure. Basically a server is getting spun up on demand and being provisioned for you. This will take a few minutes so time to grab a coffee!\n\n\n\nThe deployment is now running.\n\n\n\n\n# Step 6: Testing your project\n\nTo test your project, click on the 3 ellipsis and select "View on SubQuery Explorer".\n\n\n\nThis will take you to the ever familiar "Playground" where you can click the play button and see the results of the query.\n\n\n\n\n# Step 7: Bonus step\n\nFor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple GET query. To do this, we will need to grab the "Query Endpoint" displayed in the deployment details.\n\n\n\nYou can then send a GET request to this endpoint either using your favourite client such as Postman or Mockoon or via cURL in your terminal. For simplicity, cURL will be shown below.\n\nThe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterEntities (first: 5, orderBy: CREATED_AT_DESC) { totalCount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterEntities":{"totalCount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nReadability is not a concern here as you will probably have some front end code to consume and parse this JSON response.\n\n\n# Summary\n\nIn this SubQuery hosted quick start we showed how quick and easy it was to take a Subql project and deploy it to SubQuery Projects where all the infrastructure is provided for your convenience. There is an inbuilt playground for running various queries as well as an API endpoint for your code to integrate with.',normalizedContent:'# hello world (subquery hosted)\n\nthe aim of this quick start is to show how you can get the default starter project running in subquery projects (our managed service) in a few easy steps.\n\nwe will take the simple starter project (and everything we\'ve learned thus far) but instead of running it locally within docker, we\'ll take advantage of subquery\'s managed hosting infrastructure. in other words, we let subquery do all the heavy lifting, running and managing production infrastructure.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * be able host a project in subquery projects\n * run a simple query to get the block height of the polkadot mainnet using the playground\n * run a simple get query to get the block height of the polkadot mainnet using curl\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * a github account\n\n\n# step 1: create your project\n\nlet\'s create a project called subql_hellowworld and run the obligatory install, codegen and build with your favourite package manager.\n\n> subql init --starter subqlhelloworld\nyarn install\nyarn codegen\nyarn build\n\n\ndo not run the docker commands though.\n\n\n# step 2: create a github repo\n\nin github, create a new public repository. provide a name and set your visibility to public. here, everything is kept as the default for now.\n\n\n\ntake note of your github url, this must be public for subquery to access it.\n\n\n\n\n# step 3: push to github\n\nback in your project directory, initialise it as a git directory. otherwise, you might get the error "fatal: not a git repository (or any of the parent directories): .git"\n\ngit init\n\n\nthen add a remote repository with the command:\n\ngit remote add origin https://github.com/seandotau/subqlhelloworld.git\n\n\nthis basically sets your remote repository to âhttps://github.com/seandotau/subqlhelloworld.gitâ and gives it the name âoriginâ which is the standard nomenclature for a remote repository in github.\n\nnext we add the code to our repo with the following commands:\n\n> git add .\n> git commit -m "first commit"\n[master (root-commit) a999d88] first commit\n10 files changed, 3512 insertions(+)\ncreate mode 100644 .gitignore\ncreate mode 100644 readme.md\ncreate mode 100644 docker-compose.yml\ncreate mode 100644 package.json\ncreate mode 100644 project.yaml\ncreate mode 100644 schema.graphql\ncreate mode 100644 src/index.ts\ncreate mode 100644 src/mappings/mappinghandlers.ts\ncreate mode 100644 tsconfig.json\ncreate mode 100644 yarn.lock\n> git push origin master\nenumerating objects: 14, done.\ncounting objects: 100% (14/14), done.\ndelta compression using up to 12 threads\ncompressing objects: 100% (13/13), done.\nwriting objects: 100% (14/14), 59.35 kib | 8.48 mib/s, done.\ntotal 14 (delta 0), reused 0 (delta 0)\nto https://github.com/seandotau/subqlhelloworld.git\n * [new branch]      master -> master\n\n\n\nthe push command means "please push my code to the origin repo from my master local repo". refreshing github should show all the code in github.\n\n\n\nnow that you have got your code into github, let\'s look at how we can host it in subquery projects.\n\n\n# step 4: create your project\n\nnavigate to https://project.subquery.network and log in with your github account.\n\n\n\nthen create a new project,\n\n\n\nand fill in the various fields with the appropriate details.\n\n * github account: if you have more than one github account, select what account this project will be created under. projects created in an github organisation account are shared between members in that organisation.\n * project name: give your project a name here.\n * subtitle: provide a subtitle for your project.\n * description: explain what your subquery project does.\n * github repository url: this must be a valid github url to a public repository that contains your subquery project. the schema.graphql file must be in the root of your directory.\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\n\n\nwhen you click create, you\'ll be taken to your dashboard.\n\n\n\nthe dashboard contains lots of useful information such as the network it is using, the github repository url of the source code it is running, when it was created and last updated, and in particular the deployment details.\n\n\n# step 5: deploy your project\n\nnow that you have created your project within subquery projects, setting up the display behaviour, the next step is to deploy your project making it operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nyou can choose to deploy to various environments such as a production slot or a staging slot. here we\'ll deploy to a production slot. clicking on the "deploy" button brings up a screen with the following fields:\n\n\n\n * commit hash of new version: from github select the correct commit of the subquery project codebase that you want deployed\n * indexer version: this is the version of subquery\'s node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery\'s query service that you want to run this subquery on. see @subql/query\n\nbecause we only have one commit, there is only a single option in the drop down. we\'ll also work with the latest version of the indexer and query version so we will accept the defaults and then click "deploy update".\n\nyouâll then see your deployment in âprocessingâ status. here, your code is getting deployed onto the subquery\'s managed infrastructure. basically a server is getting spun up on demand and being provisioned for you. this will take a few minutes so time to grab a coffee!\n\n\n\nthe deployment is now running.\n\n\n\n\n# step 6: testing your project\n\nto test your project, click on the 3 ellipsis and select "view on subquery explorer".\n\n\n\nthis will take you to the ever familiar "playground" where you can click the play button and see the results of the query.\n\n\n\n\n# step 7: bonus step\n\nfor the astute amongst us, you will recall that in the learning objectives, the last point was to run a simple get query. to do this, we will need to grab the "query endpoint" displayed in the deployment details.\n\n\n\nyou can then send a get request to this endpoint either using your favourite client such as postman or mockoon or via curl in your terminal. for simplicity, curl will be shown below.\n\nthe curl command to run is:\n\ncurl https://api.subquery.network/sq/seandotau/subqueryhelloworld -d "query=query { starterentities (first: 5, orderby: created_at_desc) { totalcount nodes { id field1 field2 field3 } } }"\n\n\ngiving the results of:\n\n{"data":{"starterentities":{"totalcount":23098,"nodes":[{"id":"0x29dfe9c8e5a1d51178565c2c23f65d249b548fe75a9b6d74cebab777b961b1a6","field1":23098,"field2":null,"field3":null},{"id":"0xab7d3e0316a01cdaf9eda420cf4021dd53bb604c29c5136fef17088c8d9233fb","field1":23097,"field2":null,"field3":null},{"id":"0x534e89bbae0857f2f07b0dea8dc42a933f9eb2d95f7464bf361d766a644d17e3","field1":23096,"field2":null,"field3":null},{"id":"0xd0af03ab2000a58b40abfb96a61d312a494069de3670b509454bd06157357db6","field1":23095,"field2":null,"field3":null},{"id":"0xc9f5a92f4684eb039e11dffa4b8b22c428272b2aa09aff291169f71c1ba0b0f7","field1":23094,"field2":null,"field3":null}]}}}\n\n\n\nreadability is not a concern here as you will probably have some front end code to consume and parse this json response.\n\n\n# summary\n\nin this subquery hosted quick start we showed how quick and easy it was to take a subql project and deploy it to subquery projects where all the infrastructure is provided for your convenience. there is an inbuilt playground for running various queries as well as an api endpoint for your code to integrate with.',charsets:{cjk:!0}},{title:"Hello World (localhost + Docker)",frontmatter:{},regularPath:"/zh/quickstart/helloworld-localhost.html",relativePath:"zh/quickstart/helloworld-localhost.md",key:"v-9101a14a",path:"/zh/quickstart/helloworld-localhost.html",headers:[{level:2,title:"Learning objectives",slug:"learning-objectives",normalizedTitle:"learning objectives",charIndex:204},{level:2,title:"Intended audience",slug:"intended-audience",normalizedTitle:"intended audience",charIndex:491},{level:2,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:647},{level:2,title:"Pre-requisites",slug:"pre-requisites",normalizedTitle:"pre-requisites",charIndex:663},{level:2,title:"Step 1: Initialise project",slug:"step-1-initialise-project",normalizedTitle:"step 1: initialise project",charIndex:1435},{level:2,title:"Step 2: Install dependencies",slug:"step-2-install-dependencies",normalizedTitle:"step 2: install dependencies",charIndex:1991},{level:2,title:"Step 3: Generate code",slug:"step-3-generate-code",normalizedTitle:"step 3: generate code",charIndex:2401},{level:2,title:"Step 4: Build code",slug:"step-4-build-code",normalizedTitle:"step 4: build code",charIndex:2976},{level:2,title:"Step 5: Run Docker",slug:"step-5-run-docker",normalizedTitle:"step 5: run docker",charIndex:3186},{level:2,title:"Step 6: Browse playground",slug:"step-6-browse-playground",normalizedTitle:"step 6: browse playground",charIndex:4421},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:4839}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Learning objectives Intended audience Video guide Pre-requisites Step 1: Initialise project Step 2: Install dependencies Step 3: Generate code Step 4: Build code Step 5: Run Docker Step 6: Browse playground Summary",content:'# Hello World (localhost + Docker)\n\nWelcome to this SubQuery Hello World quick start. The quick start aims to show you how you get the default starter project running in Docker in a few simple steps.\n\n\n# Learning objectives\n\nAt the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the Polkadot mainnet\n\n\n# Intended audience\n\nThis guide is geared towards new developers who have some development experience and are interested in learning more about SubQuery.\n\n\n# Video guide\n\n\n# Pre-requisites\n\nYou will need:\n\n * yarn or npm package manager\n * SubQuery CLI (@subql/cli)\n * Docker\n\nYou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nFor more advanced users, copy and paste the following:\n\necho -e "My yarn version is:" `yarn -v` "\\nMy subql version is:" `subql -v`  "\\nMy docker version is:" `docker -v`\n\n\nThis should return: (for npm users, replace yarn with npm)\n\nMy yarn version is: 1.22.10\nMy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nMy docker version is: Docker version 20.10.5, build 55c4c88\n\n\nIf you get the above, then you are good to go. If not, follow these links to install them:\n\n * yarn or npm\n * SubQuery CLI\n * Docker\n\n\n# Step 1: Initialise project\n\nThe first step when starting off with SubQuery is to run the subql init command. Let\'s initialise a start project with the name subqlHelloWorld. Note that only author is mandatory. Everything else is left empty below.\n\n> subql init --starter subqlHelloWorld\nGit repository:\nRPC endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nAuthors: sa\nDescription:\nVersion: [1.0.0]:\nLicense: [Apache-2.0]:\nInit the starter package... subqlHelloWorld is ready\n\n\n\nDon\'t forget to change into this new directory.\n\ncd subqlHelloWorld\n\n\n\n# Step 2: Install dependencies\n\nNow do a yarn or node install to install the various dependencies.\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\nAn example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo No lockfile found.\n[1/4] ð  Resolving packages...\n[2/4] ð  Fetching packages...\n[3/4] ð  Linking dependencies...\n[4/4] ð¨  Building fresh packages...\nsuccess Saved lockfile.\nâ¨  Done in 31.84s.\n\n\n\n# Step 3: Generate code\n\nNow run yarn codegen to generate Typescript from the GraphQL schema.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nAn example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------Subql Codegen---------\n===============================\n* Schema StarterEntity generated !\n* Models index generated !\n* Types index generated !\nâ¨  Done in 1.02s.\n\n\nWarning When changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# Step 4: Build code\n\nThe next step is to build the code with yarn build.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\nAn example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  Done in 5.68s.\n\n\n\n# Step 5: Run Docker\n\nUsing Docker allows you to run this example very quickly because all the required infrastructure can be provided within the Docker image. Run docker-compose pull && docker-compose up.\n\nThis will kick everything into life where eventually you will get blocks being fetched.\n\n> #SNIPPET\nsubquery-node_1   | 2021-06-05T22:20:31.450Z <subql-node> INFO node started\nsubquery-node_1   | 2021-06-05T22:20:35.134Z <fetch> INFO fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05T22:20:38.412Z <fetch> INFO fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05T22:20:39.353Z <nestjs> INFO Starting Nest application...\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO AppModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.382Z <nestjs> INFO ConfigureModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.383Z <nestjs> INFO GraphqlModule dependencies initialized\ngraphql-engine_1  | 2021-06-05T22:20:39.809Z <nestjs> INFO Nest application successfully started\nsubquery-node_1   | 2021-06-05T22:20:41.122Z <fetch> INFO fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05T22:20:43.244Z <express> INFO request completed\n\n\n\n\n# Step 6: Browse playground\n\nNavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterEntities(last:10, orderBy:FIELD1_ASC ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nSubQuery playground on localhost.\n\n\n\nThe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# Summary\n\nIn this quick start, we demonstrated the basic steps to get a starter project up and running within a Docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet Polkadot network.',normalizedContent:'# hello world (localhost + docker)\n\nwelcome to this subquery hello world quick start. the quick start aims to show you how you get the default starter project running in docker in a few simple steps.\n\n\n# learning objectives\n\nat the end of this quick start, you should:\n\n * understand the required pre-requisites\n * understand the basic common commands\n * be able to navigate to localhost:3000 and view the playground\n * run a simple query to get the block height of the polkadot mainnet\n\n\n# intended audience\n\nthis guide is geared towards new developers who have some development experience and are interested in learning more about subquery.\n\n\n# video guide\n\n\n# pre-requisites\n\nyou will need:\n\n * yarn or npm package manager\n * subquery cli (@subql/cli)\n * docker\n\nyou can run the following commands in a terminal to see if you already have any of these pre-requisites.\n\nyarn -v (or npm -v)\nsubql -v\ndocker -v\n\n\nfor more advanced users, copy and paste the following:\n\necho -e "my yarn version is:" `yarn -v` "\\nmy subql version is:" `subql -v`  "\\nmy docker version is:" `docker -v`\n\n\nthis should return: (for npm users, replace yarn with npm)\n\nmy yarn version is: 1.22.10\nmy subql version is: @subql/cli/0.9.3 darwin-x64 node-v16.3.0\nmy docker version is: docker version 20.10.5, build 55c4c88\n\n\nif you get the above, then you are good to go. if not, follow these links to install them:\n\n * yarn or npm\n * subquery cli\n * docker\n\n\n# step 1: initialise project\n\nthe first step when starting off with subquery is to run the subql init command. let\'s initialise a start project with the name subqlhelloworld. note that only author is mandatory. everything else is left empty below.\n\n> subql init --starter subqlhelloworld\ngit repository:\nrpc endpoint [wss://polkadot.api.onfinality.io/public-ws]:\nauthors: sa\ndescription:\nversion: [1.0.0]:\nlicense: [apache-2.0]:\ninit the starter package... subqlhelloworld is ready\n\n\n\ndon\'t forget to change into this new directory.\n\ncd subqlhelloworld\n\n\n\n# step 2: install dependencies\n\nnow do a yarn or node install to install the various dependencies.\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\nan example of yarn install\n\n> yarn install\nyarn install v1.22.10\ninfo no lockfile found.\n[1/4] ð  resolving packages...\n[2/4] ð  fetching packages...\n[3/4] ð  linking dependencies...\n[4/4] ð¨  building fresh packages...\nsuccess saved lockfile.\nâ¨  done in 31.84s.\n\n\n\n# step 3: generate code\n\nnow run yarn codegen to generate typescript from the graphql schema.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nan example of yarn codegen\n\n> yarn codegen\nyarn run v1.22.10\n$ ./node_modules/.bin/subql codegen\n===============================\n---------subql codegen---------\n===============================\n* schema starterentity generated !\n* models index generated !\n* types index generated !\nâ¨  done in 1.02s.\n\n\nwarning when changes are made to the schema file, please remember to re-run yarn codegen to regenerate your types directory.\n\n\n# step 4: build code\n\nthe next step is to build the code with yarn build.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\nan example of yarn build\n\n> yarn build\nyarn run v1.22.10\n$ tsc -b\nâ¨  done in 5.68s.\n\n\n\n# step 5: run docker\n\nusing docker allows you to run this example very quickly because all the required infrastructure can be provided within the docker image. run docker-compose pull && docker-compose up.\n\nthis will kick everything into life where eventually you will get blocks being fetched.\n\n> #snippet\nsubquery-node_1   | 2021-06-05t22:20:31.450z <subql-node> info node started\nsubquery-node_1   | 2021-06-05t22:20:35.134z <fetch> info fetch block [1, 100]\nsubqlhelloworld_graphql-engine_1 exited with code 0\nsubquery-node_1   | 2021-06-05t22:20:38.412z <fetch> info fetch block [101, 200]\ngraphql-engine_1  | 2021-06-05t22:20:39.353z <nestjs> info starting nest application...\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info appmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.382z <nestjs> info configuremodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.383z <nestjs> info graphqlmodule dependencies initialized\ngraphql-engine_1  | 2021-06-05t22:20:39.809z <nestjs> info nest application successfully started\nsubquery-node_1   | 2021-06-05t22:20:41.122z <fetch> info fetch block [201, 300]\ngraphql-engine_1  | 2021-06-05t22:20:43.244z <express> info request completed\n\n\n\n\n# step 6: browse playground\n\nnavigate to http://localhost:3000/ and paste the query below into the left side of the screen and then hit the play button.\n\n{\n query{\n   starterentities(last:10, orderby:field1_asc ){\n     nodes{\n       field1\n     }\n   }\n }\n}\n\n\n\nsubquery playground on localhost.\n\n\n\nthe block count in the playground should match the block count (technically the block height) in the terminal as well.\n\n\n# summary\n\nin this quick start, we demonstrated the basic steps to get a starter project up and running within a docker environment and then navigated to localhost:3000 and ran a query to return the block number of the mainnet polkadot network.',charsets:{cjk:!0}},{title:"Quick Start Guide",frontmatter:{},regularPath:"/zh/quickstart/quickstart.html",relativePath:"zh/quickstart/quickstart.md",key:"v-2c2604bd",path:"/zh/quickstart/quickstart.html",headers:[{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:429},{level:3,title:"Local Development Environment",slug:"local-development-environment",normalizedTitle:"local development environment",charIndex:445},{level:3,title:"Install the SubQuery CLI",slug:"install-the-subquery-cli",normalizedTitle:"install the subquery cli",charIndex:672},{level:2,title:"Initialise the Starter SubQuery Project",slug:"initialise-the-starter-subquery-project",normalizedTitle:"initialise the starter subquery project",charIndex:1026},{level:2,title:"Configure and Build the Starter Project",slug:"configure-and-build-the-starter-project",normalizedTitle:"configure and build the starter project",charIndex:2536},{level:3,title:"GraphQL Model Generation",slug:"graphql-model-generation",normalizedTitle:"graphql model generation",charIndex:2979},{level:2,title:"Build the Project",slug:"build-the-project",normalizedTitle:"build the project",charIndex:3346},{level:2,title:"Running and Querying your Starter Project",slug:"running-and-querying-your-starter-project",normalizedTitle:"running and querying your starter project",charIndex:3574},{level:3,title:"Run your SubQuery Project",slug:"run-your-subquery-project",normalizedTitle:"run your subquery project",charIndex:3927},{level:3,title:"Query your Project",slug:"query-your-project",normalizedTitle:"query your project",charIndex:4490},{level:2,title:"Next Steps",slug:"next-steps",normalizedTitle:"next steps",charIndex:5151}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Preparation Local Development Environment Install the SubQuery CLI Initialise the Starter SubQuery Project Configure and Build the Starter Project GraphQL Model Generation Build the Project Running and Querying your Starter Project Run your SubQuery Project Query your Project Next Steps",content:"# Quick Start Guide\n\nIn this Quick Start guide, we're going to create a simple starter project that you can be used as a framework for developing your own SubQuery Project.\n\nAt the end of this guide, you'll have a working SubQuery project running on a SubQuery node with a GraphQL endpoint that you can query data from.\n\nIf you haven't already, we suggest that you familiarise yourself with the terminology used in SubQuery.\n\n\n# Preparation\n\n\n# Local Development Environment\n\n * Typescript is required to compile project and define types.\n * Both SubQuery CLI and generated Project have dependencies and require a modern version Node.\n * SubQuery Nodes require Docker\n\n\n# Install the SubQuery CLI\n\nInstall SubQuery CLI globally on your terminal by using NPM:\n\n# NPM\nnpm install -g @subql/cli\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nYou can then run help to see available commands and usage provide by CLI\n\nsubql help\n\n\n\n# Initialise the Starter SubQuery Project\n\nInside the directory in which you want to create a SubQuery project, simply replace PROJECT_NAME with your own and run the command:\n\nsubql init --starter PROJECT_NAME\n\n\nYou'll be asked certain questions as the SubQuery project is initalised:\n\n * Git repository (Optional): Provide a Git URL to a repo that this SubQuery project will be hosted in (when hosted in SubQuery Explorer)\n * RPC endpoint (Required): Provide a wss URL to a running RPC endpoint that will be used by default for this project. You can quickly access public endpoints for different Polkadot networks or even create your own private dedicated node using OnFinality or just use the default Polkadot endpoint.\n * Authors (Required): Enter the owner of this SubQuery project here\n * Description (Optional): You can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * Version (Required): Enter a custom version number or use the default (1.0.0)\n * License (Required): Provide the software license for this project or accept the default (Apache-2.0)\n\nAfter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. The contents of this directoy should be identical to what's listed in the Directory Structure.\n\nLast, under the project directory, run following command to install the new project's dependencies.\n\ncd PROJECT_NAME\n\n# Yarn\nyarn install\n\n# NPM\nnpm install\n\n\n\n# Configure and Build the Starter Project\n\nIn the starter package that you just initialised, we have provided a standard configuration for your new project. You will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\nFor more information on how to write your own SubQuery, check out our documentation under Create a Project\n\n\n# GraphQL Model Generation\n\nIn order to index your SubQuery project, you must first generate the required GraphQL models that you have defined in your GraphQL Schema file (schema.graphql). Run this command in the root of the project directory.\n\n# Yarn\nyarn codegen\n\n# NPM\nnpm run-script codegen\n\n\nYou'll find the generated models in the /src/types/models directory\n\n\n# Build the Project\n\nIn order run your SubQuery Project on a locally hosted SubQuery Node, you need to build your work.\n\nRun the build command from the project's root directory.\n\n# Yarn\nyarn build\n\n# NPM\nnpm run-script build\n\n\n\n# Running and Querying your Starter Project\n\nAlthough you can quickly publish your new project to SubQuery Projects and query it using our Explorer, the easiest way to run SubQuery nodes locally is in a Docker container, if you don't already have Docker you can install it from docker.com.\n\nSkip this and publish your new project to SubQuery Projects\n\n\n# Run your SubQuery Project\n\nAll configuration that controls how a SubQuery node is run is defined in this docker-compose.yml file. For a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our Run a Project section\n\nUnder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Query your Project\n\nOpen your browser and head to http://localhost:3000.\n\nYou should see a GraphQL playground is showing in the explorer and the schemas that are ready to query. On the top right of the playground, you'll find a Docs button that will open a documentation draw. This documentation is automatically generated and helps you find what entities and methods you can query.\n\nFor a new SubQuery starter project, you can try the following query to get a taste of how it works or learn more about the GraphQL Query language.\n\n{\n  query {\n    starterEntities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# Next Steps\n\nCongratulations, you now have a locally running SubQuery project that accepts GraphQL API requests for sample data. In the next guide, we'll show you how to publish your new project to SubQuery Projects and query it using our Explorer\n\nPublish your new project to SubQuery Projects",normalizedContent:"# quick start guide\n\nin this quick start guide, we're going to create a simple starter project that you can be used as a framework for developing your own subquery project.\n\nat the end of this guide, you'll have a working subquery project running on a subquery node with a graphql endpoint that you can query data from.\n\nif you haven't already, we suggest that you familiarise yourself with the terminology used in subquery.\n\n\n# preparation\n\n\n# local development environment\n\n * typescript is required to compile project and define types.\n * both subquery cli and generated project have dependencies and require a modern version node.\n * subquery nodes require docker\n\n\n# install the subquery cli\n\ninstall subquery cli globally on your terminal by using npm:\n\n# npm\nnpm install -g @subql/cli\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nyou can then run help to see available commands and usage provide by cli\n\nsubql help\n\n\n\n# initialise the starter subquery project\n\ninside the directory in which you want to create a subquery project, simply replace project_name with your own and run the command:\n\nsubql init --starter project_name\n\n\nyou'll be asked certain questions as the subquery project is initalised:\n\n * git repository (optional): provide a git url to a repo that this subquery project will be hosted in (when hosted in subquery explorer)\n * rpc endpoint (required): provide a wss url to a running rpc endpoint that will be used by default for this project. you can quickly access public endpoints for different polkadot networks or even create your own private dedicated node using onfinality or just use the default polkadot endpoint.\n * authors (required): enter the owner of this subquery project here\n * description (optional): you can provide a short paragraph about your project that describe what data it contains and what users can do with it\n * version (required): enter a custom version number or use the default (1.0.0)\n * license (required): provide the software license for this project or accept the default (apache-2.0)\n\nafter the initialisation process is complete, you should see a folder with your project name has been created inside the directory. the contents of this directoy should be identical to what's listed in the directory structure.\n\nlast, under the project directory, run following command to install the new project's dependencies.\n\ncd project_name\n\n# yarn\nyarn install\n\n# npm\nnpm install\n\n\n\n# configure and build the starter project\n\nin the starter package that you just initialised, we have provided a standard configuration for your new project. you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\nfor more information on how to write your own subquery, check out our documentation under create a project\n\n\n# graphql model generation\n\nin order to index your subquery project, you must first generate the required graphql models that you have defined in your graphql schema file (schema.graphql). run this command in the root of the project directory.\n\n# yarn\nyarn codegen\n\n# npm\nnpm run-script codegen\n\n\nyou'll find the generated models in the /src/types/models directory\n\n\n# build the project\n\nin order run your subquery project on a locally hosted subquery node, you need to build your work.\n\nrun the build command from the project's root directory.\n\n# yarn\nyarn build\n\n# npm\nnpm run-script build\n\n\n\n# running and querying your starter project\n\nalthough you can quickly publish your new project to subquery projects and query it using our explorer, the easiest way to run subquery nodes locally is in a docker container, if you don't already have docker you can install it from docker.com.\n\nskip this and publish your new project to subquery projects\n\n\n# run your subquery project\n\nall configuration that controls how a subquery node is run is defined in this docker-compose.yml file. for a new project that has been just initalised you won't need to change anything here, but you can read more about the file and the settings in our run a project section\n\nunder the project directory run following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# query your project\n\nopen your browser and head to http://localhost:3000.\n\nyou should see a graphql playground is showing in the explorer and the schemas that are ready to query. on the top right of the playground, you'll find a docs button that will open a documentation draw. this documentation is automatically generated and helps you find what entities and methods you can query.\n\nfor a new subquery starter project, you can try the following query to get a taste of how it works or learn more about the graphql query language.\n\n{\n  query {\n    starterentities(first: 10) {\n      nodes {\n        field1\n        field2\n        field3\n      }\n    }\n  }\n}\n\n\n\n# next steps\n\ncongratulations, you now have a locally running subquery project that accepts graphql api requests for sample data. in the next guide, we'll show you how to publish your new project to subquery projects and query it using our explorer\n\npublish your new project to subquery projects",charsets:{}},{title:"Hello World Explained",frontmatter:{},regularPath:"/zh/quickstart/understanding-helloworld.html",relativePath:"zh/quickstart/understanding-helloworld.md",key:"v-31d49c56",path:"/zh/quickstart/understanding-helloworld.html",headers:[{level:2,title:"subql init",slug:"subql-init",normalizedTitle:"subql init",charIndex:378},{level:2,title:"yarn install",slug:"yarn-install",normalizedTitle:"yarn install",charIndex:1161},{level:2,title:"yarn codegen",slug:"yarn-codegen",normalizedTitle:"yarn codegen",charIndex:1998},{level:2,title:"yarn build",slug:"yarn-build",normalizedTitle:"yarn build",charIndex:2339},{level:2,title:"docker-compose",slug:"docker-compose",normalizedTitle:"docker-compose",charIndex:2566},{level:2,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:3233}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"subql init yarn install yarn codegen yarn build docker-compose Summary",content:"# Hello World Explained\n\nIn the Hello World quick start guide, we ran through some simple commands and very quickly got an example up and running. This allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from SubQuery. Here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nThe first command we ran was subql init --starter subqlHelloWorld.\n\nThis does the heavy lifting and creates a whole bunch of files for you. As noted in the official documentation, you will mainly be working on the following files:\n\n * The Manifest in project.yaml\n * The GraphQL Schema in schema.graphql\n * The Mapping functions in src/mappings/ directory\n\n\n\nThese files are the core of everything we do. As such, we'll dedicate more time to these files in another article. For now though, just know that the schema contains a description of the data users can request from the SubQuery API, the project yaml file which contains \"configuration\" type parameters and of course the mappingHandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nThe next thing we did was yarn install. npm install can be used as well.\n\n> A short history lesson. Node Package Manager or npm was initially released in 2010 and is a tremendously popular package manager among JavaScript developers. It is the default package that is automatically installed whenever you install Node.js on your system. Yarn was initially released by Facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nWhat yarn does is look at the package.json file and download various other dependencies. Looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. This is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nThen we ran yarn codegen or npm run-script codegen. What this does is fetch the GraphQL schema (in the schema.graphql) and generates the associated typescript model files (Hence the output files will have a .ts extension). You should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. This should be familiar for seasoned programmers. It creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nThe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). The pull command grabs all the required images from Docker Hub and the up command starts the container.\n\n> docker-compose pull\nPulling postgres        ... done\nPulling subquery-node   ... done\nPulling graphql-engine  ... done\n\n\nWhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the GraphQL engine. It's when you see:\n\nsubquery-node_1   | 2021-06-06T02:04:25.490Z <fetch> INFO fetch block [1, 100]\n\n\nthat you know that the SubQuery node has started to synchronise.\n\n\n# Summary\n\nNow that you've had an insight into what is happening under the covers, the question is where to from here? If you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. The manifest file, the GraphQL schema, and the mappings file.\n\nOtherwise, continue to our tutorials section where we look at how we can run this Hello World example on SubQuery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running SubQuery projects by running readily available and open source projects.",normalizedContent:"# hello world explained\n\nin the hello world quick start guide, we ran through some simple commands and very quickly got an example up and running. this allowed you to ensure that you had all the pre-requisites in place and could use a local playground to make a simple query to get your first data from subquery. here, we take a closer look at what all those commands mean.\n\n\n# subql init\n\nthe first command we ran was subql init --starter subqlhelloworld.\n\nthis does the heavy lifting and creates a whole bunch of files for you. as noted in the official documentation, you will mainly be working on the following files:\n\n * the manifest in project.yaml\n * the graphql schema in schema.graphql\n * the mapping functions in src/mappings/ directory\n\n\n\nthese files are the core of everything we do. as such, we'll dedicate more time to these files in another article. for now though, just know that the schema contains a description of the data users can request from the subquery api, the project yaml file which contains \"configuration\" type parameters and of course the mappinghandlers containing typescript which contains functions that transform the data.\n\n\n# yarn install\n\nthe next thing we did was yarn install. npm install can be used as well.\n\n> a short history lesson. node package manager or npm was initially released in 2010 and is a tremendously popular package manager among javascript developers. it is the default package that is automatically installed whenever you install node.js on your system. yarn was initially released by facebook in 2016 with the intention to address some of the performance and security shortcomings of working with npm (at that time).\n\nwhat yarn does is look at the package.json file and download various other dependencies. looking at the package.json file, it doesn't look like there are many dependencies, but when you run the command, you'll notice that 18,983 files are added. this is because each dependency will also have its own dependencies.\n\n\n\n\n# yarn codegen\n\nthen we ran yarn codegen or npm run-script codegen. what this does is fetch the graphql schema (in the schema.graphql) and generates the associated typescript model files (hence the output files will have a .ts extension). you should never change any of these generated files, only change the source schema.graphql file.\n\n\n\n\n# yarn build\n\nyarn build or npm run-script build was then executed. this should be familiar for seasoned programmers. it creates a distribution folder performing things such as code optimisation preparing for a deployment.\n\n\n\n\n# docker-compose\n\nthe final step was the combined docker command docker-compose pull && docker-compose up (can be run separately as well). the pull command grabs all the required images from docker hub and the up command starts the container.\n\n> docker-compose pull\npulling postgres        ... done\npulling subquery-node   ... done\npulling graphql-engine  ... done\n\n\nwhen the container is started, you'll see the terminal spit out lots of text showing the status of the node and the graphql engine. it's when you see:\n\nsubquery-node_1   | 2021-06-06t02:04:25.490z <fetch> info fetch block [1, 100]\n\n\nthat you know that the subquery node has started to synchronise.\n\n\n# summary\n\nnow that you've had an insight into what is happening under the covers, the question is where to from here? if you are feeling confident, you can jump into learning about how to create a project and learn more about the three key files. the manifest file, the graphql schema, and the mappings file.\n\notherwise, continue to our tutorials section where we look at how we can run this hello world example on subquery's hosted infrastructure, we'll look at modifying the start block, and we'll take a deeper dive at running subquery projects by running readily available and open source projects.",charsets:{cjk:!0}},{title:"Running SubQuery Locally",frontmatter:{},regularPath:"/zh/run/run.html",relativePath:"zh/run/run.md",key:"v-5f7fb3cd",path:"/zh/run/run.html",headers:[{level:2,title:"Using Docker",slug:"using-docker",normalizedTitle:"using docker",charIndex:392},{level:2,title:"Running an Indexer (subql/node)",slug:"running-an-indexer-subql-node",normalizedTitle:"running an indexer (subql/node)",charIndex:855},{level:3,title:"Installation",slug:"installation",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Key Commands",slug:"key-commands",normalizedTitle:"key commands",charIndex:1498},{level:4,title:"Point to local project path",slug:"point-to-local-project-path",normalizedTitle:"point to local project path",charIndex:1668},{level:4,title:"Using a Dictionary",slug:"using-a-dictionary",normalizedTitle:"using a dictionary",charIndex:1733},{level:4,title:"Connect to database",slug:"connect-to-database",normalizedTitle:"connect to database",charIndex:2410},{level:4,title:"Specify a configuration file",slug:"specify-a-configuration-file",normalizedTitle:"specify a configuration file",charIndex:2809},{level:4,title:"Change the block fetching batch size",slug:"change-the-block-fetching-batch-size",normalizedTitle:"change the block fetching batch size",charIndex:3106},{level:4,title:"Local mode",slug:"local-mode",normalizedTitle:"local mode",charIndex:3544},{level:2,title:"Running a Query Service (subql/query)",slug:"running-a-query-service-subql-query",normalizedTitle:"running a query service (subql/query)",charIndex:3874},{level:3,title:"Installation",slug:"installation-2",normalizedTitle:"installation",charIndex:1215},{level:3,title:"Running the Query service",slug:"running-the-query-service",normalizedTitle:"running the query service",charIndex:4111}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Using Docker Running an Indexer (subql/node) Installation Key Commands Point to local project path Using a Dictionary Connect to database Specify a configuration file Change the block fetching batch size Local mode Running a Query Service (subql/query) Installation Running the Query service",content:"# Running SubQuery Locally\n\nThis guide works through how to run a local SubQuery node on your infrastructure, which includes both the indexer and query service. Don't want to worry about running your own SubQuery infrastructure? SubQuery provides a managed hosted service to the community for free. Follow our publishing guide to see how you can upload your project to SubQuery Projects.\n\n\n# Using Docker\n\nAn alternative solution is to run a Docker Container, defined by the docker-compose.yml file. For a new project that has been just initialised you won't need to change anything here.\n\nUnder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nIt may take some time to download the required packages (@subql/node, @subql/query, and Postgres) for the first time but soon you'll see a running SubQuery node.\n\n\n# Running an Indexer (subql/node)\n\nRequirements:\n\n * Postgres database (version 12 or higher). While the SubQuery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\nA SubQuery node is an implementation that extracts substrate-based blockchain data per the SubQuery project and saves it into a Postgres database.\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/node\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nOnce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# Key Commands\n\nThe following commands will assist you to complete the configuration of a SubQuery node and begin indexing. To find out more, you can always run --help.\n\n# Point to local project path\n\nsubql-node -f your-project-path\n\n\n# Using a Dictionary\n\nUsing a full chain dictionary can dramatically speed up the processing of a SubQuery project during testing or during your first index. In some cases, we've seen indexing performance increases of up to 10x.\n\nA full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nYou can add the dictionary endpoint in your project.yaml file (see Manifest File), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# Connect to database\n\nexport DB_USER=postgres\nexport DB_PASS=postgres\nexport DB_DATABASE=postgres\nexport DB_HOST=localhost\nexport DB_PORT=5432\nsubql-node -f your-project-path \n\n\nDepending on the configuration of your Postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# Specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nThis will point the query node to a configuration file which can be in YAML or JSON format. Check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryName: extrinsics\nbatchSize:100\nlocalMode:true\n\n\n# Change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nResult:\n[IndexerManager] fetch block [203, 402]\n[IndexerManager] fetch block [403, 602]\n\n\nWhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. Increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. The current default batch size is 100.\n\n# Local mode\n\nsubql-node -f your-project-path --local\n\n\nFor debugging purposes, users can run the node in local mode. Switching to local model will create Postgres tables in the default schema public.\n\nIf local mode is not used, a new Postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# Running a Query Service (subql/query)\n\n\n# Installation\n\n# NPM\nnpm install -g @subql/query\n\n\nPlease note that we DO NOT encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# Running the Query service\n\n``` export DB_HOST=localhost subql-query --name <project_name> --playground ````\n\nMake sure the project name is the same as the project name when you initialize the project. Also, check the environment variables are correct.\n\nAfter running the subql-query service successfully, open your browser and head to http://localhost:3000. You should see a GraphQL playground showing in the Explorer and the schema that is ready to query.",normalizedContent:"# running subquery locally\n\nthis guide works through how to run a local subquery node on your infrastructure, which includes both the indexer and query service. don't want to worry about running your own subquery infrastructure? subquery provides a managed hosted service to the community for free. follow our publishing guide to see how you can upload your project to subquery projects.\n\n\n# using docker\n\nan alternative solution is to run a docker container, defined by the docker-compose.yml file. for a new project that has been just initialised you won't need to change anything here.\n\nunder the project directory run the following command:\n\ndocker-compose pull && docker-compose up\n\n\nit may take some time to download the required packages (@subql/node, @subql/query, and postgres) for the first time but soon you'll see a running subquery node.\n\n\n# running an indexer (subql/node)\n\nrequirements:\n\n * postgres database (version 12 or higher). while the subquery node is indexing the blockchain, the extracted data is stored in an external database instance.\n\na subquery node is an implementation that extracts substrate-based blockchain data per the subquery project and saves it into a postgres database.\n\n\n# installation\n\n# npm\nnpm install -g @subql/node\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\nonce installed, you can start a node with the following command:\n\nsubql-node <command>\n\n\n\n# key commands\n\nthe following commands will assist you to complete the configuration of a subquery node and begin indexing. to find out more, you can always run --help.\n\n# point to local project path\n\nsubql-node -f your-project-path\n\n\n# using a dictionary\n\nusing a full chain dictionary can dramatically speed up the processing of a subquery project during testing or during your first index. in some cases, we've seen indexing performance increases of up to 10x.\n\na full chain dictionary pre-indexes the location of all events and extrinsics within the specific chain and allows your node service to skip to relevant locations when indexing rather than inspecting each block.\n\nyou can add the dictionary endpoint in your project.yaml file (see manifest file), or specify it at run time using the following command:\n\nsubql-node --network-dictionary=https://api.subquery.network/sq/subquery/dictionary-polkadot\n\n\n# connect to database\n\nexport db_user=postgres\nexport db_pass=postgres\nexport db_database=postgres\nexport db_host=localhost\nexport db_port=5432\nsubql-node -f your-project-path \n\n\ndepending on the configuration of your postgres database (e.g. a different database password), please ensure also that both the indexer (subql/node) and the query service (subql/query) can establish a connection to it.\n\n# specify a configuration file\n\nsubql-node -c your-project-config.yml\n\n\nthis will point the query node to a configuration file which can be in yaml or json format. check out the example below.\n\nsubquery: ../../../../subql-example/extrinsics\nsubqueryname: extrinsics\nbatchsize:100\nlocalmode:true\n\n\n# change the block fetching batch size\n\nsubql-node -f your-project-path --batch-size 200\n\nresult:\n[indexermanager] fetch block [203, 402]\n[indexermanager] fetch block [403, 602]\n\n\nwhen the indexer first indexes the chain, fetching single blocks will significantly decrease the performance. increasing the batch size to adjust the number of blocks fetched will decrease the overall processing time. the current default batch size is 100.\n\n# local mode\n\nsubql-node -f your-project-path --local\n\n\nfor debugging purposes, users can run the node in local mode. switching to local model will create postgres tables in the default schema public.\n\nif local mode is not used, a new postgres schema with the initial subquery_ and corresponding project tables will be created.\n\n\n# running a query service (subql/query)\n\n\n# installation\n\n# npm\nnpm install -g @subql/query\n\n\nplease note that we do not encourage the use of yarn global due to its poor dependency management which may lead to an errors down the line.\n\n\n# running the query service\n\n``` export db_host=localhost subql-query --name <project_name> --playground ````\n\nmake sure the project name is the same as the project name when you initialize the project. also, check the environment variables are correct.\n\nafter running the subql-query service successfully, open your browser and head to http://localhost:3000. you should see a graphql playground showing in the explorer and the schema that is ready to query.",charsets:{}},{title:"The Sandbox",frontmatter:{},regularPath:"/zh/run/sandbox.html",relativePath:"zh/run/sandbox.md",key:"v-63936655",path:"/zh/run/sandbox.html",headers:[{level:2,title:"Restriction",slug:"restriction",normalizedTitle:"restriction",charIndex:742}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Restriction",content:"# The Sandbox\n\nIn our envisioned usage scenario, the SubQuery node is usually run by a trusted host, and the code of the SubQuery project submitted by the user to the node is not entirely trustworthy.\n\nSome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. Therefore, we use the VM2 sandbox secured mechanism to reduce risks. This:\n\n * Runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * Securely calls methods and exchanges data and callbacks between sandboxes.\n\n * Is immune to many known methods of attack.\n\n\n# Restriction\n\n * To limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * We support 3rd party modules written in CommonJS and hybrid libraries like @polkadot/* that use ESM as default.\n\n * Any modules using HTTP and WebSocket are forbidden.",normalizedContent:"# the sandbox\n\nin our envisioned usage scenario, the subquery node is usually run by a trusted host, and the code of the subquery project submitted by the user to the node is not entirely trustworthy.\n\nsome malicious code is likely to attack the host or even compromise it, and cause damage to the data of other projects in the same host. therefore, we use the vm2 sandbox secured mechanism to reduce risks. this:\n\n * runs untrusted code securely in an isolated context and malicious code will not access the network and file system of the host unless through the exposed interface we injected into the sandbox.\n\n * securely calls methods and exchanges data and callbacks between sandboxes.\n\n * is immune to many known methods of attack.\n\n\n# restriction\n\n * to limit access to certain built-in modules, only assert, buffer, crypto,util and path are whitelisted.\n\n * we support 3rd party modules written in commonjs and hybrid libraries like @polkadot/* that use esm as default.\n\n * any modules using http and websocket are forbidden.",charsets:{}},{title:"Tutorials",frontmatter:{},regularPath:"/zh/tutorials_examples/howto.html",relativePath:"zh/tutorials_examples/howto.md",key:"v-58f735ed",path:"/zh/tutorials_examples/howto.html",headers:[{level:2,title:"How to start at a different block height?",slug:"how-to-start-at-a-different-block-height",normalizedTitle:"how to start at a different block height?",charIndex:16},{level:3,title:"Video guide",slug:"video-guide",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why not start from zero?",slug:"why-not-start-from-zero",normalizedTitle:"why not start from zero?",charIndex:928},{level:3,title:"What are the drawbacks of not starting from zero?",slug:"what-are-the-drawbacks-of-not-starting-from-zero",normalizedTitle:"what are the drawbacks of not starting from zero?",charIndex:1235},{level:3,title:"How to figure out the current blockchain height?",slug:"how-to-figure-out-the-current-blockchain-height",normalizedTitle:"how to figure out the current blockchain height?",charIndex:1415},{level:3,title:"Do I have to do a rebuild or a codegen?",slug:"do-i-have-to-do-a-rebuild-or-a-codegen",normalizedTitle:"do i have to do a rebuild or a codegen?",charIndex:1609},{level:2,title:"How to change the blockchain fetching batch size?",slug:"how-to-change-the-blockchain-fetching-batch-size",normalizedTitle:"how to change the blockchain fetching batch size?",charIndex:1814},{level:3,title:"Video guide",slug:"video-guide-2",normalizedTitle:"video guide",charIndex:62},{level:3,title:"Introduction",slug:"introduction-2",normalizedTitle:"introduction",charIndex:78},{level:3,title:"Why change the batch size?",slug:"why-change-the-batch-size",normalizedTitle:"why change the batch size?",charIndex:2509}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"How to start at a different block height? Video guide Introduction Why not start from zero? What are the drawbacks of not starting from zero? How to figure out the current blockchain height? Do I have to do a rebuild or a codegen? How to change the blockchain fetching batch size? Video guide Introduction Why change the batch size?",content:'# Tutorials\n\n\n# How to start at a different block height?\n\n\n# Video guide\n\n\n# Introduction\n\nBy default, all starter projects start synchronising the blockchain from the genesis block. In otherwords, from block 1. For large blockchains, this can typically take days or even weeks to fully synchronise.\n\nTo start a SubQuery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startBlock key.\n\nBelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecVersion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndataSources:\n  - name: main\n    kind: substrate/Runtime\n    startBlock: 1000000\n    mapping:\n      handlers:\n        - handler: handleBlock\n          kind: substrate/BlockHandler\n\n\n\n# Why not start from zero?\n\nThe main reason is that it can reduce the time to synchronise the blockchain. This means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# What are the drawbacks of not starting from zero?\n\nThe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# How to figure out the current blockchain height?\n\nIf you are using the Polkadot network, you can visit https://polkascan.io/, select the network, and then view the "Finalised Block" figure.\n\n\n# Do I have to do a rebuild or a codegen?\n\nNo. Because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# How to change the blockchain fetching batch size?\n\n\n# Video guide\n\n\n# Introduction\n\nThe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nYou need to this to the command line as an extra flag or if you are using Docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      DB_USER: postgres\n      DB_PASS: postgres\n      DB_DATABASE: postgres\n      DB_HOST: postgres\n      DB_PORT: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nThis example sets the batch size to 50.\n\n\n# Why change the batch size?\n\nUsing a smaller batch size can reduce memory usage and not leave users hanging for large queries. In otherwords, your application can be more responsive. However, more API calls are being made so if you are being charged on an I/O basis or if you have API limits somewhere in your chain, this could work to your disadvantage.',normalizedContent:'# tutorials\n\n\n# how to start at a different block height?\n\n\n# video guide\n\n\n# introduction\n\nby default, all starter projects start synchronising the blockchain from the genesis block. in otherwords, from block 1. for large blockchains, this can typically take days or even weeks to fully synchronise.\n\nto start a subquery node synchronising from a non-zero height, all you have to do is to modify your project.yaml file and change the startblock key.\n\nbelow is a project.yaml file where the start block has been set to 1,000,000\n\nspecversion: 0.0.1\ndescription: ""\nrepository: ""\nschema: ./schema.graphql\nnetwork:\n  endpoint: wss://polkadot.api.onfinality.io/public-ws\n  dictionary: https://api.subquery.network/sq/subquery/dictionary-polkadot\ndatasources:\n  - name: main\n    kind: substrate/runtime\n    startblock: 1000000\n    mapping:\n      handlers:\n        - handler: handleblock\n          kind: substrate/blockhandler\n\n\n\n# why not start from zero?\n\nthe main reason is that it can reduce the time to synchronise the blockchain. this means that if you are only interested in transactions in the last 3 months, you can only synchronise the last 3 months worth meaning less waiting time and you can start your development faster.\n\n\n# what are the drawbacks of not starting from zero?\n\nthe most obvious drawback will be that you wonât be able to query for data on the blockchain for blocks that you donât have.\n\n\n# how to figure out the current blockchain height?\n\nif you are using the polkadot network, you can visit https://polkascan.io/, select the network, and then view the "finalised block" figure.\n\n\n# do i have to do a rebuild or a codegen?\n\nno. because you are modifying the project.yaml file, which is essentially a configuration file, you will not have to rebuild or regenerate the typescript code.\n\n\n# how to change the blockchain fetching batch size?\n\n\n# video guide\n\n\n# introduction\n\nthe default batch size is 100, but this can be changed by using the extra command --batch-size=xx.\n\nyou need to this to the command line as an extra flag or if you are using docker, modify the docker-compose.yml with:\n\nsubquery-node:\n    image: onfinality/subql-node:latest\n    depends_on:\n      - "postgres"\n    restart: always\n    environment:\n      db_user: postgres\n      db_pass: postgres\n      db_database: postgres\n      db_host: postgres\n      db_port: 5432\n    volumes:\n      - ./:/app\n    command:\n      - -f=/app\n      - --local\n      - --batch-size=50\n\n\n\nthis example sets the batch size to 50.\n\n\n# why change the batch size?\n\nusing a smaller batch size can reduce memory usage and not leave users hanging for large queries. in otherwords, your application can be more responsive. however, more api calls are being made so if you are being charged on an i/o basis or if you have api limits somewhere in your chain, this could work to your disadvantage.',charsets:{}},{title:"Tutorials & Examples",frontmatter:{},regularPath:"/zh/tutorials_examples/introduction.html",relativePath:"zh/tutorials_examples/introduction.md",key:"v-aff4ed22",path:"/zh/tutorials_examples/introduction.html",headers:[{level:2,title:"SubQuery Examples",slug:"subquery-examples",normalizedTitle:"subquery examples",charIndex:155}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"SubQuery Examples",content:"# Tutorials & Examples\n\nHere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# SubQuery Examples\n\nEXAMPLE                     DESCRIPTION                                                    TOPICS\nextrinsic-finalized-block   Indexes extrinsics so they can be queried by their hash        The simplest example with a block handler function\nblock-timestamp             Indexes timestamp of each finalized block                      Another simple call handler function\nvalidator-threshold         Indexes the least staking amount required for a validator to   More complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  Indexes staking bond, rewards, and slashes from the events     More complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             Indexes balance transfers between accounts, also indexes       One-to-many and many-to-many relationships and complicated\n                            utility batchAll to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       Indexes birth info of kitties.                                 Complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",normalizedContent:"# tutorials & examples\n\nhere we will list our tutorials and explore various examples to help you get up and running in the easiest and fastest manner.\n\n\n# subquery examples\n\nexample                     description                                                    topics\nextrinsic-finalized-block   indexes extrinsics so they can be queried by their hash        the simplest example with a block handler function\nblock-timestamp             indexes timestamp of each finalized block                      another simple call handler function\nvalidator-threshold         indexes the least staking amount required for a validator to   more complicated block handler function that makes external\n                            be elected.                                                    calls to the @polkadot/api for additional on-chain data\nsum-reward                  indexes staking bond, rewards, and slashes from the events     more complicated event handlers with a one-to-many\n                            of finalized block                                             relationship\nentity-relation             indexes balance transfers between accounts, also indexes       one-to-many and many-to-many relationships and complicated\n                            utility batchall to find out the content of the extrinsic      extrinsic handling\n                            calls\nkitty                       indexes birth info of kitties.                                 complex call handlers and event handlers, with data indexed\n                                                                                           from a custom chain",charsets:{}},{title:"Terminology",frontmatter:{},regularPath:"/zh/tutorials_examples/terminology.html",relativePath:"zh/tutorials_examples/terminology.md",key:"v-cc84cb26",path:"/zh/tutorials_examples/terminology.html",headers:[{level:2,title:"Terminology",slug:"terminology",normalizedTitle:"terminology",charIndex:2}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Terminology",content:"# Terminology\n\n * SubQuery Project (where the magic happens): A definition (@subql/cli) of how a SubQuery Node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful GraphQL queries\n * SubQuery Node (where the work is done): A package (@subql/node) that will accept a SubQuery project definiton, and run a node that constantly indexes a connected network to a database\n * SubQuery Query Service (where we get the data from): A package (@subql/query) that interacts with the GraphQL API of a deployed SubQuery node to query and view the indexed data\n * GraphQL (how we query the data): A query langage for APIs that is specifically suited for flexible graph based data - see graphql.org",normalizedContent:"# terminology\n\n * subquery project (where the magic happens): a definition (@subql/cli) of how a subquery node should traverse and aggregate a projects network and how the data should the transformed and stored to enable useful graphql queries\n * subquery node (where the work is done): a package (@subql/node) that will accept a subquery project definiton, and run a node that constantly indexes a connected network to a database\n * subquery query service (where we get the data from): a package (@subql/query) that interacts with the graphql api of a deployed subquery node to query and view the indexed data\n * graphql (how we query the data): a query langage for apis that is specifically suited for flexible graph based data - see graphql.org",charsets:{}},{title:"Publish your SubQuery Project",frontmatter:{},regularPath:"/vi/publish/publish.html",relativePath:"vi/publish/publish.md",key:"v-fb63c69a",path:"/vi/publish/publish.html",headers:[{level:2,title:"Benefits of hosting your project with SubQuery",slug:"benefits-of-hosting-your-project-with-subquery",normalizedTitle:"benefits of hosting your project with subquery",charIndex:36},{level:2,title:"Create your First Project",slug:"create-your-first-project",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Login to SubQuery Projects",slug:"login-to-subquery-projects",normalizedTitle:"login to subquery projects",charIndex:534},{level:4,title:"Create your First Project",slug:"create-your-first-project-2",normalizedTitle:"create your first project",charIndex:505},{level:4,title:"Deploy your first Version",slug:"deploy-your-first-version",normalizedTitle:"deploy your first version",charIndex:2735},{level:2,title:"Next Steps - Connect to your Project",slug:"next-steps-connect-to-your-project",normalizedTitle:"next steps - connect to your project",charIndex:3806},{level:2,title:"Add GitHub Organization Account to SubQuery Projects",slug:"add-github-organization-account-to-subquery-projects",normalizedTitle:"add github organization account to subquery projects",charIndex:4261}],lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:"Benefits of hosting your project with SubQuery Create your First Project Login to SubQuery Projects Create your First Project Deploy your first Version Next Steps - Connect to your Project Add GitHub Organization Account to SubQuery Projects",content:"# Publish your SubQuery Project\n\n\n# Benefits of hosting your project with SubQuery\n\n * We'll run your SubQuery projects for you in a high performance, scalable, and managed public service\n * This service is being provided to the community for free!\n * You can make your projects public so that they'll be listed in the SubQuery Explorer and anyone around the world can view them\n * We're integrated with GitHub, so anyone in your GitHub organisations will be able to view shared organisation projects\n\n\n# Create your First Project\n\n# Login to SubQuery Projects\n\nBefore starting, please make sure that your SubQuery project is online in a public GitHub repository. The schema.graphql file must be in the root of your directory.\n\nTo create your first project, head to project.subquery.network. You'll need to authenticate with your GitHub account to login.\n\nOn first login, you will be asked to authorize SubQuery. We only need your email address to identify your account, and we don't use any other data from your GitHub account for any other reasons. In this step, you can also request or grant access to your GitHub Organization account so you can post SubQuery projects under your GitHub Organization instead of your personal account.\n\n\n\nSubQuery Projects is where you manage all your hosted projects uploaded to the SubQuery platform. You can create, delete, and even upgrade projects all from this application.\n\n\n\nIf you have a GitHub Organization accounts connected, you can use the switcher on the header to change between your personal account and your GitHub Organization account. Projects created in a GitHub Organization account are shared between members in that GitHub Organization. To connect your GitHub Organization account, you can follow the steps here.\n\n\n\n# Create your First Project\n\nLet's start by clicking on \"Create Project\". You'll be taken to the New Project form. Please enter the following (you can change this in the future):\n\n * GitHub account: If you have more than one GitHub account, select which account this project will be created under. Projects created in a GitHub organisation account are shared between members in that organisation.\n * Name\n * Subtitle\n * Description\n * GitHub Repository URL: This must be a valid GitHub URL to a public repository that has your SubQuery project. The schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * Hide project: If selected, this will hide the project from the public SubQuery explorer. Keep this unselected if you want to share your SubQuery with the community!\n\nCreate your project and you'll see it on your SubQuery Project's list. *We're almost there! We just need to deploy a new version of it. *\n\n# Deploy your first Version\n\nWhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. Deploying a version triggers a new SubQuery indexing operation to start, and sets up the required query service to start accepting GraphQL requests. You can also deploy new versions to existing projects here.\n\nWith your new project, you'll see a Deploy New Version button. Click this, and fill in the required information about the deployment:\n\n * Commit Hash of new Version: From GitHub, copy the full commit hash of the version of your SubQuery project codebase that you want deployed\n * Indexer Version: This is the version of SubQuery's node service that you want to run this SubQuery on. See @subql/node\n * Query Version: This is the version of SubQuery's query service that you want to run this SubQuery on. See @subql/query\n\n\n\nIf deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. This process may take time until it reaches 100%.\n\n\n# Next Steps - Connect to your Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed GraphQL Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in-browser playground to get started - read more about how to user our Explorer here.\n\n\n\n\n# Add GitHub Organization Account to SubQuery Projects\n\nIt is common to publish your SubQuery project under the name of your GitHub Organization account rather than your personal GitHub account. At any point your can change your currently selected account on SubQuery Projects using the account switcher.\n\n\n\nIf you can't see your GitHub Organization account listed in the switcher, the you may need to grant access to SubQuery for your GitHub Organization (or request it from an administrator). To do this, you first need to revoke permissions from your GitHub account to the SubQuery Application. To do this, login to your account settings in GitHub, go to Applications, and under the Authorized OAuth Apps tab, revoke SubQuery - you can follow the exact steps here. Don't worry, this will not delete your SubQuery project and you will not lose any data.\n\n\n\nOnce you have revoked access, log out of SubQuery Projects and log back in again. You should be redirected to a page titled Authorize SubQuery where you can request or grant SubQuery access to your GitHub Organization account. If you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nOnce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct GitHub Organization account in the account switcher.",normalizedContent:"# publish your subquery project\n\n\n# benefits of hosting your project with subquery\n\n * we'll run your subquery projects for you in a high performance, scalable, and managed public service\n * this service is being provided to the community for free!\n * you can make your projects public so that they'll be listed in the subquery explorer and anyone around the world can view them\n * we're integrated with github, so anyone in your github organisations will be able to view shared organisation projects\n\n\n# create your first project\n\n# login to subquery projects\n\nbefore starting, please make sure that your subquery project is online in a public github repository. the schema.graphql file must be in the root of your directory.\n\nto create your first project, head to project.subquery.network. you'll need to authenticate with your github account to login.\n\non first login, you will be asked to authorize subquery. we only need your email address to identify your account, and we don't use any other data from your github account for any other reasons. in this step, you can also request or grant access to your github organization account so you can post subquery projects under your github organization instead of your personal account.\n\n\n\nsubquery projects is where you manage all your hosted projects uploaded to the subquery platform. you can create, delete, and even upgrade projects all from this application.\n\n\n\nif you have a github organization accounts connected, you can use the switcher on the header to change between your personal account and your github organization account. projects created in a github organization account are shared between members in that github organization. to connect your github organization account, you can follow the steps here.\n\n\n\n# create your first project\n\nlet's start by clicking on \"create project\". you'll be taken to the new project form. please enter the following (you can change this in the future):\n\n * github account: if you have more than one github account, select which account this project will be created under. projects created in a github organisation account are shared between members in that organisation.\n * name\n * subtitle\n * description\n * github repository url: this must be a valid github url to a public repository that has your subquery project. the schema.graphql file must be in the root of your directory (learn more about the directory structure).\n * hide project: if selected, this will hide the project from the public subquery explorer. keep this unselected if you want to share your subquery with the community!\n\ncreate your project and you'll see it on your subquery project's list. *we're almost there! we just need to deploy a new version of it. *\n\n# deploy your first version\n\nwhile creating a project will setup the display behaviour of the project, you must deploy a version of it before it becomes operational. deploying a version triggers a new subquery indexing operation to start, and sets up the required query service to start accepting graphql requests. you can also deploy new versions to existing projects here.\n\nwith your new project, you'll see a deploy new version button. click this, and fill in the required information about the deployment:\n\n * commit hash of new version: from github, copy the full commit hash of the version of your subquery project codebase that you want deployed\n * indexer version: this is the version of subquery's node service that you want to run this subquery on. see @subql/node\n * query version: this is the version of subquery's query service that you want to run this subquery on. see @subql/query\n\n\n\nif deployed successfully, you'll see the indexer start working and report back progress on indexing the current chain. this process may take time until it reaches 100%.\n\n\n# next steps - connect to your project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed graphql query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in-browser playground to get started - read more about how to user our explorer here.\n\n\n\n\n# add github organization account to subquery projects\n\nit is common to publish your subquery project under the name of your github organization account rather than your personal github account. at any point your can change your currently selected account on subquery projects using the account switcher.\n\n\n\nif you can't see your github organization account listed in the switcher, the you may need to grant access to subquery for your github organization (or request it from an administrator). to do this, you first need to revoke permissions from your github account to the subquery application. to do this, login to your account settings in github, go to applications, and under the authorized oauth apps tab, revoke subquery - you can follow the exact steps here. don't worry, this will not delete your subquery project and you will not lose any data.\n\n\n\nonce you have revoked access, log out of subquery projects and log back in again. you should be redirected to a page titled authorize subquery where you can request or grant subquery access to your github organization account. if you don't have admin permissions, you must make a request for an adminstrator to enable this for you.\n\n\n\nonce this request has been approved by your administrator (or if are able to grant it youself), you will see the correct github organization account in the account switcher.",charsets:{}},{title:"Connect to your New Project",frontmatter:{},regularPath:"/vi/publish/connect.html",relativePath:"vi/publish/connect.md",key:"v-d805f606",path:"/vi/publish/connect.html",lastUpdated:"8/11/2021, 11:16:10 PM",headersStr:null,content:"# Connect to your New Project\n\nOnce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed Query endpoint.\n\n\n\nAlternatively, you can click on the three dots next to the title of your project, and view it on SubQuery Explorer. There you can use the in browser playground to get started.\n\n\n\n\n# Learn more about GraphQL\n\nYou can follow the official GraphQL guide here to learn more about GraphQL, how it works, and how to use it:\n\n * There are libraries to help you implement GraphQL in many different languages\n * For an in-depth learning experience with practical tutorials, see How to GraphQL.\n * Check out the free online course, Exploring GraphQL: A Query Language for APIs.",normalizedContent:"# connect to your new project\n\nonce your deployment has succesfully completed and our nodes have indexed your data from the chain, you'll be able to connect to your project via the displayed query endpoint.\n\n\n\nalternatively, you can click on the three dots next to the title of your project, and view it on subquery explorer. there you can use the in browser playground to get started.\n\n\n\n\n# learn more about graphql\n\nyou can follow the official graphql guide here to learn more about graphql, how it works, and how to use it:\n\n * there are libraries to help you implement graphql in many different languages\n * for an in-depth learning experience with practical tutorials, see how to graphql.\n * check out the free online course, exploring graphql: a query language for apis.",charsets:{}}],themeConfig:{logo:"/assets/img/logo.png",logoLink:"https://subquery.network",lastUpdated:!0,nav:[{text:"Explorer",link:"https://explorer.subquery.network/",target:"_blank",rel:""},{text:"Projects",link:"https://project.subquery.network/",target:"_blank",rel:""},{text:"Documentation",link:"/"},{text:"GitHub",link:"https://github.com/subquery/subql",target:"_blank",rel:""}],sidebarDepth:2,sidebar:[{title:"Welcome to SubQuery",path:"/"},{title:"Quick Start Guide",path:"/quickstart/quickstart",children:["/quickstart/quickstart.md","/quickstart/helloworld-localhost.md","/quickstart/understanding-helloworld.md","/quickstart/helloworld-hosted.md"]},{title:"Installation",path:"/install/install",children:["/install/install.md"]},{title:"Create a Project",path:"/create/introduction",children:["/create/introduction.md","/create/manifest.md","/create/graphql.md","/create/mapping.md"]},{title:"Run a Project",path:"/run/run",children:["/run/run.md","/run/sandbox.md"]},{title:"Publish a Project",path:"/publish/publish",children:["/publish/publish.md","/publish/upgrade.md","/publish/connect.md"]},{title:"Query your Data",path:"/query/query",children:["/query/query.md","/query/graphql.md"]},{title:"Tutorials & Examples",path:"/tutorials_examples/introduction",children:["/tutorials_examples/howto.md","/tutorials_examples/terminology.md"]},{title:"FAQs",path:"/faqs/faqs.md",children:[]},{title:"Miscellaneous",path:"/miscellaneous/contributing",children:["/miscellaneous/contributing.md","/miscellaneous/social_media.md","/miscellaneous/branding.md","/miscellaneous/ambassadors.md"]}]},locales:{"/":{lang:"English",title:"SubQuery Docs",description:"Explore and transform your chain data to build intuitive dApps faster!.",path:"/"}}};t(303);La.component("Badge",(function(){return Promise.all([t.e(0),t.e(3)]).then(t.bind(null,656))})),La.component("CodeBlock",(function(){return Promise.all([t.e(0),t.e(4)]).then(t.bind(null,379))})),La.component("CodeGroup",(function(){return Promise.all([t.e(0),t.e(5)]).then(t.bind(null,380))}));t(304),t(187);var cs=[{},function(e){e.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(e){e.router}],hs=[];function ps(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}t(177);function ys(e,n){for(var t=0;t<n.length;t++){var o=n[t];o.enumerable=o.enumerable||!1,o.configurable=!0,"value"in o&&(o.writable=!0),Object.defineProperty(e,o.key,o)}}function ms(e,n,t){return n&&ys(e.prototype,n),t&&ys(e,t),e}t(168);t(159);function gs(e,n){return(gs=Object.setPrototypeOf||function(e,n){return e.__proto__=n,e})(e,n)}t(160);function bs(e){return(bs=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}var fs=t(165),ws=t.n(fs);function vs(e,n){return!n||"object"!==ws()(n)&&"function"!=typeof n?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):n}function ks(e){var n=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}();return function(){var t,o=bs(e);if(n){var a=bs(this).constructor;t=Reflect.construct(o,arguments,a)}else t=o.apply(this,arguments);return vs(this,t)}}var qs=function(e){!function(e,n){if("function"!=typeof n&&null!==n)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(n&&n.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),n&&gs(e,n)}(t,e);var n=ks(t);function t(){return ps(this,t),n.apply(this,arguments)}return t}(function(){function e(){ps(this,e),this.store=new La({data:{state:{}}})}return ms(e,[{key:"$get",value:function(e){return this.store.state[e]}},{key:"$set",value:function(e,n){La.set(this.store.state,e,n)}},{key:"$emit",value:function(){var e;(e=this.store).$emit.apply(e,arguments)}},{key:"$on",value:function(){var e;(e=this.store).$on.apply(e,arguments)}}]),e}());Object.assign(qs.prototype,{getPageAsyncComponent:Gi,getLayoutAsyncComponent:Ri,getAsyncComponent:Bi,getVueComponent:Wi});var xs={install:function(e){var n=new qs;e.$vuepress=n,e.prototype.$vuepress=n}};function js(e){e.beforeEach((function(n,t,o){if(Ss(e,n.path))o();else if(/(\/|\.html)$/.test(n.path))if(/\/$/.test(n.path)){var a=n.path.replace(/\/$/,"")+".html";Ss(e,a)?o(a):o()}else o();else{var r=n.path+"/",i=n.path+".html";Ss(e,i)?o(i):Ss(e,r)?o(r):o()}}))}function Ss(e,n){var t=n.toLowerCase();return e.options.routes.some((function(e){return e.path.toLowerCase()===t}))}var Ts={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(e){var n=this.pageKey||this.$parent.$page.key;return Ui("pageKey",n),La.component(n)||La.component(n,Gi(n)),La.component(n)?e(n):e("")}},Is={functional:!0,props:{slotKey:String,required:!0},render:function(e,n){var t=n.props,o=n.slots;return e("div",{class:["content__".concat(t.slotKey)]},o()[t.slotKey])}},Ps={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Qs=(t(306),t(307),Object(ss.a)(Ps,(function(){var e=this.$createElement,n=this._self._c||e;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function zs(){return(zs=Object(o.a)(regeneratorRuntime.mark((function e(n){var t,o,a,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:ds.routerBase||ds.base,js(o=new ji({base:t,mode:"history",fallback:!1,routes:us,scrollBehavior:function(e,n,t){return t||(e.hash?!La.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})}})),a={},e.prev=4,e.next=7,Promise.all(cs.filter((function(e){return"function"==typeof e})).map((function(e){return e({Vue:La,options:a,router:o,siteData:ds,isServer:n})})));case 7:e.next=12;break;case 9:e.prev=9,e.t0=e.catch(4),console.error(e.t0);case 12:return r=new La(Object.assign(a,{router:o,render:function(e){return e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},hs.map((function(n){return e(n)})))])}})),e.abrupt("return",{app:r,router:o});case 14:case"end":return e.stop()}}),e,null,[[4,9]])})))).apply(this,arguments)}La.config.productionTip=!1,La.use(ji),La.use(xs),La.mixin(function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:La;Si(n),t.$vuepress.$set("siteData",n);var o=e(t.$vuepress.$get("siteData")),a=new o,r=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),i={};return Object.keys(r).reduce((function(e,n){return n.startsWith("$")&&(e[n]=r[n].get),e}),i),{computed:i}}((function(e){return function(){function n(){ps(this,n)}return ms(n,[{key:"setPage",value:function(e){this.__page=e}},{key:"$site",get:function(){return e}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var e,n,t=this.$site.locales,o=void 0===t?{}:t;for(var a in o)"/"===a?n=o[a]:0===this.$page.path.indexOf(a)&&(e=o[a]);return e||n||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var e=this.$page.frontmatter.canonicalUrl;return"string"==typeof e&&e}},{key:"$title",get:function(){var e=this.$page,n=this.$page.frontmatter.metaTitle;if("string"==typeof n)return n;var t=this.$siteTitle,o=e.frontmatter.home?null:e.frontmatter.title||e.title;return t?o?o+" | "+t:t:o||"VuePress"}},{key:"$description",get:function(){var e=function(e){if(e){var n=e.filter((function(e){return"description"===e.name}))[0];if(n)return n.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(e,n){for(var t=0;t<e.length;t++){var o=e[t];if(o.path.toLowerCase()===n.toLowerCase())return o}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),n}()}),ds)),La.component("Content",Ts),La.component("ContentSlotsDistributor",Is),La.component("OutboundLink",Qs),La.component("ClientOnly",{functional:!0,render:function(e,n){var t=n.parent,o=n.children;if(t._isMounted)return o;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),La.component("Layout",Ri("Layout")),La.component("NotFound",Ri("NotFound")),La.prototype.$withBase=function(e){var n=this.$site.base;return"/"===e.charAt(0)?n+e.slice(1):e},window.__VUEPRESS__={version:"1.8.2",hash:"b75beaa"},function(e){return zs.apply(this,arguments)}(!1).then((function(e){var n=e.app;e.router.onReady((function(){n.$mount("#app")}))}))}]);